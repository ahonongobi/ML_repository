{"02.classification_and_perceptron.pdf": "02: Classification and Perceptron\nMachine Learning\nBachelor in Computer Science [SP24]\nMarch 04, 2024\n\nWhere are we\n\u2022 Processes can be studied as some underlying function\nthat generates data\n\u2022 Machine learning approximates this underlying function\nby adapting parametrized models\n\u2022 Supervised Learning: learn the model\u2019s parameters\nusing labeled data examples\n\u2022 Once you learn the model, you can use it for many\napplications (regression, classi\ufb01cation, etc.)\n1\n\nDid you refresh your math?\nTopics we will use in this lecture:\n\u2022 Vector Space\n\u2022 Inner Product\n\u2022 Linear Function\n\u2022 A\ufb03ne Function\n2\n\nToday\u2019s menu\n\u2022 Binary classi\ufb01cation\n\u2022 Features\n\u2022 Decision trees\n\u2022 Linear classi\ufb01cation\n\u2022 Perceptron algorithm\n3\n\nBinary Classi\ufb01cation\n\u2022 Given an input, decide to which of\ntwo classes it belongs\n\u2022 Example: X are fruits, Y are the\nlabels: {apple, orange}. Learn\nhypothesis h : X \u2192Y that\nclassi\ufb01es as correctly as possible.\nAnd this is it for now! Keep asking\nyourself \u201cwhat does classi\ufb01cation mean,\nreally?\u201d until you feel you can articulate\nyour answer satisfactorily!\n?\n4\n\nFeatures\ncolor\nweight\nshape\nname\nyellow\n150\noval\nlemon\ngreen\n200\nround\napple\nred\n15\noval\nstrawberry\nyellow\n200\nelongated\nbanana\nred\n180\nround\napple\n\u2022 Data points x \u2208X are described by their features (components)\n\u2022 This data for example has values for color, weight, shape, and name\n\u2022 Each component xi \u2208x is an independent, meaningful feature value.\n\u2022 Features have types, such as binary (xi \u2208{0, 1}) and continuous (xi \u2208R).\n\u2022 For SL, we can choose any feature to be our label depending on what we\nwant to learn (e.g. the \u201cname\u201d): this is arbitrary\n\u2022 Note: X becomes high-dimensional from now on, expect linear algebra!\n5\n\nFeatures\ncolor\nweight\nshape\nname\nyellow\n150\noval\nlemon\ngreen\n200\nround\napple\nred\n15\noval\nstrawberry\nyellow\n200\nelongated\nbanana\nred\n180\nround\napple\n\u2022 Traditional data matrix\n\u2022 Rows represent data points\n\u2022 Columns represent features\n\u2022 Easily human-readable (if tiny)\n\u2022 The number of features correspond\nto the data dimensionality\nfeature\ntype\nvalues\ncolor\ndiscr\nred, green, yellow\nweight\ncont\n1g to 10\u2032000g\nshape\ndiscr\nround, oval, elongated\nname\ndiscr\napple, lemon, banana\n\u2022 Metadata: the description of the\ndescriptors (features)\n\u2022 Features have either continuous or\ndiscrete values\n\u2022 Discrete features are often\ncategorical (e.g. front/back) or\nnumerical (e.g. ordinal 4/5 stars, or\ncardinal 3 apples).\n6\n\nDecision Trees\n\u2022 An intuitive way towards classi\ufb01cation is to build a graph of discriminant\ncharacteristics that can be navigated as a \u201cif / else\u201d construct.\n\u2022 Such graph is called a decision tree and can be used to predict for example\nthe type of fruit from its properties (feature values).\n\u2022 A decision tree makes a series of (possibly simple) decisions, one at a time,\neach involving only a single feature such as \u201cis the weight less than 30g?\u201d\n\u2022 Since there are two possible answers (yes/no), this question splits the data\ninto two subsets, e\ufb00ectively splitting the input space into two partitions.\n\u2022 To further the decision, we consider only the subset for which the answer is\nyes.\n7\n\nMulticlass classi\ufb01cation\n\u2022 A further question could be:\n\u201cof which shape?\u201d.\n\u2022 Such a question can have\nmultiple answers, which\ngenerate an arbitrary amount\nof disjoint subsets.\n\u2022 You can also derive multiclass\nclassi\ufb01cation from binary\nsimply by adding more binary\ndecisions (keep it in mind for\nlater!)\n8\n\nweight<150\nweight<80\nshape\nno\nyes\nweight<10\nmelon\npear\nyes\nround\nother\ncherry\nstrawberry\nyes\nno\npineapple\noval\nbanana\nelongated\nkiwi\nno\n9\n\nDiscriminant Features\n\u2022 Some features can be more helpful than other in distinguishing data\n\u2022 For example, we may \ufb01nd that (nearly) all data points with the properties\nweight < 30 and shape = oval are \u201cstrawberries\u201d\n\u2022 For fruits over 30 grams, the shape may not be so helpful as for example the\ncolor. Di\ufb00erent sequences of questions help di\ufb00erently in resolving the data\npartition\n\u2022 A question that leads to a shorter sequence of follow-up questions has a\nhigh information gain, and is said to be highly discriminant\n\u2022 Pay attention to the di\ufb00erence between data and information,\nas this will become a central theme in this course\n10\n\nInterpretability\n\u2022 One key advantage of decision trees is that they are easily interpretable by\nhumans, as they can also be constructed by hand. This is often done to\nrepresent expert knowledge\n\u2022 Running a decision tree corresponds to asking a sequence of questions on\nthe data features, which in turn partitions the decision space in areas of\ndi\ufb00erent size\n\u2022 To construct a decision tree by hand, the most important question is to\nrecognize for each step which is the next most discriminant question to pose\n\u2022 Machine learning algorithms take these decisions using specialized metric\nmeasures applied to feature vectors\n11\n\nGeneralization (?)\n\u2022 Why do you train a ML model? In any task, you always want to create a\ntool that will be useful on new (unseen) data, which will become available in\nthe future\n\u2022 The ability of a model to provide a useful response on new, previously unseen\ndata, is called generalization, and is another key concept in this course\n\u2022 Let us consider a decision tree trained on the fruit features described earlier\n\u2022 If we split elongated fruit by color, we \ufb01nd that the training data does not\ninclude any red elongated fruit\n\u2022 Now someone brings a weird elongated but undeniably red fruit to your\nattention\nHow will the tree classify it? Will it recognize its own uncertainty?\n12\n\nweight<150\nweight<80\nshape\nno\nyes\nweight<10\nmelon\npear\nyes\nround\nother\ncherry\nstrawberry\nyes\nno\npineapple\noval\nbanana\nelongated\nkiwi\nno\n13\n\nLimitations\n\u2022 The main limitation of decision trees is in how speci\ufb01c the leaves are, since\nthey are elements straight from the training data\n\u2022 This limits considerably the generalization capability of the model, which\nbecome hard to scale and generalize\n\u2022 As powerful as some Decision Tree methods are, you need to be very careful\nto ascertain that the answers you want it to give are well represented in\nthe training data\n\u2022 If you want a model capable of generating \u201cintermediate\u201d answers,\ninterpolated between the available data points, you may need to look for\nanother method\n\u2022 A similar reasoning led to the First AI Winter in the \u201950s, the \ufb01rst time\nMachine Learning failed to succeed in real-world applications (rule-based AI\nor Good Old AI)\n14\n\nModern applications\nDecision Trees however are far from obsolete!\n\u2022 Many modern applications rely on decision trees to incorporate expert\nknowledge, hands-on experience from humans who know the task\n\u2022 Trees with random valued thresholds have been applied to feature\nextraction, i.e. summing up discriminant information from a features set\n\u2022 Trees are the fundamental data structure (model) for several modern\nensemble learning methods, such as random forests and random ferns\n\u2022 Decision trees learning algorithms are typically based on bagging, short for\nbootstrap aggregating. We will not explore these algorithms in detail in the\ncourse, but if you are interested you can check the extra material in the end.\nDecision Trees are just very easy to understand and use when you need them.\nMany machine learning and data analysis libraries o\ufb00er methods to quickly train\nthem, check out for example XGBoost in the extra material!\n15\n\nSummary: Decision Trees\n\u2022 Decision trees are rule-based classi\ufb01cation tools.\n\u2022 The model is a set of nested conditional functions easy to visualize as a\ntree-like structure, and implemented as \u201cif-else\u201d chains\n\u2022 The parametrization is composed by thresholds on features\n\u2022 Its implementation is outside the scope of this lecture\n\u2022 The tree model requires implementation as a graph-like structure in Python\n\u2022 The learning algorithm is simplistic and has no extension for our course\n\u2022 Feel free to try it at home! (check the extra material)\n\u2022 They are still widely used methods, its main applications being:\n\u2022 Integrating expert knowledge, as humans can write them easily by hand\n\u2022 Integrating multiple ML approaches in what is called ensemble methods\n\u2022 Discrete applications that trade o\ufb00interpretability for performance (random\ntrees and ferns, gradient boosting)\n16\n\nLinear Classi\ufb01cation\n\u2022 Example: X are fruits, Y are the\nlabels {apple, orange}. Learn\nhypothesis h : X \u2192Y that\nperforms binary classi\ufb01cation\n\u2022 A linear model separates the input\nspace into two regions: the xi for\nwhich h(xi) =apple, and the xj for\nwhich h(xj) =orange\n\u2022 This decision boundary using a\nlinear model is a hyperplane in n\ndimensions (i.e. a line for n = 2 )\n?\n17\n\nLinear Classi\ufb01cation\n\u2022 Let the input space X be a vector space with inner product. Think of\nX = Rp with standard inner product.\n\u2022 We consider hypothesis (i.e. models) of the form\nh(x) =\n(\napple\nif f (x) > 0\norange\nif f (x) < 0\nf (x) = \u27e8w, x\u27e9+ b\nh(x) = sign(f (x))\n\u2022 The separating hyperplane is given by H0 = {x \u2208X | f (x) = 0}. The\nclassi\ufb01er h predicts x to be an apple in the half-space {x \u2208X | f (x) > 0}\nand an orange in the half-space {x \u2208X | f (x) < 0}.\n\u2022 The vector w determines the orientation (slope) of the hyperplane, and\nthe o\ufb00set b adjusts its distance from the origin (intercept).\n18\n\nHow to draw the decision boundary\n\u2022 We will plot in 2D for convenience, but your algorithms scales up\n\u2022 A 2-dimensional decision boundary for classi\ufb01cation is a line\n\u2022 The two axis will be x1 and x2, and correspond to the two features of an\ninput data point x, which is a 2-dimensional vector\n\u2022 The vector w =\n\u0002\nw1\nw2\n\u0003\nhas an \u201corientation\u201d for each axis\n\u2022 Calculating the value on one point x will return a number\n\u2022 This number tells us where the point is w.r.t. the line:\n\u2022 positive \u2192above\n\u2022 negative \u2192below\n\u2022 0 \u2192on the line\n\u2022 Solving for 0 gives us the line to plot: f (x) =\n\u0002\nw1\nw2\n\u0003 \u0014x1\nx2\n\u0015\n+ b = 0\nw1x1 + w2x2 + b = 0\n\u2192\nx2 = \u2212w1\nw2\nx1 \u2212b\nw2\nTO PLOT: x axis is x1, y axis is x2, m is \u2212w1\nw2, q is \u2212b\nw2\n19\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\n20\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\n21\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\n\u0002\n1\n2\n\u0003 \u0014x1\nx2\n\u0015\n+ 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\n22\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\n23\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1\nx2\n24\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\n25\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\n26\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n27\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n28\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n29\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nw\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n30\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nw\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n31\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nw\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n32\n\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u22122\n\u22121\n1\n2\nw\nb\n||w||\nf(x)\n=\n\u27e8w, x\u27e9+ b\nf(x)\n=\nx1 + 2x2 + 4\nw\n=\n\u0002\n1\n2\n\u0003\nx\n=\n\u0014x1\nx2\n\u0015\nb\n=\n+4\nx1 \u2192x\nx2 \u2192y\ny\n=\nmx + q\ny\n=\n\u22120.5x \u22122\nm\n=\n\u2212w1\nw2\n= \u22120.5\nq\n=\n\u2212b\nw2\n= \u22122\n33\n\n34\n\nLinear Separability\n\u2022 For generalization, let\u2019s call our\n\u201capple\u201d and \u201corange\u201d classes as\nY = {\u22121, +1}, i.e. the positive\nand the negative class\n\u2022 A data set\nD =\n\u0000(x1, y1), . . . , (xn, yn)\n\u0001\nis\nlinearly separable if there exists\nf (x) = \u27e8w, x\u27e9+ b, such that\n\u2022 f (xi) > 0 whenever yi = +1\n\u2022 f (xi) < 0 whenever yi = \u22121\n\u2022 We then can simplify this rule to\nyi \u00b7 f (xi) > 0\n(think: when is this negative?)\n35\n\nMargin\n\u2022 The quantity\nyi \u00b7 f (xi) = yi \u00b7 (\u27e8w, xi\u27e9+ b)\nis the margin of point (xi, yi)\n\u2022 Think of it as a \u201csafety margin\u201d\nfrom the data to the separating\nhyperplane; for \u2225w\u2225= 1 it encodes\nthe \u201csigned distance\u201d from f , for\neach point\n\u2022 Given a data set D, the margin of a\n(a\ufb03ne) linear function f is de\ufb01ned\nas \u03b3 = mini{yi \u00b7 f (xi)}\n36\n\nMargin\n\u2022 Notice how the margin multiplies the label by the prediction\nyi \u00b7 f (xi)\n\u2022 With binary labels Y = {+1, \u22121} and decision boundary at f (x) = 0, class\n+1 represents the subspace above the boundary, and \u22121 the one below\n\u2022 The result of the multiplication of two signed numbers is positive if their\nsigns correspond, and negative if they disagree\n\u2022 If the margin yi \u00b7 f (xi) is positive then the example (xi, yi) is classi\ufb01ed\ncorrectly\n\u2022 Negative margin indicates wrong classi\ufb01cation\n37\n\nLinear separation: too easy?\n\u2022 Given some data, how do we obtain\na function f (x) = \u27e8w, x\u27e9+ b that\nrealizes a linear separation?\nHow do we \ufb01nd w and b?\n\u2022 This looks easy! We could even do\nit manually!\n\u2022 It is trivial in X = R1\n\u2022 It is still easy in X = R2\n\u2022 What happens in X = R100?\n38\n\nLinear separation: too easy?\n\u2022 In X = R100 we have to \ufb01x 100\nslopes, \ufb01nd 100 correct values!\n\u2022 How about a million dimensions?\nWhat if X is an in\ufb01nite dimensional\nspace?\n(we will encounter those soon)\n\u2022 What if we \ufb01nd a linear separation\nfor 1000 data points, but then 10\nmore data points become available\nand we are classifying them\nincorrectly: how do we adjust the\nsolution?\n\u2022 Wouldn\u2019t it be just nice to have an\nalgorithm do the job for us...\n39\n\nCorrecting Mistakes: The Perceptron Algorithm\n\u2022 We want an algorithm for \ufb01nding w and b as a function of the available\ndata D =\n\u0000(x1, y1), . . . , (xn, yn)\n\u0001\n\u2022 This is not easy, even for a computer, as there is no \u201cclosed form\u201d solution\nto this problem\n\u2022 Instead we will go for an iterative procedure: start with a viable but\nimprecise solution, then improve it over subsequent iterations\n\u2022 This procedure is known as the Perceptron training algorithm (Rosenblatt,\n1962)\n\u2022 Algorithms that compute an error vector, then update the parametrization\nagainst its direction, minimizing it, descending the inclination (slope,\ngradient) of the error function, are called gradient descent algorithms\n40\n\nCorrecting Mistakes: The Perceptron Algorithm\n\u2022 Start with an initial parametrization \u03b8 = (w0, b0) (typically random),\nwhich corresponds to the initial model f\u03b8(x) = \u27e8w0, x\u27e9+ b0\n\u2022 Given a training example (xi, yi), if the output of the model \u02c6\nyi = f\u03b8(xi) ends\nup on the wrong side of the decision boundary it means the model\nmisclassi\ufb01es xi, as can be seen by the negative margin:\nyi \u00b7 f (xi) = yi \u00b7 (\u27e8w, xi\u27e9+ b) \u22640\n\u2022 The key idea of the Perceptron algorithm is to improve the current solution\nby \ufb01xing this mistake \u2013 and then the next, until they are all correct\n41\n\nCorrecting Mistakes: The Perceptron Algorithm\n\u2022 Thanks to the de\ufb01nition of a\ufb03ne function we can use a linear separator\nf (x) = \u27e8w\u2032, x\u27e9in n + 1 dimensions that integrates the bias parameter b.\n\u2022 From now on we will call w\u2032 simply w: you can always \ufb01nd what we mean\nfrom the context, a separate bias \u2018+b\u2019 implies the original formulation with\nw in n dimensions; no bias means w is actually w\u2032 in n + 1 dimensions,\nalready including the bias\n\u2022 Assume for now that the algorithm makes its mistake on the example\n(xi, yi) with xi = (1, 0, . . . , 0) (only the \ufb01rst nonzero) and yi = +1.\n\u2022 It holds yi \u00b7 f (xi) = \u27e8w, xi\u27e9= (w)1 \u00b7 (xi)1 \u22640\n(here (xi)1 denotes the \ufb01rst component of the vector xi).\n42\n\nCorrecting Mistakes: The Perceptron Algorithm\n\u2022 To \ufb01x the mistake we have to alter w so that the product with xi increases:\nthis means we have to increase the component (w)1\n\u2022 We also know that we do not need to alter the other components of w, as\nthey were not involved because they are zeros\n\u2022 In the terms for a general vector xi, this means that we have to\nchange w in direction of xi.\nAnalogously, for yi = \u22121 we have to change w in direction of \u2212xi\n\u2022 Perceptron update rule: if the margin of point (xi, yi) is not positive,\nthen update w into w + yi \u00b7 xi\n\u2022 Note that this does not necessarily \ufb01x the mistake, but it improves the\nmargin of example (xi, yi) (by \u2225xi\u22252)\n43\n\n44\n\n45\n\n46\n\n47\n\nCorrecting Mistakes: The Perceptron Algorithm\nThe Perceptron Algorithm\nInitialize(w) with zeros\nsolved \u2190False\nwhile not solved\nforeach point (xi, yi) \u2208D do\nsolved \u2190True\n\u00b5 \u2190yi \u00b7 \u27e8w, xi\u27e9\nif \u00b5 \u22640 then\nw \u2190w + yi \u00b7 xi\nsolved \u2190False\nbreak\nreturn w\n\u2022 The Perceptron algorithm is\nguaranteed to \ufb01nd a linear\nseparation if one exists.\n\u2022 For data that is not linearly\nseparable though, the algorithm will\nnever terminate (always \ufb01nds a new\nmisclassi\ufb01cation).\n\u2022 This is a na\u00efve implementation but\nit is easy to optimize and parallelize\n48\n\nPerceptron algorithm: key properties\n\u2022 The learning procedure is iterative\n\u2022 The model is a linear equation\n\u2022 The solution parametrization is constructed from the training data (in this\ncase as a sum of terms yi \u00b7 xi)\n\u2022 There exists a learning time bound (in the number of points), which means\nthe algorithm is guaranteed to terminate (if it can)\n(this is important in statistical learning theory)\n\u2022 The Perceptron is very easy to implement\nLook out for these characteristics as some will come up again,\nin the implementation of more advanced learning algorithms\n49\n\nSummary\n\u2022 Binary classi\ufb01cation splits the decision space in two parts, attributing a class\nto each\n\u2022 Decision trees o\ufb00er a straightforward and intuitive approach to classi\ufb01cation,\nbut are hard to scale and generalize (this is addressed in more advanced\nimplementations)\n\u2022 Our \ufb01rst learning algorithm is the Perceptron, which learns a linear\nseparation from the data (supervised learning), and can thus be used for\nbinary classi\ufb01cation\n\u2022 This algorithmic approach overcomes the limit of closed forms by pro\ufb01ting\nfrom (rather than being limited by) the availability of large amounts of data\nat (relatively) low cost (\u2192Big Data)\nLet\u2019s implement it in the lab assignment\n50\n\nExtra material\nLinear function vs equation (conversion {x1, x2} \u2192{x, y}):\n\u2022 https://en.wikipedia.org/wiki/Linear_function_(calculus)\n#Relationship_with_linear_equations\nDecision trees:\n\u2022 Full tutorial in Jupyter Notebook:\nhttps://github.com/random-forests/tutorials/blob/\\master/\ndecision_tree.ipynb\n\u2022 Thorough guide (with learning explained) from Scikit-Learn:\nhttps://scikit-learn.org/stable/modules/tree.html\n\u2022 XGBoost sacri\ufb01ces interpretability for performance:\nhttps://en.wikipedia.org/wiki/XGBoost\nPerceptron:\n\u2022 Full derivation (ch. 6):\nhttp://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf\n51\n\n", "04.handling_data.pdf": "04: Handling data\nMachine Learning\nBachelor in Computer Science [SP24]\nMarch 18, 2024\n\nWhere are we\n\u2022 Data allows us to observe processes and deduce their underlaying laws\n\u2022 Machine learning aims at approximating these laws using generic models\n\u2022 Supervised learning allows us to learn these models from (labeled) data\n\u2022 The algorithms seen so far rely on the inherent properties of data, e.g.\noutliers, linear separability, etc.\n\u2022 The data used so far was carefully prepared behind the scenes to work \ufb01ne\nwith our algorithms\nSo far, everything depends on the data.\n1\n\nToday\u2019s menu\n\u2022 From available data to useful data\n\u2022 Visualizing data\n\u2022 Data for Machine Learning\n\u2022 Visualizing (training) performance\n2\n\nScenario\n\u2022 A large store has been collecting sales information for years and never used it\n(quite common since Big Data became trendy)\n\u2022 They now want it to produce value for the company but have no idea how\n(quite common since AI became trendy)\n\u2022 You are hired. What can you do? How?\nExample: what about a model that predicts sales number\nbased on price, for a product? You could then maximize returns!\nBut how do you make it?\n3\n\nConnect the ends\n\u2022 You can model data-generating functions using supervised learning\n\u2022 You know how to use linear regression to predict a continuous value\n\u2022 The data you need is pairs of useful inputs and correct targets\n\u2022 On the other end, you have access to a potentially very large dataset\n\u2022 How do you build the inputs? What constitutes useful targets?\n\u2022 Can you promise a result? Are you con\ufb01dent you can work it out?\n4\n\nData representation\n\u2022 Getting access to company data will typically be your \ufb01rst challenge:\n\u2022 Privacy issues with personal data\n\u2022 Non-disclosure agreements on company secrets\n\u2022 Experts and engineers think AI a fad and minimize \u201cwasted time\u201d\n\u2022 People collecting data may simply not want to share and stonewall you\n\u2022 Never underestimate the human component!\n\u2022 Expect data to be partial, and made available in chunks over time\n\u2022 Chunks may come from di\ufb00erent sources (departments, systems, etc.)\n\u2022 And consequently have di\ufb00erent: format, standard, naming scheme, quality,\ntime scale, noise, error types, etc.\n5\n\nData formats\nfruits_1.csv\ncolor\nsize\n\ufb02avor\nred\n0.2\nsweet\nyellow\n0.4\nsour\ngreen\n0.8\nsweet\nfruits_2.csv\nhue\nweight\ntaste\nred\n23\nsweet\nyellow\n82\nsour\ngreen\n384\nsweet\nSome common formats:\n\u2022 A database with organized tables (unlikely)\n\u2022 One or more CSV \ufb01les with lots of columns, a few duplicated\n\u2022 A folder structure with \ufb01les in a consistent format plus CSV metadata\n(common with images, documents, etc.)\nYou can expect the \u201cbig CSV table\u201d case\nto be most common, or easiest to convert to.\n6\n\nData quality\n\u2022 It is likely that up until now the company never had any return of value on\nstoring data. You may even be the \ufb01rst to ever properly look at it!\n\u2022 This means years of minimal costs and e\ufb00orts: expect the data to be bad.\n\u2022 Common sources:\n\u2022 Computer-generated statistics: this could be the best case, when you\nhave for example price reading and sales amount registered automatically\nwith precision. Expect inconsistency through time and sources though.\n\u2022 Physical sensors: they have limited precision, can spike for external causes,\nand if broken you get holes in the data until \ufb01xed (possibly a long time).\n\u2022 Human inputs: it is still embarrassingly common to (i) task engineers with\nreading analog gauges once a day, (ii) write the reading on a piece of paper,\n(iii) task an intern to spend a day each month to enter the numbers in a\nspreadsheet, and (iv) you get your data as a 5GB Excel \ufb01le.\n7\n\nCommon problems\nWhen dealing with unknown data, start by assuming the worst.\n\u2022 Reading errors: wrong format, unterminated lines, unrecognized character\nset, \ufb01le corruption, etc.\n\u2022 Noise: more or less small variations around the real value, possibly\ndepending on external causes\n\u2022 Missing data: expect holes more or less large: entire columns with little\ndata, or only one (constant) value, sensors broken and never replaced\n\u2022 Clipping: Sensors can read values beyond their operational range, resulting\non non-real readings: {count:\n-1, speed:\n999km/h, temp:\n1e10,\nweight:\n-30, ...}\n\u2022 Special values: for example a missing element could be represented as:\n[NaN, Null, Nil, 0, -1, \"Miss\", \"None\", ...]\n8\n\nCommon problems\n\u2022 Redundancy: large quantities of constant or repeating data, with little to\nno variation. Increases data size but not information quantity.\n\u2022 Human errors: expectation: the number 0; reality: [\u2018Zero\u2019, \u2018zerr\u2019,\n0.00000001, 1e-20, \u2018z\u2019, -1, NaN, ...]\n\u2022 Wrong type: if you read a set of \ufb02oats, and at some point you have a\nmissing value represented by the string \u201cNone\u201d, the whole column will be\nread as strings\n\u2022 Assumptions: they get you every time. For example you could assume that\nthis list is complete :) don\u2019t! Data errors always \ufb01nd a way to surprise you.\nEven unlikely errors, say with only a 0.01% probability, are a certainty when you\nhave a few million lines with hundreds of columns. Handling error becomes\nquickly unmanageable by hand: you need to automate the cleaning process.\n9\n\nSpot the problems\ncolor\nweight\nsize\ntaste\nprice\nred\n23\n0.2\nsweet\n10\nyellow\n82\n0.4\nsour\n7\ngreen\n384\n0.8\nsweet\n8\nred\n999\n0.3\nsweet\n12\nyellow\n75\n0.3\nsour\n0\ngreen\n380\n0.9\nsour\n7\nredd\n25\n0.2\nsweet\n11\nyellow\n82\n0.4\nsour\n7\ngreen\n352\n0.7\nsweet\n10\n\u2022 Missing value (price)\n\u2022 Typo (color)\n\u2022 Clipping (weight)\n\u2022 Low precision\n\u2022 Redundancy (repetition)\n\u2022 Input error (sour melon?)\n10\n\nMitigation: why\nFACT: incorrect data trains incorrect models.\n(typically stylized as: garbage in, garbage out)\n\u2022 But isn\u2019t ML robust to noise? Sure: Linear Regression is robust to the\nGaussian noise, but... what about outliers?\n\u2022 Data defects are \u201cmitigated\u201d, not \u201csolved\u201d, because you can only work with\nwhat you have. You can remove data (e.g. drop uninformative parts), or\ncreate new data (as variation from what you have), but you cannot create\nnew information. (Think: it would require an ideal model already available!)\n\u2022 Every method has a threshold for what is \u201ctoo much noise\u201d, beyond which it\ncannot distinguish the underlying patterns from the disturbance.\n(check out signal to noise ratio).\nNo method always works, and no cleaning produces perfect data.\nAlways prepare your data to the best of your abilities, and\nhope (or better: plan!) for the ML to handle the rest.\n11\n\nMitigation: data cleaning\n\u2022 Fundamentally, two possible approaches: drop or \ufb01x.\n\u2022 Drop: discard all lines (and/or columns) in your dataset with clearly\ndefective data: lines that generate reading errors, out-of-bound readings,\nmissing values, data from foreign processes, etc.\n\u2022 Still, track the data dropped: what if it constitutes the sole representative of\na crucial sub-process? You may need to \ufb01x it instead\n\u2022 Fix: get a rough estimate (e.g. average surrounding data), then treat it as\nthe exact value plus noise . Literature methods for denoising, imputation,\nprediction, etc. in general all apply.\n\u2022 BEWARE: do not underestimate \u201cdata cleaning\u201d and \u201cdata analysis\u201d!\nData preparation tends to take the 50%-80% of time/e\ufb00ort in most projects!\n(Consequently, most \u201cMachine Learning\u201d job positions in new labs/teams, will make you\nspend up to 80% of your time cleaning data, with no results to show for it. Beware!)\n12\n\nData analysis\n\u2022 Once the data is successfully loaded and can be processed, data analysis\nguides the project development.\n\u2022 Not a \ufb01xed method but an iterative process:\n(i) analyze the data, (ii) \ufb01nd out what the next most important problem is,\n(iii) address it, (iv) run the analysis again.\n\u2022 Some important questions: (i) Is the remaining data still representative of\nthe process of interest? (ii) Are features correlated? (iii) Is the data ready\nfor learning? (iv) What is easy/hard to learn? (v) Which model and learning\nmethod will be most promising? Etc.\n\u2022 Iterative process: the answers will change as you clean and manipulate the\ndata. (Like with the Perceptron)\n\u2022 Eventually you will gain a fundamental sense of the data: understanding\nof its values and quirks, and the most promising approach to optimizing the\nperformance of your methods.\n13\n\nData preparation pipeline\n[Cuccu et al., 2017]\n14\n\nStatistical representativeness\n\u2022 The data available can in principle represent several iteration of many\ndi\ufb00erent processes.\n\u2022 We call a sample statistically representative if its statistical properties\ncorrespond to those of the full available data for the process of interest.\nFrom a statistical perspective: the sample mirrors the population.\n\u2022 This means that redundant and extraneous data can be ditched without\nconsequences for the learning process.\n\u2022 Training on redundant data has the same (lack of) e\ufb00ect of running further\niterations after the learning already converged: there is no learning potential.\nIf I have 1000kg of \ufb02our, 1000L of milk, and 5 eggs,\nyou can only bake 5 eggs worth of cake.\n15\n\nStatistical representativeness\n\u2022 Too much data:\n\u2022 Attacking a big dataset with big methods from the beginning is needlessly\ntime consuming and highly ine\ufb03cient.\n\u2022 A carefully designed representative subsample will show meaningful results\nin a fraction of the time with most methods, allowing for a quicker feedback\nfrom performance analysis.\n\u2022 Rule of thumb: once the full range of values of all features related to the\nobjective process are represented, adding more data with similar values\n(within noise range) is unlikely to lead to performance improvements.\n\u2022 Not enough data:\n\u2022 On the other hand: you should question whether the data available does\nproperly describe the process of interest at all!\n\u2022 If not, then there is nothing you can do \u2014 besides, of course, going back\nto data collection, and designing a more proper experiment setup.\n(ML is not magic! Which seems to be not so obvious to many people!)\n16\n\nData visualization and statistics\n\u2022 Visualizing feature relationships and aggregated statistics is crucial to\nunderstand and direct the e\ufb00orts in data-centric applications.\n\u2022 Humans understand shapes, sizes and colors much more quickly (and\nthoroughly) than raw numbers.\n\u2022 We are also much more e\ufb03cient at understanding 2D images rather than 3D\nor higher dimensions. Avoid 3D plots unless the added value is indisputable\nand unavoidable (very rare). Absolutely do not to it for showmanship.\n\u2022 Whenever possible opt instead for representing the extra features using\nshapes, colors, size or transparency rather than extra axis.\n\u2022 If you have too many features you can start with a subset, especially if you\nalready have a target (e.g. for each store item: sales per price)\n17\n\nFruits data (\ufb01xed)\nAnswer quick: how many fruit types?\ncolor\nweight\nsize\ntaste\nprice\nred\n23\n0.2\nsweet\n10\nyellow\n82\n0.4\nsour\n6\ngreen\n384\n0.8\nsweet\n8\nred\n40\n0.3\nsweet\n12\nyellow\n75\n0.3\nsour\n8\ngreen\n380\n0.9\nsweet\n7\nred\n25\n0.2\nsweet\n9\nyellow\n87\n0.5\nsour\n7\ngreen\n352\n0.7\nsweet\n10\n0.2\n0.4\n0.6\n0.8\nsize\n100\n200\n300\n400\nweight\ncolor\nred\nyellow\ngreen\nprice\n6\n8\n10\n12\ntaste\nsweet\nsour\n18\n\nUseful statistics\n\u2022 Mean and variance: most basic descriptors for a feature\nMedian and mode also insightful, especially with asymmetric distributions\n\u2022 Correlation: matrix describing how features change together\n\u2022 Binning / tallying: partition the feature\u2019s value range and count the\nnumber of occurrences in each \u201cbin\u201d: insight about the data distribution\nfrom a discretized perspective\n\u2022 Defect statistics: counting the amount of defects (e.g. missing values) per\neach feature, and correlating the number against the actual values of\ndi\ufb00erent features, helps understanding the cause of the defect\n\u2022 Time regularization: if data is cyclical over time, aggregating data over its\nperiod (e.g. averaging temperature every 24h cycles) isolates its patterns\n19\n\nUseful statistics\n20\n\nUseful plots\nA few plots are most commonly used for their readability and insight:\n\u2022 Scatterplot: plot numerical features as points on axis. Can be augmented\nby connecting lines, or overlaying line plots of data statistics.\n\u2022 Line plot: highlights continuity along the input axis (x). For example,\nvisualize the decision boundary of a trained model.\n\u2022 Boxplot: plot the mean of a numeric feature, with error bars giving insight\nabout its variability. Can be enhanced to highlight outlier information.\n\u2022 Histogram: plots feature values as adjacent columns of proportional height.\nParticularly useful to compare quantities.\n\u2022 Heatmap: plots a grid where neighboring squares gives continuity while\nsquare color portrays numerical values. It provides a quick way to visualize\ne.g. covariance or correlation.\n21\n\nExample of scatterplot and lineplot\n22\n\nExploring the Iris dataset\n23\n\nScatterplot of the Iris dataset\n24\n\nPairplot Iris dataset\n25\nType text here\n\nPairplot Iris dataset\n26\n\nIris dataset statistics\n27\n\nBoxplot (w/ bonus: violinplot)\n28\n\nHistogram and heatmap\n29\n\nImportant tips\n\u2022 No free lunch: deciding a preparation pipeline uniquely depends on the\ndata at hand, and cannot be entirely automated (so far: see AutoML).\n\u2022 Sometime less is better: data quality impacts the learning process much\nmore than data quantity.\n\u2022 Representativeness: aim for a clean subset that is statistically\nrepresentative of the target process.\n\u2022 Ranges and normalization: the model is optimized for inputs in a certain\nrange: it is your responsibility to make sure your data complies\n(normalization).\n\u2022 Categorical variables: your model is a mathematical equation, so\ncategorical inputs (strings) need to be transformed into numbers (or\nvectors).\n30\n\nHandling categorical variables\nCategorical variables need converting into real numbered vectors in for use in\nyour models. For example, let\u2019s hypothesize a feature color \u2208{red, green,\nyellow}; then for a red fruit:\n\u2022 One-hot encoding: map the feature to a binary vectors with all zeros but\nfor the point\u2019s value, here red 7\u2192[1,0,0]. Raises the dimensionality of\nthe input (1 to 3), but distinguishes more sharply between values.\nWorks well e.g. for Neural Networks (Lecture 09).\n\u2022 Binary encoding: take one-hot encoding, treat as binary number, then\nconvert to decimal, here red 7\u2192b100 \u21924. The distinction is less stark\nbut becomes continuous and the input size does not grow.\nWorks well for e.g. SVMs (Lecture 06).\nThen you can correspondingly map green \u2192010 and yellow \u2192001.\n31\n\nHandling categorical variables\n\u2022 Embeddings map the feature to an arbitrary high-dimensional space using\nan arbitrary similarity metric on your data, here\nred 7\u2192[1.3, -7.2, . . . , 2.4] \u2208Rq, q >\n> 3.\n\u2022 For example it is common to use n-grams to create an embedding for\ntextual data: map each word to a vector, then the vectors of words which\nappear close in the text (2-gram is a distance of 2 words) are changed to be\ncloser in vector space. (in e.g. word2vec typically q = 300)\n\u2022 This trick is akin to kernelization (Lecture 07), and it greatly enhances data\ndistinction, but also data dimensionality. It is commonly applied to raw\nfeatureless data such as text.\n\u2022 Advanced preprocessing may be required, such as stemming, where words in\na text are put together based on their root.\n{red, reddening, reddest} \u2192\u201cred\u201d 7\u2192[1.3, -7.2, . . . , 2.4]\nWe will see more in the following lectures,\nfor now just let the concept start to sink in.\n32\n\nModel performance: statistical validation\n\u2022 How can we check if the training is \ufb01nished?\nDid it succeed? Is the model useful?\n\u2022 So far we only plotted the model against the same data we trained it from.\n\u2022 To be useful though we should validate its applicability on previously unseen\ndata, i.e. its ability to generalize, without over- or under\ufb01t.\n\u2022 The standard way is to split the available data in two: the training data,\nwhich is used for training, and the test data, which is uniquely used to\nverify the performance of a trained model.\n\u2022 The model is \ufb01rst trained using the training data; then its performance is\nevaluated on the (so far unseen) test data, which simulates an actual\ndeployment, and measures the model\u2019s practical utility.\n\u2022 A typical data split is 80% or 90% for the training (and 20% or 10% for test\nrespectively), but opinions vary wildly.\n33\n\nCross-validation\n\u2022 The next step is to verify if the training algorithm is reliable, i.e. statistically\nrobust to variations in data and initial conditions\n\u2022 We need to execute multiple runs (training from scratch) then average the\nperformance of the resulting models.\n\u2022 Cross-validation (n-fold: partition the training data from before into n\npartitions (or folds); for each run, one partition is selected as the validation\nset, while the others joined are the training set.\n\u2022 This creates n (slightly: 1/n) di\ufb00erent training sets, with n validation sets.\nThe model can be trained n\n\u2022 Common choices for n are 5 and 10, corresponding again to 80-20 and\n90-10 splits.\n\u2022 Careful with these (yet again) confusing naming conventions! In the old\nliterature the validation set is used to test the algorithm performence; in\nmodern applications it is used to test hyperparameter optimization and stop\ntraining before over\ufb01tting, which is why the test set is still separated!\n34\n\nModel outputs as data: visualizing performance\n\u2022 We saw how plotting data helps understanding the underlying patterns.\n\u2022 This is valid also for the data produced by the model, i.e. its predictions.\n\u2022 Visualization is a powerful tool to understand, correct and improve the\nmodel\u2019s and learning algorithm\u2019s settings, much more so than bare numbers.\nA few useful plots:\n\u2022 Predictions over data\n\u2022 Loss over algorithm iterations\n\u2022 Predictions over observations\n\u2022 Errors over predictions\n35\n\nPredictions over data\n36\n\nLoss over algorithm iterations\n[Cuccu et al., 2020]\n37\nOverfitting\n\nPredictions over observations\n[Cuccu et al., 2017]\n38\n\nErrors over predictions\n[Cuccu et al., 2017]\n39\n\nSummary\n\u2022 Applying ML to the real world requires \ufb01rst to adapt the data\nto a \ufb01t format\n\u2022 Data analysis provides tools to highlight patterns and mitigate\ndata defects\n\u2022 Visualization is a powerful tool to understanding data features\nand simple patterns\n\u2022 Performance analysis helps tracking the learning process and\nvalidate its results\n40\n\nExtra material\n\u2022 Modern plotting with Python (check out the interactive plots!): https:\n//towardsdatascience.com/plotting-with-python-c2561b8c0f1f\n\u2022 Understanding indexing with Pandas: https://medium.com/dunder-\ndata/selecting-subsets-of-data-in\\-pandas-6fcd0170be9c\n\u2022 Explore, clean and transform data with OpenRe\ufb01ne:\nhttps://openrefine.org/\n\u2022 Signal to noise ratio:\nhttps://en.wikipedia.org/wiki/Signal-to-noise_ratio\n\u2022 Pandas cheatsheet:\nhttps://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n(CAREFUL: we do not access data as df.feature_name!)\n\u2022 More on encodings: https://towardsdatascience.com/smarter-ways-\nto-encode-\\categorical-data-for-machine-learning-part-1-of-\n3-6dca2f71b159\n41\n\n", "06.svm_supplement.pdf": "06: SVM Supplement\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 08, 2024\n\nHere is a question I receive most years, in one form or another:\nHow is it possible that the margin of the support vectors is always one?\nGeometrically, the margin is a distance, so if we have two classes that are far\nfrom each other (with the boundary centered right between) then the nearest\ndata points from the boundary would still be quite far away. How comes that the\nsupport vectors have the margin always equal to 1?\nTypically my answer was: your doubts are in the context, you should have a look\nat the full derivation from the Stanford lectures. This topic is explained\nparticularly clearly. But if it is still unclear, here\u2019s a full rundown.\nI reused material from the tutorial below, including some of their images. It is\nsimpler than the full Stanford derivation, but not really shorter. You cannot\nsimplify this topic beyond a certain point, and you eventually need to understand\nthe math even if you have a geometrical intuition.\nhttps://sustech-cs-courses.github.io/IDA/materials/\nClassification/SVM_tutorial.pdf\n1\n\nAn hyperplane \u03c0 in d dimensions is de\ufb01ned by parameters w, b (with w \u2208Rd\nvector and b \u2208Rscalar) , and the equation \u27e8w, x\u27e9+ b . Vector w is called the\nnormal vector of the hyperplane, as it is perpendicular (normal, orthogonal) to the\nhyperplane (and all vectors parallel to the hyperplane). Scalar b is the intercept\nof the hyperplane. This means that the distance between the hyperplane and the\norigin of the space (coordinates 0) is b/||w|| (remember this).\n2\n\nThe maximum margin distance is de\ufb01ned as the shortest line segment connecting\nthe two classes (let\u2019s call them C1 and C2), which in the slides was the channel\nin dashed lines. These dashed lines are parallel by construction, so the shortest\nsegment between the two is on a line perpendicular to them. The decision\nboundary is by construction centered in this channel, and thus also parallel to\nthem and perpendicular to the maximum margin distance segment. In the image\nbelow, this distance is d (we used \u03b3 in the slides).\n3\n\nIf we multiply the hyperplane parameters w, b by a nonzero scalar c, then cw, cb\nstill de\ufb01nes the same hyperplane. Go ahead, try on paper with a line. This means\nthat we have a choice on which w, b to use, so we choose w, b such that\n||w|| = 1 . If you write out the math of the norm, you will see that this means\nthat we are choosing the w for which, if you sum all the element squared, the\ntotal is 1, which also means that the dot product with itself is 1: \u27e8wT, w\u27e9= 1 .\nIf ||w|| = 1, then the distance between an example x (which is a vector in the\nspace) and the decision boundary hyperplane \u03c0 is denoted as: \u03c1(x, \u03c0) .\nNote that this is a signed distance, meaning it is a distance over a direction and\nwith an orientation. Note that \u03c1(x, \u03c0) > 0 in the positive half-space, which is\nwhen x \u2208(Rn)+\n\u03c0 , and \u03c1(x, \u03c0) < 0 when x \u2208(Rn)\u2212\n\u03c0 . And as you would expect\n\u03c1(x, \u03c0) = 0 when x \u2208\u03c0, this is the same trick that we are using already for\nclassi\ufb01cation since the beginning.\nBut then, if ||w|| = 1, then the distance of the point from the hyperplane is\nsimply \u03c1(x, \u03c0) = \u27e8w, x\u27e9+ b .\n4\n\nTwo classes are formally called linearly separable if there exists at least one\nhyperplane that separates them exactly. If hyperplane \u03c0(w, b) = \u27e8w, x\u27e9+ b\nseparate class C1 and C2, then the decision function that we use for classi\ufb01cation\nis de\ufb01ned as:\nf (x) = sign(\u27e8w, x\u27e9+ b) =\n(\n1, if \u27e8w, x\u27e9+ b \u22650\n\u22121, if \u27e8w, x\u27e9+ b < 0\nThis means that we need to \ufb01nd parameters w, b of the hyperplane, which in\nSVM is done by solving the following optimization problem:\nargmin\nw,b\n\u001a1\n2||w||2\n\u001b\nsubject to\n(\n\u27e8w, x\u27e9+ b \u22651, \u2200x \u2208C1\n\u27e8w, x\u27e9+ b \u2264\u22121, \u2200x \u2208C2\n5\n\nargmin\nw,b\n\u001a1\n2||w||2\n\u001b\nsubject to\n(\n\u27e8w, x\u27e9+ b \u22651, \u2200x \u2208C1\n\u27e8w, x\u27e9+ b \u2264\u22121, \u2200x \u2208C2\nin coordinates form, as you saw it sometimes in the lecture, this is written as:\nargmin\nw,b\n(\n1\n2\nn\n\u2211\nk=1\nw2\nk\n)\nsuch that\n(\n\u2211n\nk=1 wkxk + b \u22651, \u2200x \u2208C1\n\u2211n\nk=1 wkxk + b \u2264\u22121, \u2200x \u2208C2\nWhere xk are training vectors (examples from the training dataset). Next we use\nmonotonic transformations, which change the equation while preserving the\norder: minimizing 1/2||w||2 is equivalent to minimizing ||w||, the same w is the\nsolution to both minimization problems. This allows us to choose any form of the\nequation, just as long as the solution is guaranteed to be the same for the\noriginal problem. We can also switch between minimization and maximization:\nminimizing ||w|| is equivalent to maximizing 1/||w||.\n6\n\nWith that in mind, now we can rewrite the optimization problem as:\nargmax\nw,b\n\u001a\n1\n||w||\n\u001b\nsubject to\n(\n\u27e8w, x\u27e9+ b \u22651, \u2200x \u2208C1\n\u27e8w, x\u27e9+ b \u2264\u22121, \u2200x \u2208C2\nWe can divide both constraints by the same amount without changing the\nsolution, so we divide by ||w||:\nargmax\nw,b\n\u001a\n1\n||w||\n\u001b\nsubject to\n\uf8f1\n\uf8f2\n\uf8f3\n\u27e8w,x\u27e9+b\n||w||\n\u2265\n1\n||w||, \u2200x \u2208C1\n\u27e8w,x\u27e9+b\n||w||\n\u2264\u2212\n1\n||w||, \u2200x \u2208C2\nThis is just another intermediate step: as long as it\u2019s valid, accept it and the\nreason will only be clear by the end of it. These steps are always designed with a\ngoal in mind, but until you see the goal it may seem unreasonable to divide\neverything by ||w||. It does not matter as long as it is correct, enjoy the ride and\nlook forward to the \ufb01nal form.\n7\n\nNow we got to:\nargmax\nw,b\n\u001a\n1\n||w||\n\u001b\nsubject to\n\uf8f1\n\uf8f2\n\uf8f3\n\u27e8w,x\u27e9+b\n||w||\n\u2265\n1\n||w||, \u2200x \u2208C1\n\u27e8w,x\u27e9+b\n||w||\n\u2264\u2212\n1\n||w||, \u2200x \u2208C2\nWith this form, now the distance between the decision boundary hyperplane \u03c0\nand any point x (example, vector) can be calculated as \u27e8w, x\u27e9+ b/||w|| If we\ndenote this distance as \u03b3 = 1/||w||, the optimization problem can be written as:\nargmax\nw,b {\u03b3}\nsubject to\n(\n\u03c1(\u03c0, x) \u2265\u03b3, \u2200x \u2208C1\n\u03c1(\u03c0, x) \u2264\u2212\u03b3, \u2200x \u2208C2\nSolving this optimization problem you obtain the parameters w, b that maximizes\nthe margin size 2\u03b3 = 2/||w|| between C1 and C2.\nHow do we solve it? In ML you have solved similar problems with Random\nGuessing (actually even this exact problem), which may give you an idea.\nQuadratic Programming addresses the same goal but without the randomness.\n8\n\n", "03.linear_regression_and_lda.pdf": "03: Linear Regression and \nDiscriminant Analysis\nProf. Philippe Cudr\u00e9-Mauroux\nslides by Dr. Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP24]\nMarch 11, 2024\n\nWhere are we\n\u2022 Supervised learning\n\u2022 Classi\ufb01cation\n\u2022 Linear separability\n\u2022 Decision trees\n\u2022 Perceptron algorithm\n1\n\nToday\u2019s menu\n\u2022 Statistical data analysis\n\u2022 Linear regression\n\u2022 Linear discriminant analysis\n2\n\nDid you refresh these topics?\n\u2022 Probability distribution (P) vs probability density (p)\n\u2022 Random variables\n\u2022 Descriptive statistics\n\u2022 Derivatives\n3\n\nClassical Statistical Data Analysis?\n\u2022 Method names in ML often refer to model and learning algorithm together\n(for historical reasons)\n\u2022 \u201cPerceptron\u201d really refers the linear model plus the learning algorithm,\nbecause it depends on the model being linear. It was invented in the late 50s\nand still constitutes the foundation for today\u2019s Arti\ufb01cial Neural Networks\n\u2022 There are other, even older methods from the classical statistics literature for\n\ufb01nding linear models given data. These have traditionally being separated\nfrom \u201cmachine learning\u201d and classi\ufb01ed instead as part of \u201cdata analysis\u201d\n\u2022 Nonetheless, ML is founded on these studies, and methods like linear\nregression and linear discriminant analysis (LDA) are still used today\nespecially to provide baselines on complex data\n\u2022 This course aims at a deeper understanding of the concept of modeling\nregardless of the method\u2019s history; we consider data analysis as an integral,\ncrucial step to enable learning.\n4\n\nClassical Statistical Data Analysis!\n\u2022 Statistical data analysis models (the function underlying the) data with a\nhypothesis h : X \u2192Y , as we saw before. The main di\u02d9erences are (i) the\nunderlying function is treated as a probability distribution, and (ii) it focuses\non the available data, i.e. it does not concern itself with the predictions of\nunseen points\n\u2022 Example: assume we have measurements xi \u2208R, measuring the diameter of\nan apple (centimeters), and yi \u2208R of the weight of the apple (grams)\n(Note all measurements have limited precision, even though we say xi \u2208R)\n\u2022 We say that we infer a function h : R \u2192R, which maps the diameter xi to\nthe weight yi, by introducing a probability model of their relation.\n\u2022 This model can be written as yi = h\u03b8(xi) + \u03f5i:\n\u2022 \u03b8 is its parametrization\n\u2022 \u03f5i models the noise (typically i.i.d. random variable with zero mean)\n\u2022 The goal of statistical inference is to derive (\u201clearn!\u201d) the parameters \u03b8\nfrom the data\n5\n\nLinear Regression\n\u2022 Linear regression is a special case. We use a\ufb03ne linear functions\nhw(x) = \u27e8w, x\u27e9to model the data center, as well as Gaussian noise\n\u03f5 \u223cN (0, \u03c32)\n\u2022 This results in interpreting the underlying function as a (Gaussian)\nprobability distribution:\nyi \u223cN\n\u0000\u27e8w, xi\u27e9, \u03c32\u0001 = \u27e8w, xi\u27e9+ \u03f5i\nfor unknown parametrization w and \u03c3\n\u2022 The label (target) is then interpreted as a random variable drawn from this\ndistribution\n\u2022 This generalizes to multiple outputs (i.e. vector-valued, high-dimensional\noutput) by applying the same algorithm to each output component\nindependently\n6\n\nOptimizing the parameters with the log likelihood\nTo \ufb01nd the parameter w, linear regression uses standard log likelihood\nmaximization. The likelihood function is based on the de\ufb01nition of goodness of\n\ufb01t, itself being a standard statistical measure of how well a probability\ndistribution \ufb01ts a set of observations\np(Y |x1, . . . , xn) =\nn\n\u220f\ni=1\np(yi | xi)\nThe product is harder to maximize than e.g. a sum, so it is common to maximize\ninstead the log likelihood: just compute the logarithm of the product above.\nSubstituting the equation for the Gaussian density for each p(yi | xi) we obtain:\nlog\n\u0000p(Y | x1, . . . , xn)\n\u0001= n \u00b7 log\n \u2212\n1\n2\u03c32\nn\n\u2211\ni=1\n(yi \u2212\u27e8w, xi\u27e9)2\n7\n\nOptimizing the parameters with the log likelihood\nlog likelihood = \nCheck the equation again. The only term that uses w (depends on w) is the last\none, the others will remain constant quantities even if we change it, so we can\nexclude them from the maximization:\nargmax\nw {log likelihood} = argmax\nw\n(\n\u2212\nn\n\u2211\ni=1\n(yi \u2212\u27e8w, xi\u27e9)2\n)\n\u2022 Very important: we have dropped \u03c3 out of the equation!\n\u2022 Maximizing a negative value is equivalent to minimizing its magnitude\n8\nn \u00b7 log\n \u2212\n1 \u2211\n2\u03c32\ni=1\n(yi \u2212\u27e8w, xi\u27e9)2\n\nOptimizing the parameters as least squares\nIt turns out that maximizing the (log) likelihood w.r.t. w is equivalent to\nminimizing the Risk with a squared errors Loss, which is a relatively common\nchoice (because it is de\ufb01nite positive and it highlights outliers):\nE =\nn\n\u2211\ni=1\n(yi \u2212\u27e8w, xi\u27e9)2\nargmax\nw {log likelihood} = argmin\nw {E}\nThe technique of minimizing the squared error is known as the least squares\nmethod\nFor Linear Regression, maximum likelihood estimation\nand least squares are equivalent\n9\n\nLeast squares on w\nTo calculate the minimizer we set the\nderivative of E to zero\nE =\nn\n\u2211\ni=1\n(yi \u2212\u27e8w, xi\u27e9)2\n\u2202E\n\u2202w = 0\n\u21d4\u22122 \u00b7\nn\n\u2211\ni=1\n(yi \u2212wTxi) \u00b7 xT\ni\n= 0\nn\n\u2211\ni=1\nyi \u00b7 xT\ni\n= wT\nn\n\u2211\ni=1\nxixT\ni\nIt is simpler if we switch to matrix\nnotation, with\n\u2022 y \u2208Rn vector of targets for each\nof the n inputs {y1, . . . , yn}\n\u2022 X \u2208Rn\u00d7p input data matrix,\nhaving inputs {x1, . . . , xn} as rows\nThen we can solve for w:\nyTX = wTXTX\n\u21d4XTy = XTXw\nw = (XTX)\u22121XTy\n10\n\nfw(x) that minimizes the sum of squared errors\n11\n\nfw(x) that minimizes the sum of squared errors\n12\n\nModel is linear, outliers\u2019 errors are squared\n13\n\nModel is linear, outliers\u2019 errors are squared\n14\n\nHypothesis is linear, passes through center of mass\n15\n\nHypothesis is linear, passes through center of mass\n16\n\nOutliers from data error can really stretch the model\n17\n\nOutliers from data error can really stretch the model\n18\n\nLinear Regression recap\n\u2022 Linear regression \ufb01ts a linear model h : X \u2192Y based on labeled data\n\u2022 The model is found through likelihood maximization, which for this\nparticular algorithm corresponds to the least squares optimization\n\u2022 The solution is obtained as a closed-form function of the data:\nh(x) = \u27e8w, x\u27e9;\nw = (XTX)\u22121XTy\nThis is not an iterative process, but a closed form\n\u2022 Important: when implementing these equations (at the lab), remember that\nthey are a\ufb03ne functions, and both w and X must include the bias\n\u2022 The learned model should work \ufb01ne if the relation of inputs to labels is\nindeed (approximately) linear.\nCareful because linear regression is very sensitive to outliers!\n19\n\nLimitations of linear regression\n\u2022 Real data is most commonly nonlinear\n\u2022 Local approximation: any nonlinear function can be approximated to\narbitrary precision by a linear model in an arbitrarily small window\n\u2022 Linear models can return sensible predictions on nonlinear data because of\nlocal approximation\n\u2022 Linear regression though is computed as a closed form on a dataset\n\u2022 Split the data into windows and you could work it out to make sense with\nmultiple models, one for each window\n\u2022 Run it on data large enough though, and you are almost guaranteed a bad\npredictor \u2013 even on linear data (when noisy)\n20\n\nLinear Discriminant Analysis (LDA)\n\u2022 Linear discriminant analysis (LDA) is a classi\ufb01cation method derived\nfrom linear regression.\n\u2022 Note: there are many variants of this method under the same name. As\nusual, learn the concepts and be \ufb02exible with the naming.\n\u2022 LDA interprets the input xi of each example (xi, yi) as a noisy observation\nof \u00b5yi, a true but unknown prototype of class yi.\n\u2022 For each point it holds that xi = \u00b5yi + \u03f5i, with \u03f5i \u2208Rp being the speci\ufb01c\nnoise/variation for each example.\n21\n\nxi = \u00b5yi + \u03f5i\n22\n\nInterpreting data as random variables\n\u2022 LDA models the noise terms as i.i.d. Gaussian random variables\n\u03f5 \u223cN (0, \u03a3), with zero mean and a covariance matrix \u03a3 \u2208Rp\u00d7p that is\nthe same for all classes (class-independent).\n\u2022 The class-conditional distributions P(x | y) are therefore Gaussians\nN (\u00b5y, \u03a3).\n\u2022 A model trained with LDA maps each input to the (class of the) distribution\nthat most likely generated it, by maximizing the conditional probability\nargmaxy {p(y | x)}.\n\u2022 To \ufb01nd this quantity from the data we use Bayes\u2019 rule:\np(y | x) = p(x | y) p(y)\np(x)\n23\n\nEstimating the probability terms from data\np(y | x) = p(x | y) p(y)\np(x)\n\u2022 The distribution that generates the data given the label p(x | y) can be\nwritten using the equation of the Gaussian density (see math recap) with\nmean \u00b5y and covariance \u03a3.\n\u2022 The probability of each label p(y) is called prior probability, and is de\ufb01ned\nas the probability of observing an example of class y regardless (irrespective,\nindependently) of the observed value of x.\nIt is typically denoted by \u03c0y = Pr(y).\n\u2022 The probability of any input p(x) is independent of the class y. Since we\nwill later take the argmaxy of this quantity, this term (which does not\ndepend on y) will disappear. So it can be safely ignored.\n\u2022 The quantity p(x | y) p(y) is called joint probability of x and y, and is\ndenoted as p(x, y).\n24\n\nEstimating the probability terms from data\n\u2022 Putting everything together we obtain:\np(x, y) = p(x | y)p(y) = N (\u00b5y, \u03a3) Pr(y) =\n1\np\n(2\u03c0)p \u00b7 det(\u03a3) \u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u00b5y)T \u03a3\u22121(x \u2212\u00b5y)\n\u0013\n\u00b7 \u03c0y\n\u2022 This approximates the data generating distribution using a model built\naround a set of Gaussians: we have one of these equations for each class\n(as the terms depending on y change with the label).\n\u2022 The terms of the parametrization \u03b8y = (\u00b5y, \u03a3, \u03c0y) are respectively the class\ncenters \u00b5y (alternative names: prototypes, means, centroids), the covariance\nmatrix \u03a3, and the class priors \u03c0y.\n\u2022 To predict the class for input \u02c6\nx \u2208Rp: \ufb01rst compute the joint probability\np(\u02c6\nx, yj) for every class yj \u2208(1, . . . , m), then use the maximum likelihood\nto \ufb01nd which distribution would have been the most likely to generate \u02c6\nx.\n25\n\nEstimating LDA parameters\n\u2022 The \ufb01rst step is to estimate parameters \u00b5, \u03a3 and \u03c0 from the data.\nTurns out this is rather straightforward: the following estimates are used to\nindependently produce a Gaussian for each of the classes.\n\u2022 Class priors \u03c0y. The class frequencies are unbiased estimates, so we just\nuse the class proportions:\n\u02c6\n\u03c0y = ny\nn\nwith ny = |{(xi, yi) | yi = y}| denoting the number of training examples\navailable for class y.\n\u2022 Class centers \u00b5y: just average the inputs for each class to produce\nclass-wise means:\n\u02c6\n\u00b5y = 1\nny\nn\n\u2211\ni=1\nxi\n26\n\nEstimating LDA parameters\n\u2022 Covariance matrix \u03a3: for a Gaussian with known mean \u00b5 we can use the\nempirical covariance or scatter matrix 1\nn \u2211n\ni=1(xi \u2212\u00b5)(xi \u2212\u00b5)T, which can\nbe interpreted as the average \u201csquared distance\u201d of each point from its\nclass-mean.\n\u2022 For a mixture of Gaussians with class-wise center estimates \u02c6\n\u00b5y and uniform\ncovariance matrix this corresponds to:\n\u02c6\n\u03a3 = 1\nn\nn\n\u2211\ni=1\n(xi \u2212\u02c6\n\u00b5yi)(xi \u2212\u02c6\n\u00b5yi)T\nThe training of an LDA model is nothing more than estimating the parameters.\nSame as linear regression, all parameters are\nclosed-form functions of the data (again, not iterative)\n27\n\nUsing the trained model for classi\ufb01cation\nNow we have a trained model. How does LDA classify a new input x?\nWe use x to predict \u02c6\ny = argmaxy {p(x, y)}:\nargmax\ny\n\uf8f1\n\uf8f2\n\uf8f3\n1\nq\n(2\u03c0)d \u00b7 det( \u02c6\n\u03a3)\n\u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u02c6\n\u00b5y)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b5y)\n\u0013\n\u00b7 \u02c6\n\u03c0y\n\uf8fc\n\uf8fd\n\uf8fe\nNote that the \ufb01rst term does not depend on y, so we can safely ignore it\nThe argmax basically compares each pair of classes:\nLet\u2019s take Y = {A, B}, LDA decides for class A if:\n\u02c6\n\u03c0A \u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u02c6\n\u00b5A)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b5A)\n\u0013\n> \u02c6\n\u03c0B \u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u02c6\n\u00b5B)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b5B)\n\u0013\n28\n\n\u21d4\nlog( \u02c6\n\u03c01) \u22121\n2(x \u2212\u02c6\n\u00b51)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b51)\n> log( \u02c6\n\u03c02) \u22121\n2(x \u2212\u02c6\n\u00b52)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b52)\n\u21d4\n(x \u2212\u02c6\n\u00b52)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b52) \u2212(x \u2212\u02c6\n\u00b51)T \u02c6\n\u03a3\u22121(x \u2212\u02c6\n\u00b51)\n+2 log( \u02c6\n\u03c01) \u22122 log( \u02c6\n\u03c02) > 0\n\u21d4\nxT \u02c6\n\u03a3\u22121x \u22122xT \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 + \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u2212xT \u02c6\n\u03a3\u22121x\n+2xT \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 \u2212\u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 + 2 log( \u02c6\n\u03c01) \u22122 log( \u02c6\n\u03c02) > 0\n\u21d4\n2xT \u02c6\n\u03a3\u22121( \u02c6\n\u00b51 \u2212\u02c6\n\u00b52) + \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u2212\u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51\n+2 log( \u02c6\n\u03c01) \u22122 log( \u02c6\n\u03c02) > 0\n\u21d4\nxT \u0000 \u02c6\n\u03a3\u22121( \u02c6\n\u00b51 \u2212\u02c6\n\u00b52)\n\u0001\n+\n\u00141\n2 \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u22121\n2 \u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 + log( \u02c6\n\u03c01) \u2212log( \u02c6\n\u03c02)\n\u0015\n> 0\n29\n\nHave we seen this before?\nxT \u0000 \u02c6\n\u03a3\u22121( \u02c6\n\u00b51 \u2212\u02c6\n\u00b52)\n\u0001\n+\n\u00141\n2 \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u22121\n2 \u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 + log( \u02c6\n\u03c01) \u2212log( \u02c6\n\u03c02)\n\u0015\n> 0\nw = \u02c6\n\u03a3\u22121( \u02c6\n\u00b51 \u2212\u02c6\n\u00b52)\nb = 1\n2 \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u22121\n2 \u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 + log( \u02c6\n\u03c01) \u2212log( \u02c6\n\u03c02)\n30\n\nLinear separation using LDA\nThe decision function is then of the form\nh(x) =\n(\nclass 1\nif f (x) > 0\nclass 2\nif f (x) < 0\nf (x) = \u27e8w, x\u27e9+ b\nw = \u02c6\n\u03a3\u22121( \u02c6\n\u00b51 \u2212\u02c6\n\u00b52)\nb = 1\n2 \u02c6\n\u00b5T\n2 \u02c6\n\u03a3\u22121 \u02c6\n\u00b52 \u22121\n2 \u02c6\n\u00b5T\n1 \u02c6\n\u03a3\u22121 \u02c6\n\u00b51 + log( \u02c6\n\u03c01) \u2212log( \u02c6\n\u03c02)\nIntuitively, each pair of classes is linearly separated\nalong the di\ufb00erence-of-means vectors \u02c6\n\u00b51 \u2212\u02c6\n\u00b52.\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n43\n\nLinear Discriminant Analysis recap\n\u2022 LDA decides between two classes with a linear decision function built from\ndata using a closed-form expression\n\u2022 The model interprets the data as class-conditional Gaussians distributions\nwith the same covariance structure\n\u2022 This assumption is problematic, since it is rarely true in real data\n\u2022 Moreover LDA, like linear regression, is very sensitive to outliers\n44\n\nLDA vs. Perceptron\nWe have now two methods for binary classi\ufb01cation: Perceptron and Linear \nDiscriminant Analysis. Both use the same type of predictive model (linear), but \nthe way their parametrization is constructed / trained is considerably di\u02d9erent:\n\u2022 LDA can handle data that is not linearly separable data, the Perceptron\ncannot (will never terminate, remember?)\n\u2022 The Perceptron always \ufb01nds a linear separation if one exists, LDA does not\nprovide such a guarantee\n\u2022 Training the Perceptron is an iterative procedure, while LDA estimates its\nparameters with a closed form solution\n\u2022 While the Perceptron starts from a linear discrimination function and only\ntreats the data as correctly/wrongly classi\ufb01ed, LDA starts from a\nprobabilistic model on how the data was generated, and the linear\ndiscriminant function is obtained as a derivation\n45\n\nExtra material\n\u2022 Additional information on linear regression and linear models\nhttp://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf\n\u2022 More on the the statistics behind today\u2019s lecture\nhttps://en.wikipedia.org/wiki/Normal_distribution \nhttps://en.wikipedia.org/wiki/Likelihood_function \nhttps://en.wikipedia.org/wiki/Log_probability\n46\n\n", "00.course_info.pdf": "Course info\nMachine Learning\nBachelor in Computer Science [SP24]\nFebruary 26, 2024\n\nTeam\n\u2022 Prof.  \nPhilippe\n \n\u00e9\nCudr -Mauroux\n\u2022 Dr.  \nSimon  \nRuffieux\n \n \n \n \n1\n\u2022 Dr. Anna  \nFischer\n\u2022 Matthias D\u00fcrmeier (exercises)\n\u2022 Dr. Giuseppe Cuccu (slides!)\n\nIntended audience\nThis course is designed for the\n3rd year Bachelor in Computer Science\n\u2022 It is also available (sometimes required) for other programs,\nbut only at Masters level.\nYour experience makes up for the di\ufb00erent background.\n\u2022 Requirements: functional analysis, linear algebra, statistics\nand Python programming.\n\u2022 Course pace is high, topic is vast, exercises are mandatory\n2\n\nCourse objectives\n1. Understanding + hands-on experience of\nthe foundations of Machine Learning\n2. Con\ufb01dence to study any ML topic by yourself in the future\nOne day all you need may be to import\none library and call one function.\nThis course teaches you which library, which function, when it\u2019s\napplicable, limitations to keep in mind, what are its are its\nparameters and plausible value ranges, etc.\n3\n\nSchedule (subject to change)\nFeb\n-\n26: Foundations\nand Supervised Learning\nM\n-\nar 4: Classi\ufb01cation\nand Perceptron\nMar\n-\n11: Linear Regression and\nDiscriminant Analysis\nMar\n-\n18: Handling Data\nMar\n-\n25:\nve\n\u00ef\nNa\nBayes\n  \nApr\n-\n29: Neural Networks (Basic)\n May\n-\n06: Neural\nNetworks Advanced)\n- May 13: Deep Learning\n \n- May 27: Reinforcement Learning\n4\n[Easter\n \nbreak]\n[May\n \n20:\n \nPentecost,\n \nno\n \ncourse]\n- Apr 8: Support Vector Machines\n- Apr 15: Mappings, Embeddings \nand Kernels\n- Apr 22: Unsupervised Learning\n\nCourse organization\n\u2022 Every Monday, 14:15 to 17:00\n\u2022 Pauses: ~10-15m at ~15:00 and ~16:00 (according to needs)\n\u2022 The course language is English\n\u2022 Slides uploaded weekly on Moodle (print only if necessary)\n\u2022 Timetable:\nhttps://www.unifr.ch/timetable/en/course.html?show=112239\n5\n\nRegistration\nOnly you can register yourself to the course\nOnly you can register yourself to the exam (not automatic)\nYou can only register within the o\ufb03cial windows\nIf you don\u2019t, WE CANNOT HELP YOU, please take responsibility\nand note down your deadlines:\nhttps://www3.unifr.ch/scimed/en/info/dates\n6\n\nRegistration\n\u2022 Deadlines: https://www3.unifr.ch/scimed/en/info/dates\n\u2022 Course codes:\n\u2022 BSc \u2192UE-SIN.06022\n\u2022 MSc \u2192UE-SIN.08022\n\u2022 Course registration deadline: April 5,  2024 (late registration)\n\u2022 Summer exam registration window: 15.04.24 \u2013 03.05.24\n\u2022 Summer exam session: 03.06.24 \u2013 22.06.24\n\u2022 You can only fail a course\u2019s exam twice: if you cannot attend remember to\ncancel your registration so you won\u2019t lose one chance\n\u2022 If you cannot make it: remember to cancel ahead! (only two chances)\n\u2022 All communication goes through Moodle: register today  \n7\n \n   https://moodle.unifr.ch/course/view.php?id=280859\n \n\nLaboratories\n\u2022 Lab exercises are mandatory. The grade is only pass/fail\n\u2022 Make up teams of 2 students, though every student submits\nindependently (no group submissions)\n\u2022 If you cannot \ufb01nd a teammate, you need to let us know and we\u2019ll help\n\u2022 Upload your assignment solution strictly by the next Sunday at 23:59\n\u2022 Solutions are uploaded on Monday mornings (following week)\n\u2022 Pass by solving 2/3rds of the points, that\u2019s 66% correct\n\u2022 It is actually easy, but very time consuming at the beginning\nAdvice from past students: don\u2019t wait until Sunday\n8\n\nExam\n\u2022 The \ufb01nal exam is the only grade (100% of the \ufb01nal note)\n\u2022 Taking the exam is restricted to those who pass 9/ 2\n1\nlabs\n\u2022 For each lab that you pass, you get extra points at the exam\n(if you need to skip one for serious reasons remember to send me proof)\n\u2022 The exam di\ufb03culty is the same in Summer and Autumn.\n\u2022 Exams have the same format as the labs, and are open book:\no\ufb04ine material allowed, but obviously no communication.\nUnless you practiced though, expect to have too little time to copy.\n\u2022 Keeping up with the course and doing the assignments works:\n\u2022 Students who submit all assignments have a 100% passing rate (!)\n9\n\nStudy material\nPart of this course is based on\nMachine Learning: Supervised Methods\nfrom prof. Tobias Glasmachers at\nthe Ruhr Universit\u00e4t Bochum (thanks!!)\nExtra material:\n\u2022 Coursera: https://www.coursera.org/learn/machine-learning\n\u2022 Stanford: http://cs229.stanford.edu/syllabus.html\n\u2022 Cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/\ncheatsheet-supervised-learning\n\u2022 Visually Explained: https://www.youtube.com/@VisuallyExplained\n10\n\n", "08.unsupervised_learning.pdf": "08: Unsupervised Learning\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 22, 2024\n\nWhere are we\n\u2022 Supervised Learning models the mapping between inputs and\nlabels.\n\u2022 When labels/targets are available, we have several methods\nthat help us learning such a relationship.\n\u2022 When labels are not available, we need to refer to either\nUnsupervised or Reinforcement Learning.\n1\n\nToday\u2019s menu\n\u2022 Introduction to UL\n\u2022 Clustering\n\u2022 The k-means algorithm\n\u2022 Anomaly detection\n\u2022 Compression and encoding\n\u2022 Vector quantization\n\u2022 Sparse coding\n\u2022 Matrix decomposition techniques\n\u2022 Singular value decomposition\n\u2022 Data imputation and denoising\n\u2022 Dimensionality reduction and\nPrincipal Component Analysis\n\u2022 Recommender Systems\nWe are covering an entire\nlearning paradigm\nin one lecture.\nUnderstand that we can only go\nfor breadth, not depth\nCheck out the extra material\nfor more on the speci\ufb01c topic\nyou like most!\n2\n\nLearning paradigms\n\u2022 The methods seen so far (Supervised Learning) all stem from the same\nhypothesis: you have access to training data that associates inputs with\ncorrect outputs (labels)\n\u2022 In this case it is then possible to model the relationship between inputs and\nlabels, either with a closed form or successive iterations (minimizing an error\ngradient)\n\u2022 Of course this assumption does not necessary hold true:\n\u2022 Data most commonly comes without labels, so selecting a feature as label\nis arbitrary (and prone to human error)\n\u2022 In many processes, the correct label is not available (e.g. control problems)\n\u2022 Many problems can be formulated without the need for correct labels (more\non this later)\n3\n\nLearning paradigms\nWe mentioned at the beginning of the course three learning paradigms:\nSupervised, Unsupervised and Reinforcement Learning.\nThe similarity between the \ufb01rst two is clearer when stating them\nas variations of Reinforcement Learning:\n\u2022 Reinforcement Learning: learn to maximize a score called \ufb01tness, without\nassumptions/restrictions, over the space of models: search the model that is\nbest at that something that is described by the \ufb01tness\n\u2022 Supervised Learning: \ufb01tness is known: the error between the model\u2019s\nprediction and a correct outcome (label), available for each input\n\u2022 Unsupervised Learning: \ufb01tness is known: a similarity metric between\nelements, grouping together elements that are similar for our task\n4\n\nConvention, not partition\n\u2022 Important: this separation is a convention not a partition.\nA few examples:\n(i) The Reinforcement Learning paradigm generalizes both supervised and\nunsupervised (Risk and Similarity functions are just special Fitnesses)\n(ii) Deep Reinforcement Learning addresses Reinforcement Learning problems\nusing Supervised Learning methods (we\u2019ll see how in a few weeks)\n(iii) Self-Supervised Learning creates labels for data using Unsupervised Learning,\nthen applies Supervised Learning to learn a model\n\u2022 Q: Why we map RL and UL to SL, rather than say use RL for everything\nsince it can generalize the other paradigms?\n\u2022 A: We have a method that works, today: Deep Learning is so e\ufb00ective that\nit is easier to convert problems to SL then solve them with DL, rather than\n\ufb01nd a better solution to a more generic (thus arguably harder) paradigm\n(There are people who still try though!)\n5\n\nHow does Unsupervised Learning work\n\u2022 SL approximates the mapping from the inputs to the labels, but remember\nthat the label is just a feature!\nUL learns the relationships between all features in the data, i.e. the overall\nunderlying patterns\n\u2022 Piece of advice: \u201cto a hammer, all problems look like nails\u201d.\nYou need to understand problems beyond the simplistic mapping of\n\u201chand-picked inputs \u21d2hand-picked outputs\u201d, because often in the world\nyour choices for hand-picking will be limited, if available\n\u2022 We need techniques in our ML tool-belt that can \ufb01nd patterns in the data\nwithout relying on labels being available\n6\n\nUnderstanding similarity\n\u2022 Unsupervised Learning is centered around the concepts of similarity, patterns\nand repetition, estimated from the data using a measure of similarity\n\u2022 More formally, a similarity metric between two elements of the same space\n(we called it input space in SL) returns a score of how much they share\npatterns, due to their data coming from the same underlying process\nFor example the Euclidean distance (norm-2 of the di\ufb00erence):\nE(A, B) = \u21132(B \u2212A) = \u2225B \u2212A\u22252 =\nq\n\u2211i [Bi \u2212Ai]2\n\u2022 Remember: all metrics are (i) always non-negative, and (ii) zero between an\nelement and itself (we saw it before)\n7\n\nExample: NB with multivariate Gaussian\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n1\n2\n3\n4\n5\n6\n7\n1\n0\n1\n2\n3\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nWe saw this two-class instance of the Iris dataset in the last assignment.\nLet\u2019s say we want to classify it with Na\u00efve Bayes, using a Multivariate\nGaussian to capture the two peaks. To build a multivariate, we need to\nmodel each peak independently, then sum up the distributions.\nBut how do we split the data belonging to each of the two peaks?\nThey belong to the same class, thus have the same label!\n8\n\nClustering\n\u2022 The technique of clustering (or cluster analysis) selects data points based\non their higher similarity, while sorting less similar data into di\ufb00erent groups\nThink: similarity and patterns are a foundation and assumption to the whole\nMachine Learning, as they underlie learning\n\u2022 The goal is to identify data belonging to di\ufb00erent clusters: groups of data\nthat are closer to each other (more similar) than they are to data belonging\nto other clusters (less similar)\n\u2022 Example: when classifying apples from oranges, building the color average\nfeature would enable us to split the fruits into the two classes, without even\nusing the labels\n\u2022 Do not delete the labels just yet though, we will need them to know the\ncorrect partition, and estimate the performance of our model during testing\n9\n\nClustering\n\u2022 Clustering and UL are everywhere, if you look close enough. Last week we\nsaw word embeddings: we did not use labels, and instead modeled a\nrelationship between words using a similarity metric, \u201cdistance in text\u201d\n(n-grams).\n\u2022 Whenever data is processed based on \u201csimilarity\u201d rather than learning based\non explicit labels, look closely and you will \ufb01nd UL concepts and techniques.\nYou cannot even start state-of-the-art NLP (let alone build e.g. ChatGPT on\ntop) until you learn the embedding. Using SL to train e.g. spam vs.\nnon-spam only comes later.\n\u2022 The foundations of ML (such as clustering, pattern, similarity) are\nunavoidable. Embrace these concepts, make them your own, and the next\nbig ML news you read may be a bit less surprising and easier to understand.\n10\n\nK-Means\n\u2022 The simplest algorithm for clustering is k-means. The k hyperparameter\ncontrols the number of clusters in which you want to split the data, which is\nthen grouped based on the similarity metric\n\u2022 The algorithm starts by randomly placing in the data space k \u201ccentroids\u201d\n(means, classes, representative elements, etc: similar to LDA or NB\ndistribution means)\n\u2022 These points are then \u201cmoved around\u201d (updated) to better represent\naggregations in the data, by iterating the following steps alternatively:\n(i) maximize the total distance between the means\n(ii) minimize the distance between each point and and the mean closest to it\n(two opposite objectives \u21d2we \ufb01nd multiobjective optimization once again)\n\u2022 You can \ufb01nd a very complete tutorial on k-means in the\nextra material, but really that is all there is to it, check\nout Density Based Clustering instead\n11\n\nK-Means - How it works (1)\n12\n\nK-Means - How it works (2)\n13\n\nK-Means - How it works (3)\n14\n\nClustering vs. classi\ufb01cation\nClustering/UL cannot see labels\nThis is represented here as lack of color\nWe distinguish di\ufb00erent groups\n(e.g. apples, oranges, bananas, kiwis)\nbut we cannot tell which one is which,\nonly that they are di\ufb00erent\nTwo steps in the K-Means algorithm:\n(i) Maximize the total distance\nbetween the means\n(ii) Minimize the distance between\neach point and its closest mean\n15\n\nAnomaly detection\n\u2022 Clustering learns which points \u201cbelong together\u201d: this is directly applicable\nin anomaly detection (aka outlier detection):\nwe know which points are \u201cnormal\u201d, anything else is highlighted as an\nanomaly (outlier)\n\u2022 This is a radically di\ufb00erent task than distinguishing between two classes\n(i.e. binary classi\ufb01cation): anomaly detection recognizes one class, then\nanything that does not belong is rejected as an anomaly\n\u2022 A common application is to recognize changes in data streams, which are\njust running sequences of coherent data (e.g. readings from a sensor);\nif points are regularly spaced in time you have a time series\n\u2022 Example: have products pass on a conveyor belt under a sensor (e.g.\ncamera), and \ufb01nd which ones defy expectations which likely correspond to\nmanufacturing defects\n\u2022 Algorithms for anomaly detection include One-Class SVM and\nExpectation Maximization, so check those out if needed!\n16\n\nAnomaly detection: one more example\n\u2022 Example: you receive a stream (continuous sequence of readings) from a\ntemperature sensor, with one more number every 5 minutes (making it a\ntime series). You are tasked with writing a program that detects extreme,\npotentially dangerous weather changes (storms, hail, turbulence, etc.).\n\u2022 Hard-coded boundaries do not really work: a drop of 5 degrees may correlate\nwith storms during the day, but is perfectly normal during the night or if bad\nweather was already setting in. What you need is to model a currently\nnormal temperature, then detect outliers.\n\u2022 The simplest method for example would be to (i) compute a rolling\naverage over a recent time window (e.g. the last 3 hours), (ii) decide a\nthreshold for normal change (e.g. 2\u00d7 the standard deviation), (iii) react to\ntemperature deltas above the threshold (message, alarm, etc.).\n17\n\nQuantization, Encoding, Compression\n\u2022 Yet another application of\nrepresenting clusters with centroids\nis to encode the data into a\ncompact representation, i.e.\ndata compression\n\u2022 If a centroid represents fairly (i.e.\narbitrary precision) all points in the\ncluster, there is no reason keep all\npoints around, they can all be\nrepresented by the one centroid\n\u2022 This is a lossy encoding: the loss\nof information is the total\ndi\ufb00erence between each point and\nits representative\n18\n\nQuantization, Encoding, Compression\n\u2022 On the other hand, all points\nbelonging to a cluster are now\ncompressed into a single point;\nthe corresponding space is said to\nbe quantized, or discretized: the\ncontinuous feature is mapped to a\ndiscrete set of values, the centroids\n\u2022 A set of centroids is called a\ndictionary, and enables di\ufb00erent\ntypes of encoding (dictionary-based\nmethods)\n19\n\nImage Quantization\n20\n\nVector Quantization\n\u2022 The fundamental dictionary-based quantization algorithm is called Vector\nQuantization (VQ), and originated in yet another \ufb01eld called Signal\nProcessing\n\u2022 The concept is still the same seen so far: each element is mapped to the\nclosest out of k centroids in the dictionary. The learning algorithm is based\non Expectation Maximization, same as K-Means\n\u2022 Once the dictionary is learned, it can be used for encoding. In Na\u00efve VQ,\ndata points are simply encoded with the index of the most similar centroid,\nthus compressing the input dimensionality to a single number\n21\n\nQuick example of VQ: Training\nRepeat this for each training point in turn:\n\u2022 Our data is 2-dimensional, similarity is \u21132, learning rate is \u03b7 = 0.3, and we\nhave dictionary D = {[2, 3], [\u22121, 1], [5, \u22122]}\nWe need to train it on all training point; let\u2019s say our \ufb01rst one is [4, 3]\n\u2022 First compute \u21132 between the point and each centroid, \u21132(p \u2212ci), \u2200ci \u2208D,\ne.g.\np\n(4 \u22122)2 + (3 \u22123)2 = 2 obtaining the set {2.0, 5.4, 5.1}\n\u2022 The most similar centroid is thus the \ufb01rst, D1 = [2, 3], which we select for\ntraining and update with the value D1 = [2, 3] + \u03b7[4, 3] = [3.2, 3.9]\n\u2022 The dictionary is now D = {[3.2, 3.9], [\u22121, 1], [5, \u22122]}\n22\n\nQuick example of VQ: Encoding\nThis is how you apply a trained VQ to encode (compress) a point:\n\u2022 A new point P = [\u22123, 5] is compared with the centroids in D, yielding\ndistances {6.3, 4.5, 10.6}. The most similar centroid is thus the second:\nD2 = [\u22121, 1]\n\u2022 Point P = [\u22123, 5] is simply encoded as one number C = 2\n\u2022 To reconstruct the point (at a loss, of course), we simply fetch the\ncorresponding centroid: in this case\nP = [\u22123, 5],\nC = 2,\n\u02c6\nP = rec(C) = D2 = [\u22121, 1]\n\u2022 The information loss is calculated by accumulating the Loss over all\ncoordinates (e.g. absolute value of total di\ufb00erence):\n\u03a3{|P \u2212DC|} = \u03a3{|[\u22123, 5] \u2212[\u22121, 1]|} = | \u22123 \u22121| + |5 \u22121| = 6\n23\n\nCompression and Reconstruction Error\n\u2022 Na\u00efve VQ of course looks really na\u00efve. For example: why using only one\ncentroid to reconstruct our point? Can\u2019t we average multiple ones?\n\u2022 Formally: point P = [\u22123, 5] from before could instead be encoded as\nC = [0, 1, 0], then reconstructed as the dot product between the code and\nthe dictionary \u27e8C, D\u27e9= 0 \u00d7 D1 + 1 \u00d7 D2 + 0 \u00d7 D3 = D2.\nEven better: wouldn\u2019t a simple linear combination over the dictionary o\ufb00er\nan equal-or-better encoding, at a negligible cost?\nWhat about e.g. [0.0, 0.9, 0.1]? (yes, of course it does)\n\u2022 Dictionary-based methods are easy to understand and o\ufb00er high compression\nratio in applications where partial loss of data is acceptable. The\ninformation loss is commonly called reconstruction error: the (similarity)\ndi\ufb00erence (i.e. the Loss) between the reconstructed and the original point.\n(can you see this learning as modeling?)\n\u2022 While VQ excels at compression (cannot beat a single index), methods that\nminimize reconstruction error bene\ufb01t from more complex encodings.\n24\n\nSparse Coding\n\u2022 The most successful class of algorithms for this applications are based on\nSparse Coding, which is built on two concepts:\n(i) An over-complete dictionary, meaning with more centroids than the original\nfeatures, producing a higher-dimensional representation of the data (again\nakin to kernels) using highly specialized centroids\n(ii) A learning algorithm for the dictionary that takes the code into account and\nenforces code sparsity: the code is mostly made of zeros, meaning that the\nreconstruction should use just a few of the available centroids\n\u2022 While the dictionary size grows beyond the original dimensionality, the codes\nproduced are sparse, and can be e\ufb03ciently represented with a list of pairs\n(index, coe\ufb03cient) referencing only centroids with nonzero coe\ufb03cients.\n\u2022 Sparse coding performs great for low-error lossy compression, feature\nextraction and super-resolution. Each centroid e\ufb00ectively represents a\ndata pattern, and the code highlights which are present in each data point.\n25\n\nSingular Value Decomposition\n\u2022 The methods seen so far search for representative elements in the data by\nconsidering each data point individually in a sequence; but we also saw in\npast lectures that some ML methods prefer considering all the data as a\nsingle entity and provide a closed form solution\n\u2022 The same applies to Unsupervised Learning: the whole dataset can be\nprocessed as a single entity, meaning that rather than \ufb01nding representing\nelements for each point individually, analyzing the dataset as a matrix is an\ne\ufb00ective alternative to \ufb01nd mathematically representative vectorial entities\n\u2022 Remember eigendecomposition from linear algebra? Applying it to a dataset\nbasically extracts the fundamental (vectorial) components that generate the\ndata (eigenvectors), and the scalar weight or contribution of each to the\nreconstruction (eigenvalues)\n\u2022 There are several matrix decomposition techniques following the same\nconcept; the main tool used is Singular Value Decomposition (SVD)\n26\n\nSingular Value Decomposition\n\u2022 SVD decomposes a matrix M (our dataset) into three matrices:\nM = U \u03a3 V T, with M of size m \u00d7 n\nU: an m \u00d7 m matrix with the left-singular eigenvectors as columns\n\u03a3: an m \u00d7 n matrix with the singular values \u03c3i on the diagonal, \u03a3ii = \u03c3i\nV: an n \u00d7 n matrix with the right-singular eigenvectors as columns\n\u2022 Beware: some algorithms/implementations return a square \u03a3 and\nrectangular U and V . Others (e.g. numpy.linalg) return U,\n\u03c3 (vector of size n: values on \u03a3\u2019s diagonal), and V T (yes: transposed)\n\u2022 Here\u2019s a very good example you should check out:\nhttps://www.d.umn.edu/~mhampton/m4326svd_example.pdf\n\u2022 Careful though where it mentions \u201ckernels\u201d, this is not ML;\ncheck out the (related but di\ufb00erent) linear algebra de\ufb01nition:\nhttps://en.wikipedia.org/wiki/Kernel_(linear_algebra)\n27\n\nSVD and compression\n\u2022 The SVD decomposition produces di\ufb00erent results depending on several\nfactors: it is common to choose the decomposition that yields the singular\nvalues \u03a3ii in descending order.\n\u2022 As a consequence, the singular vectors (both left and right) will be sorted by\ndecreasing contribution to the reconstruction.\n\u2022 Using all eigenvectors (and eigenvalues) would yield lossless reconstruction.\nThis is important. Dropping the least contributing eigenvectors impacts\nminimally the precision of the reconstruction, while the data representation\nis compressed proportionally to the size of the dropped vector.\n\u2022 Depending on data redundancy, it is possible to achieve high compression\nratio while maintaining an arbitrary bound on the reconstruction error.\n28\n\nDenoising\n\u2022 Funnily enough, reconstruction following lossy compression (which could be\nseen as \u201cnoise\u201d) has direct applications for denoising\n\u2022 The least contributing eigenvectors in SVD maintain information about the\nsmallest variations over a more \u201ccentral\u201d value, dictated by the other (more\nin\ufb02uential) eigenvectors. These smaller details, which are not captured by\nthe main trend of the data, typically describe to noise in the data.\n\u2022 To reduce the noise of an element in the data (denoise):\n(i) Decompose the data matrix with SVD\n(ii) Drop the eigenvectors with smallest eigenvalues (last columns) from U and\nV (obtaining b\nU and b\nV )\n(iii) Reconstruct the data b\nM = b\nU \u03a3 b\nV T\n(iv) Replace the element in the original data M with the corresponding value\nfrom the reconstructed data b\nM\n\u2022 Step (iv) is crucial: it limits the loss of data to the value\u2019s noise, do not\nmodify the rest of the data! (which we assume is correct)\n29\n\nData imputation\n\u2022 With denoising, you just gained access to the most basic tools for\ndata imputation, which aims at reconstructing missing values in the data\n\u2022 If a value is missing, then any initial estimation can be seen as the sum of\nthe true (unknown) value, plus a certain (unknown) amount of noise:\n\u02c6\nxi = xi + \u03f5\n\u2022 A simplest \ufb01rst guess is a simple average of surrounding values in a series\n(i.e. feature). Incidentally, guessing an element\u2019s value from an average of k\nsurrounding elements is all there is to the k Nearest Neighbors (kNN)\nalgorithm\n\u2022 So from here on you can focus on denoising, knowing that imputing missing\nvalues is a direct application\n30\n\nFrom SVD to PCA\n\u2022 SVD is based on eigendecomposition (actually generalizes it), which extract\nthe vectors along which the data changes most signi\ufb01cantly\n\u2022 A geometrical interpretation of SVD is as the roto-translation matrix that\nwould rotate, shift and rescale the axis to align them with the vectors of\nhighest variance in the data, i.e. the principal components\n\u2022 A simple dot product of the data with the \ufb01rst k (right-)eigenvectors\nproduces a k-dimensional projection of the data along the directions of the\nhighest change. The \ufb01rst most-change will happen along the new x axis, the\nsecond-most (rotated to be orthogonal) along the new y axis, etc.\n\u2022 This technique is called Principal Component Analysis (PCA)\n31\n\nPCA on Iris\n3\n2\n1\n0\n1\n2\n3\n4\nPC 1\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nPC 2\nspecies\nsetosa\nversicolor\nvirginica\n32\n\nDimensionality Reduction and Principal Component Analysis\n3\n2\n1\n0\n1\n2\n3\n4\nPC 1\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nPC 2\nspecies\nsetosa\nversicolor\nvirginica\n\u2022 This is a projection of the entire, 4-dimensional Iris dataset on its \ufb01rst\ntwo Principal Components\n\u2022 PCA is the classic go-to method for Dimensionality Reduction: the\ndimensionality of the projected data only depends on the selected number of\ncomponents, which are sorted by their decreasing contribution to variability\n33\n\nPlaying with dimensionality: Higher vs. Lower\nThink of PCA as kind-of the opposite of Kernels:\n\u2022 PCA lowers dimensionality, which allows focusing on the biggest data\nchanges. This makes it ideal for humans: plotting and summarization.\n\u2022 Kernels increase dimensionality, highlighting and distinguishing all possible\ndata changes using independent dimensions. This makes them ideal for\nmachines: take on complex, non-linearly-separable data.\nAlways ask yourself who is the consumer of your analysis!\nThe fact that PCA separates the Iris data so well just hints at this problem being\ntrivial (we already knew that, the Iris dataset is such a classic)\nAs with many linear methods, a Kernelized version of PCA is available, which can\nhighlight variance across nonlinear relationships: Kernel PCA (KPCA)\n(just run PCA on the Gram matrix \u2014 as expected, right?)\n34\n\nRecommender Systems\n\u2022 One of the most marketable applications for similarity-driven methods and\nimputation of missing data is in Recommender Systems (RecSys). This\ndeals with methods that provide users with content recommendations, i.e.\nwhich movie / song to suggest next on a streaming service.\n\u2022 The origin of RecSys as a \ufb01eld is typically traced back to the Net\ufb02ix Prize.\nIn 2006 Net\ufb02ix published a huge dataset of movie user ratings, challenging\nwith a one million dollar prize for whoever came up with the best algorithm\nto predict which next movie would a user like best.\n\u2022 Their in-house results were beaten within four days. Turing Award\nG. Hinton famously participated. The price was ultimately won in 2009 by a\nprivate team, with an improvement of over 10% over Net\ufb02ix\u2019s proprietary\nimplementation (yes you bet that netted them more than $1M in earnings).\n35\n\nRecommender Systems as Data Imputation\n\u2022 The traditional formulation is simple: we have a matrix of ratings (users\nscore how much they like a movie), with users on the rows and movies on\nthe columns.\n\u2022 The data is obviously very sparse: each user reviews but a tiny subset of the\nmovies, and each movie is reviewed by relatively very few users. This adds\nan extra challenge to missing value imputation techniques.\n\u2022 State of the art recommender systems nowadays make for a huge market:\n\u2022 All the major online shops and content providers pay top money for \ufb01eld\nexperts\n\u2022 There\u2019s even a dedicated conference (RecSys) where experts meet and\ncompanies head-hunt\n\u2022 The top European player (if not globally) is just across the border: Criteo,\nwith its AI Lab based in France (Paris and Grenoble)\n36\n\nSummary\n\u2022 Unsupervised Learning learns inherent patterns in the data without using\nnor needing labels (which has its pros and cons)\n\u2022 Clustering groups together data points based on a similarity metric, by\nmaintaining representative elements for each cluster (e.g. k-means)\n\u2022 Anomaly detection utilizes clustering to \ufb01nd outliers in the data\n\u2022 Vector quantization produces a dictionary of representative elements, uses\nit to encode data in a compressed format\n\u2022 Sparse coding enforces sparsity using over-complete dictionaries in VQ-like\nencoding, which lowers reconstruction error\n\u2022 Matrix decomposition techniques search for similarity in entire datasets,\nwith applications in denoising and data imputation; a key tool is SVD\n\u2022 Dimensionality reduction allows for visualization and (human)\ninterpretation of high-dimensional data: PCA is founded on SVD, and\nKPCA extends it to nonlinear relationships\n37\n\nExtra material\n\u2022 Very complete tutorial on k-means:\nhttps://towardsdatascience.com/k-means-clustering-algorithm-\napplications-evaluation-methods-and-drawbacks-aa03e644b48a\n\u2022 Basic improvement on k-means, widely adopted, \ufb01nds k automatically:\nhttps://en.wikipedia.org/wiki/Cluster_analysis#Density-\nbased_clustering\n\u2022 SVD example by hand (you should try it! not hard!):\nhttps://www.d.umn.edu/~mhampton/m4326svd_example.pdf\n\u2022 More on Anomaly Detection:\nhttps://en.wikipedia.org/wiki/Anomaly_detection\n\u2022 One-Class SVM implementation (and explanation):\nhttps://scikit-learn.org/stable/modules/generated/sklearn.\nsvm.OneClassSVM.html\n\u2022 More about the Net\ufb02ix Prize:\nhttps://en.wikipedia.org/wiki/Netflix_Prize\n38\n\n", "10.neural_networks_advanced.pdf": "MSE - TSM-DeLearn\nMaster of Science\nin Engineering\n10: Neural Networks (advanced)\nPresentedby Dr Anna Scius-Bertrand\nSlides fromGiuseppe Cuccu and, \nMSE \u2013 TSM-DeLearn\nMachine Learning\nBachelor in Computer Science [SP24]\nMay 6, 2024\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n1\n\u2022Neural Networks are designed as graphs of connected\nneurons\n\u2022Each neuron works as an improved Perceptron\n\u2022The computational graph corresponds to a mathematical model.\n\u2022Neural Networks are Generic Function Approximators\n\u2022Larger structures allow approximating more complex\nfunctions\n\u2022The parametrization is: structure, activations, and weights\n\u2022Network weights can be trained with Backpropagation (SGD, \nSL)\nWhere are we\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n2\n\u2022Networks as Sequential Mappings\n\u2022Sparsity, Modularity, Skips\n\u2022Recurrent Networks\n\u2022Convolutional Neural Networks\n\u2022Autoencoders\n\u2022Limitations and applicability\nToday\u2019s menu\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n3\nFlexibility of the neural network model\n\u2022 The last lecture introduced fully-connected feed-forward neural networks \nwith homogeneous activation. This restriction and simplification was \nintroduced historically to derive the Universal Approximation property and \nthe Backpropagation algorithm.\n\u2022 There is however nothing that limits us in principle to this design, \nas long as we can adapt our learning method.\n\u2022 Example: neural networks for regression typically have a linear output layer \nand nonlinear hidden layers, and adapting Backpropagation to this \narchitecture is trivial.\n\u2022 Sparse connectivity is also straightforward: simply treat the missing \nconnections as zero weights during training.\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nNetworks as sequential mappings\nx\ng(x)\nf (g(x))\nNow ask yourself: what is the output of the first layer / input of the second?\nWe can see the first layer of the network as one function, \nthe second as another function.\nThis makes the first layer\u2019s output an intermediate feature space.\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nOriginal\nspace\n(pixel intensities)\nIntermediate\nspace\n(main color)\n\u03a6a\nFeature\nmapping\nDestination\nspace\n(label)\n\u03a6b\nDecision\nmapping\nOriginal\nspace\n(inputs)\nIntermediate\nspace\n(hidden layer\nactivations)\nFirst\nlayer\n(hidden)\nDestination\nspace\n(output layer\nactivations)\nSecond\nlayer\n(output)\nNetworks as sequential mappings\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nNetworks as sequential mappings\nOriginal\nspace\n(inputs)\nIntermediate\nspace\n(hidden layer\nactivations)\nFirst\nlayer\n(hidden)\nDestination\nspace\n(output layer\nactivations)\nSecond\nlayer\n(output)\n\u2022 Each layer of the network maps its inputs into a new feature space.\n\u2022 A neural network is a sequence of independent mappings.\n\u2022 Independence means that different layers can be specialized to different \ncomputational methods and strategies (e.g. recurrent, convolution, etc).\n\u2022 We will next examine a few example of connectivity variations and their \napplications.\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nDrawing fully-connected layers\n2\n2\n1\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nSparsity and modularity\n\u2022 Any acyclic directional graph can define a computational sequence, and\nthus a valid neural network, so any connection is in principle feasible.\n\u2022 The first obvious augmentation is to prune the connectivity of the graph:\nisolated portions of a larger network can in practice function as chains of\nnetworks, creating a modular design.\n\u2022Within this scope, any connection is in principle feasible; for example\nsparse connectivity can be achieved by fixing (untrainable) zero-weight\nconnections in the weights matrix\n\u2022Isolated portions of a larger network function as chains of networks,\ncreating a modular design\n\u2022Errors are computed independently for each sub-network\nsupporting specialization of each component to independent roles\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nExtracting independent features\nFruit colors \nspace\nFruit widths \nspace\n\u03a6b\nmax width\n\u2022\nFeature space\n\u2022\n(e.g. fruit pictures)\n\u03a6a\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nModular network\n\u2022 The two hidden layers are not fully \nconnected\n\u2022 The partial connectivity creates \nmodularity in the network\n\u2022 Disconnected groups of neurons \ncan more easily specialize\n\u2022 Still all neurons in the hidden layer \ncan be executed in parallel\n11\nw21\nw\n0\n0\n0\n0\n0\n0\nw\nw33 w43\n53\nw12 w22\n0\n0\n0\n0\nw34 w44\nw54\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nSkips\n\u2022 Skipping connections are connections between layers that are not \ndirectly subsequent in the computational sequence. \n\u2022 This creates a shortcut between distant layers, i.e. connections \nbetween layer\n\u2022 i and layer i + n that skip layers i + 1 . . . i + n \u2212 1\n\u2022 The final layer decides using both advanced features from the later\nlayer, and simpler, lower-abstraction features from the earlier layer\n\u2022 The classic architecture based on skiping layers is ResNet\n(Residual Neural Network)\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nBackward connections\n\u2022 What about connecting layers backwards, i.e. the input from a late \nactivation (layer i + n, with n \u22650) to the input of an earlier layer i? \nBackward connections generate a computational loop, which cannot be \nsolved in a single activation pass.\n\u2022 A more useful interpretation though is to consider backward connections as \ncarrying the past activation of a layer. Example:\n(i) At time t input xt is passed to the network\n(ii) The network is activated, with layer i generating output y\n\u02c6 i\nt\n(iii) At time t + 1 input xt+1 is passed to the network\nt+1\ni\nt\n(iv) Concatenated with x\n, also y\u02c6 is included in the input to the network\n\u02c6i\nt+1\n(v) The network is activated, with layer i generating output y\nbased on both\nits input and its previous activation\n\u2022These backward or recurrent connections provide a rudimentary\nmemory to the network: momentum of its computational process carries\nacross sequential activations\n\nSimple RNNs\n14\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nRNNs for Capturing Context Information \n\u2022 MLP are good at describing and classifying images. Are they good for \ndescribing a scene in a movie?\n\u2022 Model needs to learn capturing context information (e.g. location, day \ntime, season) by following several steps in a sequence \u2026\n\u2022 \u2026 with the goal to help explaining the scene, revealing the logic / \ncausality behind that allows drawing conclusions on observations made \nlater on.\n\u2022 RNNs are designed for that, i.e. they accumulate context information \n(over a sequence of steps) from which conclusions can be drawn.\n15\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nRNN\u2019s: Specialised for Modelling Sequences \nTimeseries\nWords in sentences\nCharacters in words\netc.\nSuccessive elements are not independent. \n\u2022 Local dependencies \n\u2022 Possibly long-term (non-local) dependencies\n(e.g. references for reasoning) \n16\nw-e-a-t-h-e-r\nToday, the weather above the sticky fog on Rigi is \nbeautiful. This morning, the train brought us there.\nSimilar to CNN\u2019s that are specialised for \nmodelling grids such as images.\nsticky fog\nRigi \u2026 there\nExamples\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nMany Applications\n\u2022 Sentiment classification\n-\nClassification of text: e.g. movie ratings\n-\nEmojification\n\u2022 Speech recognition\n-\nAssistants (Alexa, Siri, etc).\n-\nTranscription to text, video captioning\n-\netc.\n\u2022 Machine translation\n-\nDeepL\n-\nGoogle Translate\n\u2022 Captioning, subtitling\n-\nimages\n-\nyoutube videos\n17\naudio signal\n\u201cToday, the \nweather on Rigi \nabove the fog is \nbeautiful.\u201d\n\u201cToday, the \nweather on Rigi \nabove the fog is \nbeautiful.\u201d\n\u201cToday, the \nweather on Rigi \nabove the fog is \nbeautiful.\u201d\n\u201cHeute ist das \nWetter auf der Rigi \noberhalb des Nebels \nsch\u00f6n.\u201d\n\u201cThe sun is \nshining above \nthe clouds.\u201d\n(see e.g. https://arxiv.org/pdf/1411.4555v2.pdf)\nttps://www.captionbot.ai\n\u2022\nNamed entity recognition (NER), Time series modelling, prediction , Chatbots, Question/Answering\u2026 \n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nUse word representation vectors \n(e.g. embedding)\nSequence Models\n18\nToday, the weather above the fog line is beautiful.\nHeute ist das Wetter oberhalb der Nebelgrenze sch\u00f6n.\nExample: Language Translation \nOutput sequence     with length \nInput sequence    with length \nTypically, the elements of      and     are multi-dimensional vectors and are \nparametrised by an index or \u2018time\u2019 to describe the position in the sequence.\nUse word representation vectors \n(e.g. embedding)\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nSequence Model Categories\n19\nExamples\nnamed entity \nrecognition\nlanguage translation \nspeech recognition,\nchatbots\nimage captioning:\nsentiment \nclassification:\nmany-to-one\none-to-many\nmany-to-many\nmany-to-many\nin\nout\nSequence models grouped according to the length of the input and output sequences.\nsequence of words \n-> sentiment\nimage -> \nsequence of words\nsequence of words -> \nsequence of entity \nclasses (1:1)\nsequence of words \n-> sequence words \n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nEfficient Representation for Sequences\n\u2022 Represent inherent structure\n-\nElements are not independent, sequential order assumed important. \nExample: Important for syntactic correctness or semantics of sentences\n-\nSimilar to the images with the spatial structure (local connectivity of neighbouring\npixels)\n\u2022 Gain efficiency by sharing parameters:\n-\nUse the same parameters across different parts of the sequences, that share \nparameters across different parts of images by using filters.\n-\nParameter sharing also needed to cover sequences of arbitrary length. \n\u2022 Handle sequences of variable length\n-\nPossibly both, for input and output.\n-\nIn practice, however, often a fixed maximum length is set (with suitable padding)\n20\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nRecurrent Cells\n21\nFigures taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs\nprevious \nstate\n\u210e!\"#\nupdate computed \nfrom ht-1 and xt by \nnon-linear function \ud835\udc53\nvector- / tensor-\nvalued state \u210e!\ninput sequence,\nelements \ud835\udc65! are \nvectors/tensors \nin general\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nSingle Layer RNN\n22\n\u2022 Updated through a succession of steps: At each step, consumes next \nelement of input sequence (if available) and the previous state and \nupdates the state by applying suitable function \n\u2022 Parameters are shared by applying the same function for updating. \n\u2022 The state can transport information over multiple elements, i.e. it can \nmemorise information.\n\u2022 State is passed on to the next step (t+1) or, possibly, to another layer.\n\u2022 State suitably initialised before first step (often with zero values). \ngraph \nun-rolled\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nComputation for Simple RNN-Cells\n23\nComputation for forward propagation: \n\u2022 affine transformation of the inputs (as seen for MLPs)\n\u2022 non-linear activation function, e.g. tanh()\nat each step t\nParameters \nare the same for \nall time step \n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nExample Computation for Single Step  \n24\nIn typical NLP applications, \nword embeddings are taken as \ninputs, i.e. vectors of ~100 \ndimensions. \nThe hidden states vectors \ntypically have a similar \ndimensionality.\nParameters \nto be trained\nindependent \nof t (same of \nall cells in \nthe layer)\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nUnfolded vs Folded Representation\n25\nun-rolled, un-folded\nrolled, folded\nParameters \nshared.\n\u2022The technique of unrolling an RNN then applying backprop is called Backpropagation\nThrough Time (BPTT).\n\u2022Its main limitation is traditionally the same as all deep networks: the vanishing\ngradient\n\u2022Back-propagating an error through the long series of layers of a deep network, lowers\nits precision (and thus utility) with every step, quickly  making it almost worthless\n\u2022There are many successful implementations of RNNs: LSTMs.\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nInteresting Links\n\u2022 A. Karpathy\n\u201cThe unreasonable effectiveness of Recurrent Neural Networks\u201d\nVery nice and famous blog with cool applications \n(http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n\u2022 A. Ng, Deep Learning Course on Coursera (Course on \u201cSequence \nModels\u201d) \n\u2022 Fei Fei Li, Justing Johnston, Lecture 10: \nhttps://www.youtube.com/watch?v=6niqTuYFZLQ\n26\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nWrap-up\n\u2022 Why RNNs?\n-\nRNN helps wherever we need context from the previous input\n-\nMany applications: Speech recognition, video captioning, machine translation, chatbots, \ntext generation\n-\nOne -to-many, many-to-one, many-to-many approaches\n\u2022 Simple RNNs\n-\nNotion of rolled vs un-rolled representations, notion of time-step in the processing, \nnotion of state h of the cell that memories the context of past inputs\n-\nGradient backpropagation is done as usual, e.g. on the un-rolled computational graph\n27\n\nConvolutional \nNeural Networks \nCNN\nMotivations - image \nclassification\nIntuition behind a one-layer \nclassifier\n28\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nCNN Motivations\n\u2022 Initially motivated to face the challenges of object recognition in \nimages\n-\nviewpoint, zoom\n-\nillumination variability\n-\nobject deformation\n-\nobject occlusion\n-\nbackground noise\n-\nintra-class variability\n\u2022 General idea: let\u2019s define new type of layers and connections that \nwill bring\n-\npreservation of the spatial structure\n-\nhierarchical feature detection - objects are composed of features that are \nthemselves composed of other features\n-\nrobustness to object variabilities such as viewpoint, occlusion, etc\n29\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nConvolution\n\u2022 Convolution is a mathematical concept with a long history of applications in \nsignal (particularly, image) processing\n\u2022 Core idea: process a larger input over multiple activation, by applying a\nfilter iteratively to smaller areas of the input, scanning rows and columns\n\u2022 Of course where once these filters were handcrafted, in CNNs they are\nlearned through Backpropagation\n\u2022 The size of the model input is independent from data size\n\u2022 Processing a 4K video does not require a network with millions of inputs, \njust many applications of a network with e.g. 3\u00d7 3 inputs\nExample of a gaussian blur filter\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nMultidimensional convolution\n\u2022 Higher dimensional convolutions are intuitively possible, simply by using \nhigher-dimensional inputs and masks. A few example applications:\n1D: audio signal\n2D: grayscale image\n3D: color image (stack of 3 intensity matrices for red, green and blue)\n4D: video (sequence of color images)\n5D: scene (multiple videos of the same event from different perspectives)\n\u2022 Higher dimensional convolutions are still underused, mainly because:\n1 Computationally expensive\n2 The implementation in common libraries does not allow for more than three \ndimensions (plus accepting a fixed three-color-channels stack)\n3 Modern hardware is optimized for few dimensions: for example GPUs, \nstanding for Graphics Processing Units, are originally designed for processing \nsequences of 2D (+ color channels) images for screen rendering, but have \nbeen adopted for fast linear algebra over strictly 2D data\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nConvolution in practice\nh is the output, m the kernel size, x the input and w the convolution \nkernel\nh at 0,0 = 0*3 + 1*3 + 2*2 +\nh at 0,0 = 2*0 + 2*0 + 0*1 +\nh at 0,0 = 0*3 + 1*1 + 2*2\nh at 0,0 = 12\nh\nx\nw\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nImage representation\n33\nFrom Fei Fei Li - Stanford University School of Engineering - Class #2 on Image Classification  \n\nConvolution layer - 2D signals\n\u2022 The first idea is to preserve the spatial structure\n34\nimage 32x32x3\n\u201c3D matrix of pixels\u201d\nfilter 5x5x3\n\u201c3D matrix of weights\u201d\nImages in colour have \n3 \u201cchannels\u201d, Red-\nGreen-Blue RGB\nFilters have the same \ndepth as the input \n\u201cvolume\u201d\n3 5\n5\nFilters are also called \n\u201ckernels\u201d\nIn dense layers, we completely loose the information on the \nspatial structure as the output neurons are fully connected. \nE.g. we could shuffle the pixels randomly (but equally for all \nimages) and the network would perform the same.\n\nConvolution layer - 2D signals\n\u2022 The second idea is to convolve the filter with the image, in our case \u201cslide \nover the image spatially, computing dot products of the weights of the filter with \nthe window of pixels where we slide the filter.\n\u2022 The sub-area of an input that influences a component of the output is sometimes \ncalled the receptive field of the latter. \n35\nimage 32x32x3\n5x5x3 filter\n= 1 number\n= the results of computing the dot product \nbetween the weights of the filters and a small \n5x5x3 chunk of the image\n= 5*5*3 multiplication + bias\n\ud835\udc64!\ud835\udc65+ \ud835\udc4f\nSub-area of input = \nreceptive field\n\nConvolution layer - 2D signals\n\u2022 Notion of activation map that is the result of the convolution of \nthe filter, i.e. the sliding over all possible spatial locations of the \nimage.\n\u2022 This sliding of the filter gives us translation invariance regarding \nto what the filter is sensitive for\n36\nimage 32x32x3\n5x5x3 filter\nactivation map 28x28x1 \n\nConvolution layer - 2D signals\n\u2022 Notion of several filters producing several activation \nmaps\n\u2022 Now with another filter\n37\nimage 32x32x3\n5x5x3 filter\nactivation maps 28x28x2 \n\nConvolution layer - 2D signals\n\u2022 Notion of several filters producing several\nactivation maps\n\u2022 Let\u2019s say we have 6 5x5x3 filters, we\u2019ll get 6 maps\n38\nimage 32x32x3 \nactivation maps 28x28x6 \nconvolution\nlayer 6 filters \nof 5x5x3\n\nConvolution layer - 2D signals - channels\n\u2022 The input images have usually 3 channels, e.g. RGB\n\u2022 The filters have then the same depth as the input \u201cvolume\u201d\n\u2022 The activation maps have a depth equal to the number of filters\n39\nD filters\n1 filter : w x h x C\nC channels\n1 image : W x H x C\nD activation maps = depth\n1 map : (W-w+1)  x (H-h+1) x D\nImage: Fran\u00e7ois Fleuret , CAS Deep Learning, Idiap 2018\n\nConvolution layer - 1x1 convolution\n\u2022 1x1 convolutions are sometimes used\n-\nfilters apply along the depth of the input\n40\nImage from Fei Fei Li - Stanford University School of Engineering - CS231\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nConvolution layer - Padding\nVALID\nSAME\nFULL\n\u2022 The padding P specifies the size of a zeroed frame added around the \ninput.\n\u2022 Rationale:\n-\nApply filter at the border of the input\n-\nAvoid reducing the dimension from conv layer to conv layer\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nConvolution layer - Stride\nStrides affect the output size too!\nFinal formula:\no is the output, i the input size,\np the padding, k the kernel size,\ns the stride\ni=5, k=3, s=2, \np=1\ni=6, k=3, s=2, \np=1\n\u2022 The stride S specifies a step size when moving the filter across the \nsignal.\n\u2022 Rationale: reduce computation, reduce output size, no need of strong \noverlap\n\nConvolution layer - output size / # params\n\u2022 The output size can be computed with :\n\u2022 The number of parameters can be computed with:\n43\n\ud835\udc42! = \ud835\udc4a\u2212\ud835\udc64+ 2\ud835\udc43!\n\ud835\udc46\"\n+ 1\n\ud835\udc42# = \ud835\udc3b\u2212\u210e+ 2\ud835\udc43#\n\ud835\udc46$\n+ 1\n(W, H) = width and height of input\n(w, h) = width and height of filter\nP = padding, S = stride\n\ud835\udc41!\"#\"$ = \ud835\udc64\u210e\ud835\udc36\ud835\udc37+ \ud835\udc37\nC = number of channels\nD = number of filters\n5*5*3*6 + 6 =\n456 parameters\nweights            bias\n\n44\nRed\nGreen\nBlue\nAnimation from Fei Fei Li - Stanford University School of Engineering - CS231\n\nMax pooling layer\n\u2022 Reduce the spatial size of the representation\n-\nApplies independently to every depth, defined by a stride and size\n-\nRationale: \n-\nmost significant activations are kept, brings hierarchical approach \n-\nreduce the amount of computation and control overfitting\n45\nOther options: average pooling or L2-norm pooling.\nCommon settings: S=2, size=2 or S=2, size=3\nImages from Fei Fei Li - Stanford University School of Engineering - CS231\n\nOther layers\n\u2022 Dropout layer\n-\nThe layer drops out a random set of activations by setting them \nto zero\n-\nRationale: \n-\nmakes the network more robust, encourages redundancy\n-\navoids overfitting\n\u2022 Dense - fully connected layer\n-\nCan be viewed as a convolution layer with K filters spanning the \nfull input space\n-\nUsually used at the output of the CNN architecture to \u201cuse\u201d all \nthe features provided by the previous layer in order to take a \ndecision about a class label.\n46\n\nFull architecture for image recognition\n\u2022 Typical configuration: \n-\nstack sequences of CONV-RELU-POOL or CONV-RELU-CONV-RELU-\nPOOL\n-\nend with a fully connected \u201cdense\u201d layer to perform classification\n47\nImages from Fei Fei Li - Stanford University School of Engineering - CS231\n\nFurther readings\n\u2022 https://poloclub.github.io/cnn-explainer/\n48\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nWrap-up\n\u2022 Convolutional neural networks - CNN are deep \nneural network architectures composed of:\n-\nConvolution layers: principle is to convolve filters on the image \nwith the effect to provide spatial preservation and translation \ninvariance. A given filter is translated on  receptive fields of the \ninput and produces as output an activation map. The output of \nthe layer is usually a \u201cvolume\u201d corresponding to several \nactivation maps.\n-\nMax pooling layers: layers that are applied to the activation \nmaps of a previous layer in order to reduce its spatial dimension.\n-\nDense layers: regular fully connected layers usually used as \nthe last layers in the architecture to take classification \ndecisions\n49\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nAutoencoders\n1\n0\n2\n4\n2\n5\n6\n3\n2\n1\n0\n2\n4\n2\n5\n6\n3\n2\nStructure of an autoencoder: input size and output size are the same \nInput is also label: the network compresses then reconstructs the input\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n\u2022 Autoencoders are neural networks used for feature extraction and \ndimensionality reduction trained with what is called Self-Supervised Learning\n\u2022 The idea is to have two networks connected in sequence:\n\u2022 An encoder takes a high-dimensional input and outputs a compact code of it \nthrough a limited number of output neurons.\n\u2022 A decoder then uses the code to reconstruct the original image\n\u2022 The full network has though an hourglass shape, with the small, center layer \nproviding a compact code.\n\u2022 Training is straightforward: comparing the input with its reconstruction \nprovides a reconstruction error and enables the application of Supervised \nLearning algorithms such as Backpropagation\nAutoencoders\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nAutoencoders\n1\n0\n2\n4\n2\n5\n6\n3\n2\n1\n0\n2\n4\n2\n5\n6\n3\n2\n\u2022 Training target: minimizing reconstruction error between input and output\n\u2022 The activation of the central layer needs to be a faithful 32-dimensional \nrepresentation of the 1024-dimensional input data\n\u2022The output reconstructs the inputs, allowing us to compute the errors for \nthe (SL) backpropagation algorithm \n30\nEncoder\nCode\nDecoder\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n\u2022 Even though NNs are Universal Approximators, Backpropagation can only \nfollow the error gradient down to the nearest local optimum\n\u2022 Network architecture gives an upper bound for the trained model functional \ncomplexity: easy to overshoot model size, limiting potential performance\n\u2022 Large models easily overfit on noisy data, and backprop excels at that, \nremember to always have a strategy to mitigate overfitting\n\u2022 A few tips to mitigate these problems:\n\u2022 Keep the network as small as possible for your application\n\u2022 Stop training as soon as the error on the validation data set begins rising \n(early stopping)\n\u2022 Apply regularization (same as for SVMs)\nIt takes a bit of fiddling, but neural networks excel on noisy, non-linear\nproblems, especially when generalization is a must\nLimitations and applicability\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\n\u2022 Neural networks are a class of topics: in the next weeks we will explore Deep \nLearning then Reinforcement Learning, both still using neural networks in \ntheir state-of-the-art results\n\u2022 You now know enough to answer a very fundamental question: what is it so \nspecial about neural networks? Really, nothing in principle, but it is a proven \nand tested technology encompassing:\n\u2022 A parametrized generic function approximator\n\u2022 A graph-based computation flow, human interpretable\n\u2022 Easy scalability of functional complexity through network structure\n\u2022 Feature space composition quickly escalates function complexity\n\u2022 A reliable, optimized, time-proved SL training algorithm (backprop)\n\u2022 There is still so much more beyond this course, for example here are a few \nsuccessful RNN architectures: Elman networks, Jordan networks, Hopfield \nNetworks, Gated Recurrent Units (GRU), Long Short Term Memory \n(LSTM), Echo State Networks, Neural Turing Machines\nBeyond these lectures\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nThere is no such thing as \u201cjust use a neural network\u201d\nSummary\n\u2022 Neural networks are universal function approximators\n\u2022 Any computational graph is a valid neural network structure D\n\u2022 The structure describes a sequences of mappings between layers of neurons, \nmaking them easy to design, scale and adapt\n\u2022 Advanced networks utilize advanced tricks\n\u2022 Skip layers and sparse connectivity provide modularity\n\u2022 Recurrent connections provide memory and computational momentum\n\u2022 Convolutions recognize features from a larger input based on locality\n\u2022 Pooling aggregates information for translation invariance\n\u2022 Autoencoders are fundamentally UL and compactly encode their inputs\n\u2022 There is much more beyond this course, and more work is needed to address \ntasks beyond (current) neural network capabilities\n\nMSE - TSM-DeLearn\nMaster of Science\nin Engineering\nentropy-loss-function-come-from-ac3de349a715\nExtra material\n\u2022 Convolution from image processing:\nhttps://en.wikipedia.org/wiki/Kernel_(image_processing)\n\u2022 Stanford\u2019s tutorial on Convolution (and Pooling!): \nhttp://ufldl.stanford.edu/tutorial/supervised/\nFeatureExtractionUsingConvolution/\n\u2022 Stanford\u2019s tutorial on image filtering:\nhttps://ai.stanford.edu/~syyeung/cvweb/tutorial1.html\n\u2022 Understanding image kernels:\nhttps://setosa.io/ev/image-kernels/\n\u2022 Autoencoders (it\u2019s a whole field! Check it out!):\nhttps://en.wikipedia.org/wiki/Autoencoder\n\u2022 Cross-entropy:\nhttps://towardsdatascience.com/where-did-the-binary-cross-\n\n", "07.function_mappings.pdf": "07: Function Mappings\nProf. Philippe Cudr\u00e9-Mauroux\nslides by Dr Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 15, 2024\n\nWhere are we\n\u2022 Machine learning approximates the processes\nunderlying data with parametrized models\n\u2022 Data is de\ufb01ned by its features, and we can manipulate\nthem before training\n\u2022 Descriptive, aggregating features can represent the\ndata while highlighting speci\ufb01c characteristics\n1\n\nToday\u2019s menu\n\u2022 Function mappings\n\u2022 Word embeddings\n\u2022 Mercer kernels\n\u2022 Kernelization\n\u2022 Nonlinear SVMs\n2\n\nWhat is a mapping\n\u2022 A mapping \u03a6 is a formal description of the relationship between objects\nbased on the abstraction of a property\n\u2022 As an example, consider the process of map-making in actual geography:\nfeatures on the ground are abstracted and simpli\ufb01ed, then depicted in a\nrepresentation object, i.e. the map\n\u2022 Mapping is the process of generating the map: it represents the\nrelationship from the actual land to its printed abstraction\n3\n\nWhat is a mapping\nOriginal\nspace\nDestination\nspace\n\u03a6\nMapping\n4\n\nMappings, functions, relationships\n\u2022 From a mathematical standpoint, a mapping (or map) is represented by a\nfunction that describes the relationship between the original and destination\nspaces\n\u2022 All of our modeling so far can be described as a mapping of inputs to\nregressions or classi\ufb01cations, but the concept of mapping is broader than this\n\u2022 For example we used binary encoding to map a discrete feature space into a\nnumerical one, as an intermediate step\n\u2022 Data generation maps the inner state of the underlying process to a set of\nobservable features, for example through sensor readings or human\nobservation\nThe underlying function describes the real mapping\nModeling is the technique of approximating this mapping\n5\n\nInverse mapping\nOriginal\nspace\nDestination\nspace\n\u03a6-1\nInverse\nmapping\nAn inverse mapping is often possible:\ne.g. looking at the real land, searching for the features described in a map,\nis technically the inverse of the process of map-making.\nNot all processes can be inverted!\ne.g. freezing water vs. burning wood\n6\n\nMapping and ML\n\u2022 We can map features into new features: e.g. descriptive statistics map\nfeatures into their summaries, such as mean, max, count, etc.\n\u2022 Importantly: this process does not create new information. The\nquantity of learnable information remains unchanged, only its representation\nis novel. The new feature is redundant, but hopefully easier to use!\n\u2022 Modeling is itself a process, that maps the data to the space of function\napproximators (through learning). This map is relatively complex, as it\ninvolves searching for a parametrization that is evaluated in the context of a\ngiven model class\n\u2022 Ideally we would like to map directly an underlying process to its closest\nmathematical approximation; we use the data only because data is (by\nde\ufb01nition) the only way available to observe the process\n7\n\nCreating new features\nFeature\nspace\n(e.g. fruit pictures)\nFruit colors\nspace\nFruit widths\nspace\n\u03a6b\nmax width\navg pixel color\n\u03a6a\n8\n\nFeature extraction\n\u2022 Complex relationships can typically be deconstructed into multiple steps, like\ninstructions in a recipe.\n\u2022 Think for example about recognizing a fruit from an image. Each pixel is\nrepresented by three (color) features, corresponding to the intensities of the\nred, green and blue components making up its color. With a tiny 64x64\nimage and 3 color channels we already have 12\u2019288 features. Recognizing an\napple from an orange based on this staggeringly large 3D tensor is pretty\ncomplex.\n\u2022 Imagine now instead to map this feature space to a set of intermediate\nspaces, each describing a di\ufb00erent, higher level feature: fruit color, fruit\nshape, overall size, etc. Then mapping from these spaces to the destination\nspace of {apple, orange} classes would be much simpler!\n\u2022 Such process is called (confusingly enough) either feature mapping or\nfeature extraction.\n9\n\nFeature extraction\nFeature\nspace\n(e.g. fruit pictures)\nFruit colors\nspace\nFruit widths\nspace\n\u03a6b\nmax width\navg pixel color\n\u03a6a\n10\n\nIntermediate spaces\nOriginal\nspace\n(pixel intensities)\nIntermediate\nspace\n(main color)\n\u03a6a\nDestination\nspace\n( {apple, orange} )\n\u03a6b\nFeature\nmapping\nDecision\nmapping\n11\n\nIntermediate steps \u2192function composition\n\u2022 Describing complex relationships in a compact, single-step fashion is hard\n\u2022 Think about the complexity and performance of LDA vs. Na\u00efve Bayes, but\nalso their di\ufb00erence in modeling \ufb02exibility\n\u2022 In human experience, breaking down a problem into simpler components\ntypically makes it easier to address. The same holds in machine learning: we\nalready discussed decision trees and ensemble methods\n\u2022 Intermediate steps generate new, intermediate data features with higher\nabstractions, which improve aggregation or highlighting of the information\ntechnically already present in the original features\n\u2022 This type of sequence corresponds to function composition: a compact\nand e\ufb00ective way to raise quickly the functional complexity\nf : A 7\u2192B;\ng : B 7\u2192C;\ng(f (a)) = c,\na \u2208A, c \u2208C\n12\n\nFunction composition\nOriginal\nspace\n(pixel intensities)\nIntermediate\nspace\n(main color)\n\u03a6a\nDestination\nspace\n( {apple, orange} )\n\u03a6b\nFeature\nmapping\nDecision\nmapping\nDecision = \u03a6b(\u03a6a(orig))\n13\n\nNotable mappings\n\u2022 There are several notable examples of feature mapping across the history of\nmachine learning. The lack of a common generalized perspective has led to\nreinventing the same concept over and over also in this case. We will see\ntwo of the most important applications: word embeddings and kernels\n\u2022 \u201cEmbedding\u201d is just another word for mapping, as the original space is\nembedded into a destination space. It is most commonly used when\nencoding discrete features into continuous spaces\n\u2022 Lately the term embedding most commonly refers to word embeddings, as in\nNatural Language Processing (NLP): the discipline that studies\nhuman-computer interaction by means of natural (human) languages, either\nwritten or spoken\n\u2022 Some applications: machine translation, allowing communicating between\npeople who only speak di\ufb00erent languages; text summarization, providing\nsensible compact descriptions of larger texts; sentiment analysis, allowing\nthe study of the emotion and intention from writing cues; and many more\n14\n\nWord Embeddings\n\u2022 Word embeddings map words from a text (or corpus) to (relatively)\nlow-dimensional vector spaces: e.g. mapping a dictionary of tens of\nthousands of words to a 300-dimensional continuous vector space R300\n\u2022 Each word is thus represented by a vector of 300 numbers. This vectorial\nspace allows to numerically calculate similarities between words as simple\nvector distances (e.g. typically cosine similarity)\n\u2022 The vectors are updated iteratively by parsing the text corpus: (i) select\nwords that are close in a sentence (ii) update the values of the\ncorresponding vectors to make them a bit closer\n\u2022 The distance considered between words in the text is an hyperparameter.\nAlso the de\ufb01nition of \u201cword\u201d is used loosely here, better corresponding to\nn-grams: sequences of words that are commonly found together\n15\n\nEmbedding space\nParis\nBerlin\nRome\nFrance\nItaly\nGermany\nFlower\nCar\n\u2022 Think of vectors as n-dimensional\ncoordinates (here, n = 2)\n\u2022 Words that appear close in the text\nare given closer coordinates\n\u2022 Directions become meaningful\n16\n\nEmbedding space\nParis\nBerlin\nRome\nFrance\nItaly\nGermany\nFlower\nCar\nCountries\u00a0\nCapital\ncities\nParis\nBerlin\nRome\nFrance\nItaly\nGermany\nFlower\nCar\nOther\nconcepts\n17\n\nSummary\n\u2022 Mappings describe relationships between spaces, and are typically\nrepresented mathematically using functions\n\u2022 Feature maps generate new features by mapping existing features with\nad-hoc functions\n\u2022 Complex mappings can be split into intermediate steps by using sequences\nof feature mapping; the mapping can be reconstructed using function\ncomposition\n\u2022 Higher abstraction features can be more informative, in the sense that they\ncarry higher information in a more compact and explicit format.\n\u2022 Mappings do not (cannot) increase the available information, only change its\nrepresentation\n18\n\nExtra material\n\u2022 Mathematical de\ufb01nition of mapping:\nhttps://en.wikipedia.org/wiki/Map_(mathematics)\n\u2022 Wikipedia has a good page on NLP:\nhttps://en.wikipedia.org/wiki/Natural_language_processing\n\u2022 Good introduction to NLP: https://towardsdatascience.com/your-\nguide-to-\\natural-language-processing-nlp-48ea2511f6e1\n\u2022 Free dedicated NLP course on Coursera: https://www.coursera.org/\nspecializations/natural-language-processing\n\u2022 Cosine similarity:\nhttps://en.wikipedia.org/wiki/Cosine_similarity\n19\n\n", "05.naive_bayes.pdf": "   05: Na\u00efve Bayes\nProf. Philippe Cudre-Mauroux\nslides by Dr. Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP24]\nMarch 25, 2024\n\nWhere are we\n\u2022 Classical statistical analysis has been used for modeling\ndata for decades\n\u2022 LDA models the data under the assumption that it is\ngenerated by Gaussian probability distributions\n\u2022 LDA requirements and assumptions limit its\napplicability (e.g. same covariance for all classes)\n\u2022 We recently refreshed and used conditional\nprobabilities and Bayes\u2019 Rule to derive LDA\n1\n\nToday\u2019s menu\n\u2022 Better understanding of Bayes\u2019 Rule\n\u2022 Na\u00efve Bayes and Gaussian Models\n\u2022 How to model discrete features\n\u2022 Classical text analysis\n2\n\nMotivation\n\u2022 Linear Discriminant Analysis builds a statistical model of the data. This\nlecture focuses on a related method: the Na\u00efve Bayes (NB) Classi\ufb01er\n\u2022 Like LDA, it uses distributions to model the underlying function generating\nthe data. The result though is a nonlinear classi\ufb01er that is \ufb02exible and\nhuman-readable\n\u2022 Na\u00efve Bayes should not come o\ufb00particularly hard after understanding LDA\n\u2022 An entire branch of Machine Learning stems from it, still holding\nstate-of-the-art results, and is often integrated with Deep Learning\n\u2022 A key advantage that should not be overlooked is its interpretability: Na\u00efve\nBayes models are human-readable, and after training they can provide\nuseful insights on the data.\n3\n\nLDA assumptions\n\u2022 Each class should form a single cluster\n\u2022 All clusters/classes are generated by Gaussians\n\u2022 All Gaussians have the exact same covariance (\u03a3)\nThese assumptions typically are\noften incorrect, and limit\nLDA\u2019s ability to model the data\n4\n\nLDA limitations\n\u2022 Estimating the mean and prior is easy, but the covariance\nmatrix grows quadratically with the dimensionality (number of\nfeatures: p)\n\u2022 Empirical estimation of the covariance matrix (scatter matrix)\nactually requires at least n \u2265p samples, else degenerate\ncovariance (zero or very small eigen values: inversion and\ndecomposition become unstable)\n\u2022 Storing the covariance requires O(p2) memory: using \ufb02oats, for\nonly 10k features p = 105 it takes 40 GB of storage!\n\u2022 Prediction requires pairwise comparisons between classes, thus\ntaking O(|Y |2) operations: does not scale to problems with a\nhigh number of classes\n5\n\nConditional Probabilities and Bayes\u2019 Rule\nA fundamental theorem in probability theory with which you should be well\nacquainted by now is called Bayes\u2019 Rule\nLet A, B \u2282\u2126denote events in a probability space. Then the theorem connects\ntheir conditional probabilities as follows:\nP(A | B) = P(A) \u00b7 P(B | A)\nP(B)\n6\n\nExample on Conditional Probabilities\nExample: let\u2019s consider the following events A and B:\n\u2022 A: a student has understood the contents of a lecture (hypothesis)\n\u2022 B: a student passes the exam (observation)\n\u2022 These two events are hopefully correlated, but not identical\nTo be formal: B is a test for A\nThe test can fail! (Remember the confusion matrix?)\n\u2022 False positive: the student has passed without understanding the essentials\n\u2022 False negative: the student has failed despite a solid understanding\nLet\u2019s assume the following numbers over 100 students:\nB (passed)\nnot B\ntotal\nA (understood)\n65\n5\n70\nnot A\n10\n20\n30\ntotal\n75\n25\n100\n7\n\nConditional Probabilities and Bayes\u2019 Rule\n\u2022 Why do we have exams? They allow testing the student\u2019s understanding\n(event B) without the need to open up each student\u2019s head and look inside\nfor true understanding (event A)\n\u2022 The real world is only ever partially observable. Conditional tests allow us to\nverify measurable conditions and draw statistically signi\ufb01cant deductions on\nunobserved processes (such as underlying functions).\n\u2022 If a company is looking to hire a student with knowledge of Machine\nLearning, they may typically look for a student who passed the exam.\n\u2022 Is this reasoning meaningful? Let\u2019s estimate the probability of a student\nbeing knowledgeable, based on the fact that the exam was passed, i.e.\nthe posterior distribution P(A | B).\n\u2022 Since we select a student who passed the exam, this becomes:\nP(A | B) = P(A, B)\nP(B)\n= 65\n75 = 86.7% \u2190sounds reasonable!\n8\n\nA\nB\nA and B\nP(A, B) = P(A | B) P(B)\n\u21d2\nP(A | B) = P(A, B)\nP(B)\n9\n\nUnderstanding Bayes\u2019 Rule\nB (passed)\nnot B\nsum\nA (understood)\n65\n5\n70\nnot A\n10\n20\n30\nsum\n75\n25\n100\nP(A | B)\n=\nP(A, B)\nP(B)\n=\nP(A, B)\nP(B)\n\u00b7 P(A)\nP(A)\n=\nP(A, B)\nP(A)\n\u00b7 P(A)\nP(B)\n=\nP(B | A) \u00b7 P(A)\nP(B)\n=\nP(A) \u00b7 P(B | A)\nP(B)\n=\n70 \u00b7 65\n70\n75\n= 86.7%\n10\n\nUnderstanding Bayes\u2019 Rule\nP(A | B) = P(A) \u00b7 P(B | A)\nP(B)\nThe posterior can hence be calculated as:\n\u2022 The prior P(A): probability that the student has understood the material\n(i.e. the probability we would have when hiring the student independently of\n/ ignoring the test), ...\n\u2022 ... times the likelihood P(B | A): probability that he passed the exam,\nprovided he understood the material (i.e. the probability of passing the test\nconditioned of success), ...\n\u2022 ... normalized by the evidence P(B): probability of passing the exam (i.e.\ntake into account the fact that the student passed the exam).\n11\n\nBayesian Approach to Classi\ufb01cation\n\u2022 The Bayes formula yields an optimal classi\ufb01cation rule for a given\nprobabilistic model.\n\u2022 Consider for example the LDA model for the joint distribution of x and y,\nwith density:\np(x, y) =\n1\np\n(2\u03c0)p \u00b7 det(\u03a3) \u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u00b5y)T \u03a3\u22121(x \u2212\u00b5y)\n\u0013\n\u00b7 \u03c0y\n\u2022 Event A: the true label is y.\n\u2022 Event B: we encounter a certain input x.\n12\n\nBayesian Approach to Classi\ufb01cation\n\u2022 Bayes optimal prediction: return the label y with maximal probability:\n\u02c6\ny = argmax\ny\u2208Y {P(y | x)}\n\u2022 Notice how this in practice minimizes the chance of making a mistake, as we\nmaximize probabilities based on what we know as true (x).\n\u2022 Since we observe (i.e. have evidence for) x, we maximize the conditional\nprobability P(y | x) with respect to y.\n\u2022 According to Bayes rule this corresponds to:\nP(y | x) = P(y) \u00b7 P(x | y)\nP(x)\n\u221d\nP(y) \u00b7 P(x | y)\n\u2022 Na\u00efve Bayes maximizes this term over all possible labels y.\nFor this purpose the denominator (the evidence, P(x)) can be dropped,\nbecause it does not depend on y (hope that sounds familiar...).\n13\n\nBayesian Approach to Classi\ufb01cation\n\u2022 We need to compute the prior P(y) and the likelihood P(x | y) for each\npossible label y.\n\u2022 We saw the Gaussian probabilistic model with LDA: P(y) = \u03c0y, and\np(x | y) =\n1\np\n(2\u03c0)p \u00b7 det(\u03a3)\u00b7 exp\n\u0012\n\u22121\n2(x \u2212\u00b5y)T \u03a3\u22121(x \u2212\u00b5y)\n\u0013\n.\n\u2022 Maximizing P(y | x) \u221dP(x | y) P(y) yields the already familiar LDA\ndecision rule.\n14\n\nThe Na\u00efve Bayes Model\n\u2022 In principle we could proceed the same way for other probabilistic models. In\npractice though, using more \ufb02exible models than the above Gaussian quickly\nmakes the calculations intractable.\n\u2022 Na\u00efve Bayes thus opts for a simpler (yet \ufb02exible!) model, that scales to large\ndimensions p. This is achieved by means of making a drastic assumption:\nWe assume that all components (features) for an input vector x \u2208Rp are\nstatistically independent, conditionally on the label y.\n\u2022 This means that the value for one feature of one point of one class has\nnothing to do with (i) the values of any of the other features of the same\npoint, nor with (ii) the value of the same feature in a point from another\nclass. (\u2190Make sure you understand this!)\n\u2022 This allows computing the conditional probability P(x | y) as a simple\nproduct of independent probabilities for each xi for each y.\n15\n\nThe Na\u00efve Bayes Model\n\u2022 Let x = (x1, . . . , xp) be the p feature values of point x.\nStatistical independence between features means assuming:\nP(x | y) = P(x1 | y) \u00b7 P(x2 | y) \u00b7 . . . \u00b7 P(xp | y),\n\u2200y \u2208Y\n\u2022 The Na\u00efve Bayes (NB) model: rather than a p-dimensional probabilistic\nmodel, have instead an ensemble of independent one-dimensional\nprobabilistic models, each describing the distribution of one feature for the\npoints of one class.\n\u2022 It comes at the price of a strong assumption, namely the statistical\nindependence of the features. Since this is rarely the case in real\napplications, what happens is that we decide to explicitly ignore the\ndependencies (i.e. correlations) of the features.\n\u2022 We lose some modeling power in exchange for a lot more \ufb02exibility and\nperformance.\n16\n\nGaussians for Na\u00efve Bayes\n\u2022 Let\u2019s consider again Gaussian models by assuming P(xi | y) = N (\u00b5i, \u03c32\ni )\nwith parameters \u00b5i \u2208R and \u03c3i > 0.\n\u2022 This generates a di\ufb00erent Gaussian for each feature i and for each class y.\nStill, this model has only O(p) parameters (in the normal case of p >\n> |y|)\nvs. O(p2) of LDA (which maintains a full covariance matrix).\n\u2022 The Gaussians look akin to what seen with LDA, but restricted to diagonal\ncovariance matrices:\n\u00b5 =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\u00b51\n\u00b52\n.\n.\n.\n\u00b5p\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u03a3 =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\u03c32\n1\n0\n. . .\n0\n0\n\u03c32\n2\n. . .\n0\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n0\n0\n. . .\n\u03c32\np\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u2022 We lose the ability to model inter-feature correlation, but we estimate an\nindependent (diagonal) covariance matrix for each class (vs. having a\nsingle common for LDA).\n17\n\nLDA: all classes/clusters have the same\ncovariance (width, height and rotation)\nNB: each class/cluster has custom \u03c3x1\nand \u03c3x2, but no covariance (no rotation)\n18\n\nGaussians for Na\u00efve Bayes\n\u2022 The independence assumption actually allows for more \ufb02exibility\n\u2022 Estimating one-dimensional distributions is really easy (compared to\nestimating multi-dimensional distributions)\n\u2022 We can then a\ufb00ord to estimate more complicated distributions, with\nmore parameters and speci\ufb01c properties tailored to the feature\u2019s type\n\u2022 This allows us to drop LDA\u2019s assumption that each class needs to form\nonly one, homogeneous cluster Instead the class distributions can be\nmulti-modal (i.e. have multiple peaks).\n19\n\nExample: mixture of Gaussians\ng\n\u2211\nj=1\nai,j \u00b7\n1\n\u221a\n2\u03c0 \u03c3i,j\nexp\n \n\u2212(xi \u2212\u00b5i,j)2\n2\u03c32\ni,j\n!\nThis corresponds to a model with (3 g) parameters per feature and per class.\n20\n\nNa\u00efve Bayes and Discrete Features\n\u2022 A feature xi is said to be discrete if it can only take one out of k possible \nvalues (as seen before)\n\u2022 Most models cannot handle discrete features directly: this is typically \nhandled by encoding the feature into a real value \n\u2022 Na\u00efve Bayes instead can model independently the probabilities pi,1, . . . , \npi,k \nfor each of the k possible values of the discrete feature i (for each class y)\n\u2022 We will see in detail how to directly handle two common types of discrete \nfeatures: (i) modeling binary (or boolean) features with Bernoulli \ndistributions, and (ii) modeling counter features with binomial distributions\n\u2022 Na\u00efve Bayes\u2019 independence assumption makes it possible (and easy) to \nintegrate multiple distributions, ideally with few parameters each, and \ntargeted speci\ufb01cally to each feature\u2019s type of data\n21\n\nBinary Features (k = 2): Bernoulli Distribution\n\u2022 Binary features typically represent the presence (or truth) of a property.\nThey are arguably the most common type of discrete features\n\u2022 Multiple binary classi\ufb01cations can be used for multiclass classi\ufb01cation (as\nseen before)\n\u2022 Na\u00efve Bayes models them directly by using Bernoulli distributions: easy\nand e\ufb03cient, they are designed on purpose to handle binary data, making\nthem perfect for this role\n\u2022 Each distribution requires estimating a single parameter pi = pi,1 (since\npi,2 = 1 \u2212pi). This is straightforward: pi is the number of data points with\nfeature i in the data (for each class), divided by the total number of data\npoints (for each class).\nThis corresponds to the maximum likelihood estimator\n22\n\nCounter Features: Binomial Distribution\n\u2022 Remember that features can be extracted from the data using statistical\nanalysis (mean, max, min, etc.). A simple example is a counter feature,\nwhich measures the number of occurrences of an event in the data\n\u2022 Take for example a text document: the number of occurrences of the term\n\u201cmachine learning\u201d in it could constitute a useful counter feature to classify\nthe document\n\u2022 The binomial distribution is designed for counter data, making it ideal to\nmodel such features. It hypothesizes that the occurrences of the counted\nentities are independent of each other; i.e. ideal to model counts of the word\npair \u201cmachine learning\u201d, less so if \u201cmachine\u201d and \u201clearning\u201d are separate\npossible values since their connection is lost\n23\n\nCounter Features: Binomial Distribution\n\u2022 For e events occurring independently with probability p, the number of\noccurrences follows the binomial distribution with parameters e and p. Let\n\u02c6\nxi denote one particular value of ith feature xi, and c the number of\noccurrences (count) of \u02c6\nxi (for points in class y). Then it holds:\nP(xi = \u02c6\nxi) =\n\u0012e\nc\n\u0013\npc(1 \u2212p)e\u2212c\n\u2022 A fundamental assumption is that the value that is being counted can\nappear anywhere in the document with the same probability, and each\noccurrence is independent of the content of other locations in the text\ndocument (i.i.d., remember?)\n\u2022 Alternatively, the multinomial and Poisson distributions are other types of\ncounter distributions also applicable in this context\n24\n\nRegularized Na\u00efve Bayes\n\u2022 Problem: with both binary and counter features, if one (or several) of the\npossible k values is missing from the training set, this estimator sets the\ncorresponding parameter pi to zero\n\u2022 This is not inherently wrong: in its empirical evidence, the value never\noccurs. Training a model always uniquely depends on the available data (as\nseen before)\n\u2022 However, this is obviously an implausible model. For any training set\n(especially if small), we can always assume that some values will remain\nunobserved. This does not mean that they will not appear in the\n(population) data eventually\n\u2022 Because of the product between probabilities, setting pi = 0 results in\nP(y | x) = 0 even if x is a perfect match in all other features. Setting any\nof the probabilities to zero is a really bad idea\n25\n\nRegularized Na\u00efve Bayes\n\u2022 This problem is solved with the introduction of regularization: the model is\nsmoothened (made more regular), by adding a pseudo-observation to all\nfeature counts. This corresponding to mitigating the learning, aiming at\nbetter generalization towards unseen data.\n\u2022 Regularization adds 1/n to all probabilities, thus pretending that each\nfeature value was observed at least once\n\u2022 For better control we can use \u03b1/n, where \u03b1 \u22650 is a parameter set by the\nuser. This is our \ufb01rst hands-on example of a hyperparameter, more\nspeci\ufb01cally a regularization parameter\n\u2022 Controlling regularization using a hyperparameter for custom tuning is very\ncommon in machine learning, particularly in correspondence of the most\nadaptable and generalizing learning methods\n26\n\nDiscretizing Continuous Features\n\u2022 Since Na\u00efve Bayes is so \ufb02exible with\ndiscrete features, can we apply the\nsame reasoning to continuous\nfeatures?\n\u2022 We can discretize the continuous\nvalues into k discrete bins (ranges,\nintervals) (as seen before:\nhistograms of continuous data)\n\u2022 The number of bins and their\nboundaries (starting point and size)\nare arbitrary. The strategy for\nde\ufb01ning them typically introduces\nfurther hyperparameters\n27\n\n\u2022 Each column width corresponds to a range on x\n\u2022 Each column height corresponds to the number of points in that range\n28\n\nText Document Classi\ufb01cation\n\u2022 Today most text documents are written and processed using computers.\nThis makes them readily available in a machine-readable format.\n\u2022 Recent advancements in Optical Character Recognition (OCR) applications\nalso makes paper-only document, such as handwritten or historical material,\neasily available for digital import.\n\u2022 Classifying text documents into di\ufb00erent categories has a number of\napplications: news categorization, sentiment analysis, topic classi\ufb01cation,\nreview rating, etc.\n\u2022 A classic example still valid today is an email spam \ufb01lter. Its task is to\nclassify spam from non-spam, making it a binary classi\ufb01cation problem.\n29\n\nClassical Text Analysis\n\u2022 Text documents constitute non-numerical data, thus cannot be processed\ndirectly: they need to be translated (transformed, converted, mapped,\nencoded) into vectors of features.\n\u2022 Bag-of-words: de\ufb01ne a list of keywords, then count the number of\noccurrences of each keyword in the text. This generates a series of counter\nfeatures with speci\ufb01c, independent information, making the document ready\nfor classi\ufb01cation.\n\u2022 For large lists of keywords, the resulting feature vectors can become both\nvery high-dimensional and very sparse. Example: a list of p = 103 keywords\ngenerates 103 features, but most values will be zero since most email\ncontain only a small subset of the keywords list.\n\u2022 This can be applied to e\ufb03cient storage of large texts in preparation for\ncount-based analysis. Rather than full text, simply store (word, value) pairs\nonly for non-zero occurrences, requiring O(k) space with k di\ufb00erent words\nin the text, independently of the dictionary size p.\n30\n\nClassical Text Analysis\n\u2022 Word frequencies are typically not independent: if the word \ufb01nance is\npresent in a text, we would not be surprised to \ufb01nd the word bank too.\n\u2022 Furthermore, isolated words may not be enough for predicting the topic of a\ntext. Combinations of words in near proximity in the text are much stronger\nindicators: compare New \u2192New York \u2192New York Times.\n\u2022 A common technique to account for the co-occurrence of words, by\nconsidering n-grams: sequences of n words that appear consistently\ntogether in the text.\n\u2022 This blows up the feature dimension: with barely 103 words, for 2-grams we\nhave 106 features, for 3-grams already 109 features. The data is even more\nsparse, while the storage requirements remain unchanged.\n\u2022 Such feature sets are no problem for Na\u00efve Bayes models since each feature\nis modeled with extreme e\ufb03ciency, while they are impossible to handle with\nLDA as its performance grows with the square of the feature dimensionality.\n31\n\nTowards Modern Text Analysis\n\u2022 Counter features are an example of feature mapping: using transformation\nfunctions to map data from its original space (e.g. text) into a more\nrepresentative (meaningful, informative) feature space (e.g. the number of\noccurrences of each word in a keyword list).\n\u2022 The main limitation of keywords list counter features are (i) they are\nuser-de\ufb01ned, while automation would be preferable, and (ii) being\nhigh-dimensional and sparse their information density is low; having a\ndenser, more informative representation could be better.\n\u2022 Modern applications rely instead on embeddings, high-dimensional spaces\ncreated by feature mappings that maintain and highlight speci\ufb01c qualities in\nthe data (such as distance between words in a text corpus). We will dig\ndeeper over the next lectures, so start early with building an intuition for the\nconcept.\n32\n\nSummary\n\u2022 We have started by generalizing LDA to arbitrary probabilistic models.\nBayesian inference with complicated models quickly becomes\ncomputationally intractable.\n\u2022 We mitigate this by introducing an assumption of independence between\nfeatures and between classes. This reduces the complexity to the estimation\nof many independent one-dimensional models, which is a relatively easy task\nand scales only linearly with the number of features.\n\u2022 The resulting Na\u00efve Bayes classi\ufb01er is \ufb02exible in the choice of the family of\nthese one-dimensional distributions. We have seen Gaussians and binomial\ndistributions, and hinted at more.\n\u2022 Na\u00efve Bayes classi\ufb01ers scale gracefully to high-dimensional spaces. They are\napplicable to very large numbers of features, making them viable for text\ndocument classi\ufb01cation. Spam email detection for example was until\nrecently only possible thanks to these methods.\n33\n\nExtra material\n\u2022 Play with mentioned distributions (and the links themselves!):\nfor name in [\"Gaussian\", \"Bernoulli\", \"binomial\"]:\nvisit(f\"wolframalpha.com/input/?i={name}%20distribution\")\n\u2022 Full Bayes derivation:\nhttps://www.probabilisticworld.com/anatomy-bayes-theorem/\n\u2022 What\u2019s wrong with these people (frequentists vs. Bayesians):\nhttps://stats.stackexchange.com/questions/22/bayesian-and-\n\\frequentist-reasoning-in-plain-english\n(scroll down, there are so many deserving, upvoted answers)\n\u2022 The closer you look, the more \ufb02ames you \ufb01nd (e.g. objectivism vs.\nsubjectivism)\nhttps://stats.stackexchange.com/questions/381825/objective-\n\\vs-subjective-bayesian-paradigms\n\u2022 Lying with statistics: numbers don\u2019t tell stories, people do\nhttps://www.youtube.com/watch?v=bVG2OQp6jEQ\n34\n\n", "07.kernels_supplement.pdf": "07: Kernels Supplement\nDr Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 15, 2024\n\nHilbert Space\nLet H be a vector space over R. A function \u27e8\u00b7, \u00b7\u27e9: H \u00d7 H \u2192R is an inner\nproduct on H with three properties:\n\u2022 The function is symmetric in its arguments: \u27e8f , g\u27e9= \u27e8g, f \u27e9\n\u2022 The function is linear in its arguments:\n\u27e8\u03b21f1 + \u03b22f2, g\u27e9= \u03b21\u27e8f1, g\u27e9+ \u03b22\u27e8f2, g\u27e9\n\u2022 The function is positive de\ufb01nite: \u27e8f , f \u27e9\u22650and \u27e8f , f \u27e9= 0 if and only if\nf = 0.\n\u2022 The vector space H is a Hilbert space if the dimension of the space is not\nnecessarily \ufb01nite (It contains Cauchy sequence limits).\n1\n\nReproducing Property\nDe\ufb01nition: A real-valued function k : X \u00d7 X \u2192R is a kernel of a Hilbert space\nH if and only if \u2200x \u2208X, \u2200f \u2208H:\n\u27e8f , \u03c6(x)\u27e9= \u27e8f , k(\u00b7, x)\u27e9= f (x).\n\u2022 You read \u27e8f , k(\u00b7, x)\u27e9as the inner product of a kernel function k(\u00b7, x) with\nthe \ufb01xed x and an arbitrary variable that is denoted by \u00b7\n\u2022 Consequently the relevant Hilbert space H and the kernel k are called the\nReproducing Kernel Hilbert Space (RKHS) and the reproducing kernel\nrespectively.\n\u2022 In simple words reproducing property means the value of function f for\ninput x is equal to the dot product between function f and the embedded\npoint \u03c6(x) in H, where \u03c6(x) = k(\u00b7, x).\n2\n\nKernel Trick\n\u2022 Using the reproducing property we obtain:\n\u2200x, y \u2208X, k(x, y)\n=\n\u27e8\u03c6(x), \u03c6(y)\u27e9\n=\n\u27e8k(\u00b7, x), k(\u00b7, y)\u27e9\n\u2022 This dot product de\ufb01nition of a kernel provides a straightforward and\ne\ufb03cient computation with no need for explicitly de\ufb01ning \u03c6(\u00b7).\nThis is called the kernel trick.\n\u2022 Kernel trick is very useful in real-world problems where \u03c6 is a complex and\ntime consuming function to de\ufb01ne. Some time you may not even be able to\nde\ufb01ne this function explicitly.\n3\n\nExample\nClass 1\nClass 2\nFeature Map\n\u03a6 : t \u21a6\ue234\n\u008d2\n\u008d1\n\u008d1\n\u008d2\n\u008d3\n\u03c6 : R2 \u2192R3\nx = (x1, x2) 7\u2192\u03c6(x) = (x2\n1,\n\u221a\n2x1x2, x2\n2 )\nk(x, y) = (x2\n1,\n\u221a\n2x1x2, x2\n2 )(y2\n1,\n\u221a\n2y1y2, y2\n2 )T\n4\n\nExample\nA function f of features x2\n1,\n\u221a\n2x1x2, x2\n2 is:\nf (x) = ax2\n1 + b\n\u221a\n2x1x2 + cx2\n2\nHere f is a member of a Hilbert space that maps X : R2 \u2192R. An equivalent\nrepresentation for f is: f (\u00b7) = (a, b, c).\nNote. You can denote f instead of a, b, c with f (\u00b7) = (f1, f2, f3) for instance.\nf (\u00b7) refers to the function itself or here it is a vector in R3, and f (x) \u2208R refers\nto the function evaluated at a particular point x. In the other words it is just a\nreal value.\nf (x) = f (\u00b7)\u03c6(x)T := \u27e8f (\u00b7), \u03c6(x)\u27e9\nEvlauation of f at x is written as an inner product in the feature space, which is\na R3 in our example.\n5\n\nExample\nIf someone gives us y \u2208X, corresponding to that y, we have a vector in\nk(\u00b7, y) \u2208H such that:\nk(\u00b7, y) = (y2\n1,\n\u221a\n2y1y2, y2\n2 ) = \u03c6(y)\nThe relation between x, y \u2208X in H subsequently can be written as:\n\u27e8k(\u00b7, y), \u03c6(x)\u27e9= ax2\n1 + b\n\u221a\n2x1x2 + cx2\n2\nWhere a = y2\n1, b =\n\u221a\n2y1y2, c = y2\n2.\nDue to symmetric property of kernel functions we can write:\n\u27e8k(\u00b7, x), \u03c6(y)\u27e9= uy2\n1 + v\n\u221a\n2y1y2 + wy2\n2 = k(x, y)\n6\n\nRepresenter Property\nFunctions in the RKHS can be written as linear combinations of feature maps,\nf (\u00b7) := \u2211n\ni=1 \u03b1ik(xi, \u00b7), thus evaluation of function f at point x is equal to:\nf (x) :=\nn\n\u2211\ni=1\n\u03b1ik(xi, x) =\nn\n\u2211\ni=1\n\u03b1i\u27e8\u03c6(xi), \u03c6(x)\u27e9\n= \u27e8\nn\n\u2211\ni=1\n\u03b1i\u03c6(xi), \u03c6(x)\u27e9= \u27e8f (\u00b7), \u03c6(x)\u27e9\nIf you have only one point n = 1 and \u03b11 = 1 then according to the\nabovementioned formula we have:\nf (x) = k(x1, x) = \u27e8k(x1, \u00b7), \u03c6(x)\u27e9= \u27e8k(x, \u00b7), \u03c6(x1)\u27e9\nIt is possible to write functions of in\ufb01nitely many features, e.g. Gaussian kernel.\nf (x) =\n\u221e\n\u2211\nm=1\nfm\u03c6m(x) = \u27e8f (\u00b7), \u03c6(x)\u27e9\n7\n\nRepresenter Property\n\u2200x, y \u2208X and \u2200f \u2208H we have:\n\u03c6(x) = k(\u00b7, x) \u2208H\n\u03c6(y) = k(\u00b7, y) \u2208H\n\u27e8f , k(\u00b7, x) = f (x)\nk(x, y) = \u27e8k(\u00b7, x), k(\u00b7, y)\u27e9\n8\n\nNorm\nNorm of a function in Hilbert space H is de\ufb01ned as:\n\u2225f \u2225H =\nq\n\u27e8f , f \u27e9H\nThe weight function w \u2208H can be written as linear combinations of feature\nmaps, w = \u2211n\nj=1 \u03b2jk(xj, \u00b7). Therefore we have:\n\u2225w\u22252 = \u27e8w, w\u27e9= \u27e8\nn\n\u2211\nj=1\n\u03b2jk(xj, \u00b7),\nn\n\u2211\ni=1\n\u03b2ik(xi, \u00b7)\u27e9\n= \u27e8\nn\n\u2211\nj=1\nn\n\u2211\ni=1\n\u03b2j\u03b2ik(xj, xi)\u27e9\nTo review your linear algebra you can refer to:\nhttp://cs229.stanford.edu/section/cs229-linalg.pdf\n9\n\nKernel Matrix\nA kernel matrix for two training data points x = (x1, x2, x3) and y = (y1, y2, y3)\nis equal to a kernel matrix K of size 2 \u00d7 2 with entries:\nk(x,x) k(x,y)\nk(y,x) k(y,y)\nHere we assumed the row vector is x = (x1, x2, x3) of dimension 1 \u00d7 3 and a\ncolumn vector (x1, x2, x3)T is of size 3 \u00d7 1. In this way for a training dataset\nwith n data points of 3 dimensions we have a input mtrix of size n \u00d7 3 and the\ncorresponding kernel matrix is of size n \u00d7 n.\nIf we use the linear kernel function here, the value of kernel between points x, y is\nobtained by k(x, y) = \u27e8x, yT\u27e9.\nNote. Sometimes the author may use di\ufb00erent notation such that a data point\nis considered as a column vector of size 3 \u00d7 1 and therefore the dataset is of size\n3 \u00d7 n. Consequently the linear kernel value between points x, y is obtained by\nk(x, y) = \u27e8xT, y\u27e9.\n10\n\n", "01.foundations_and_sl.pdf": "01: Foundations and Supervised\nLearning\nMachine Learning\nBachelor in Computer Science [SP24]\nFebruary 26, 2024\n\nToday\u2019s menu\n\u2022 Foundations of ML\n\u2022 Models and parametrizations\n\u2022 Supervised learning\n\u2022 Error, loss and risk\n\u2022 Under\ufb01tting and over\ufb01tting\n\u2022 Simplest learning\n1\n\nWhat is Machine Learning\nProgramming means we tell a machine\nexactly what to do to solve a problem.\nMachine learning on the other hand\nmeans we tell the machine how to\nimprove at producing solutions to a\nproblem.\nThe goal of machine learning is to\nautomate the learning mechanism. \u2192\ngeneralize to unseen applications.\nThe product of machine learning is a\nlearned solution. \u2192generalize to unseen\ndata on the same application.\nProblem: formulation of what we want\nto achieve\nSolution: acceptable answer to a\nproblem\nML research: search for new ways to\nlearn solutions\nML application: use ML to \ufb01nd the\nsolution to a problem\nGeneralization: the ability to correctly\napply learned information beyond the\noriginal data\n2\n\nModels and parameters\nMachine learning uses a model plus a\nparameter set to describe a solution.\nExample: we have two data points and\nwe want to know the equation of the\nline that generates and generalizes them.\nh : R \u2192R\ny \u2208R, x \u2208R\ny = mx + q\ny = mx + q\ny = 2x \u22121\nIn\ufb01nite lines\nOne example, de\ufb01nite line\nModel: parametric equation which\nde\ufb01nes a class of functions\nParameter set: set of numerical\nconstants identifying model instances\n(speci\ufb01c functions)\nSolution (ML): a generic model plus a\nspeci\ufb01c parametrization\nCareful with the word model, as it is casually\nused to describe either the class of functions\nthat can be learned (the proper model) and the\nsolution composed of model and \ufb01xed\nparameters set (as in \u201cthe learned model\u201d). It\nshould be clear by context.\n3\n\n\u201cLearning\u201d a linear model using exact methods\nA = (2, 3); B = (4, 7)\n(\n3 = 2m + q\n7 = 4m + q\nm = 2; q = \u22121\ny = 2x \u22121\nModel: y = mx + q\nParameters (m, q): (2, \u22121)\nSolution (learned model): y = 2x \u22121\n4\n\nWhat are the applications of a model after learning?\n\u2022 Prediction: interrogate model on new input (what is y for x = 3?)\n\u2022 Compression: the line has in\ufb01nite points, but a compact form\n\u2022 Classi\ufb01cation: the line divides points lying above and below\n(interpreted as two classes)\n\u2022 Anomaly detection: is a new point what we expected? Or did\nsomething happened of which we are unaware?\n\u2022 More applications to follow!\ny = 2x \u22121\nFor input x = 0\npredict value of y\nx = 0 \u2192y = \u22121\n5\n\nA functional representation of reality\nThe fundamental assumption encompassing all of Machine Learning is that\nthere is an underlying function that generates the data.\n\u2192In principle not available to us for study (else exact methods).\n\u2192Has regularities: learn them to approximate the function.\nIf we have the function we could know any data point\never generated, and predict any that will be ever generated.\nMachine learning represents processes as functions, using\nfunction approximators and parameter sets.\n6\n\nUnderstanding Machine Learning\nTo understand a machine learning application ask yourself what is:\n\u2022 The model: the class of functions that can be learned\n\u2022 The parametrization: the set of numbers that can be adapted for learning\nand how do they in\ufb02uence the mathematical expression of the model\n\u2022 The learning algorithm: the method used to update the parameters with\nthe objective of generating a better solution\nSo far we saw a model (y = mx + q) and a parametrization (m = 2, q = \u22121).\nWe generated (learned!) the parametrization using closed-form equations.\nThere are many classes of problem where closed-form and exact methods are not\napplicable, or not su\ufb03ciently performant.\n7\n\nLimitations of exact systems\nWhen there are more data points than\nunknown parameters the system is\noverdetermined. There is typically not\na single line passing through all of the\npoints, which makes exact methods not\nwork.\nStill, the data is clearly generated by a\n(noisy) linear function: we can learn an\napproximation of this function from the\ndata using Supervised Learning.\n8\n\nSupervised Learning\n\u2022 The goal of supervised learning is to infer a function h : X \u2192Y\n(hypothesis) based on available correct data.\n\u2022 X is called input space.\nAn element x \u2208X is called an observation, or an input.\n\u2022 Y is the decision space.\nThis can be a continuous space such as Y = R (or Y = Rn) which makes a\nregression problem; or a discrete, \ufb01nite space (e.g. Y = {\u03b1, \u03b2, \u03b3}) which\nmakes a classi\ufb01cation problem.\n\u2022 Assume we have a dataset of pairs (x, y) \u2208X \u00d7 Y\nwhere x is the input and y the expected correct output (label or target)\n\u2022 We can use the data as a training data set D =\n\b(xi, yi), i \u2208{1, . . . , n}\n\t\nto learn the relationship between inputs and targets\n9\n\nSupervised Learning\n\u2022 Supervised learning is part of a family of learning paradigms with\nunsupervised learning and reinforcement learning.\n\u2022 Supervised learning has received more study over the years and is so far the\nmost successful, but strictly depends on the availability of correctly labeled\ndata\n\u2022 You already know some of the big players in Machine Learning, such as\nGoogle, Yahoo, Microsoft, IBM, but also social network companies like Meta\n(FB), Twitter and YouTube\n\u2022 These companies maintain entire research centers dedicated to machine\nlearning, and the vast majority of the work you hear about over the news\nemploys supervised learning.\n10\n\nApplications\nSome applications of\nsupervised learning:\n\u2022 Handwritten digit recognition\n(MNIST data set)\n\u2022 Face detection, face recognition\n\u2022 Tra\ufb03c sign recognition\n\u2022 Search engine optimization\n11\n\nGoal of Learning: Loss and Risk\n\u2022 For (x, y) \u2208(X, Y ) let \u02c6\ny = h(x):\n\u2022 y is the correct label for x (target)\n\u2022 \u02c6\ny is the output of the model h (prediction)\nWhat if they are di\ufb00erent?\n\u2022 Example: fruit classi\ufb01cation. Let x be an image of an apple, so y = apple\nrepresents the correct output. If our model outputs h(x) = \u02c6\ny = orange the\nprediction is incorrect. All errors are equally wrong (i.e. predicting orange\nor banana is equivalent).\n\u2022 Regression is easier to quantify for now: if y = 5 but \u02c6\ny = 3 we know that\nwe are wrong by \u22122. Errors have magnitude.\n\u2022 Loss function: L : Y \u00d7 Y \u2192R+\n0 is more than just the error: it measures\nthe \u201ccost\u201d or \u201cseverity\u201d of predicting \u02c6\ny = h(x) instead of y.\n\u2022 We usually assume L(\u02c6\ny, y) = 0 if \u02c6\ny = y, for all y \u2208Y .\n12\n\nGoal of Learning: Loss and Risk\n\u2022 The expected loss is called risk:\nR(h) = E\nh\nL(h(x), y)\ni\n=\nZ\nX\u00d7Y\nL(h(x), y) P(x, y) ,\nwith E denoting the expected value and P the data distribution (check\nthe math handout).\n\u2022 The goal of supervised learning can be de\ufb01ned as inferring (\ufb01nding, building)\nan hypothesis h with minimal risk\n\u2022 However, the distribution P is unknown. In practice we can only compute\nthe empirical risk:\n\u02c6\nR(h) =\nn\n\u2211\ni=1\nL(h(xi), yi) .\n13\n\nUnder\ufb01tting and over\ufb01tting\nWhen \ufb01tting a model to the data, matching model complexity to data complexity\nis very important.\nSometimes data visualization can hint at the complexity to expect.\nWe will later study methods that automate this process.\n14\n\nUnder\ufb01tting and over\ufb01tting\nUnder\ufb01t: the model does not capture\nthe complexity of the data.\nThe data looks somehow quadratic\n(parabola), hence a linear model cannot\ncapture its complexity.\nOver\ufb01t: the model captures inherent\nnoise in the data.\nThe model can become strongly biased\ntowards speci\ufb01c points, losing its ability\nto generalize to unseen data.\n15\n\nUnder\ufb01tting and over\ufb01tting\nThe correct \ufb01t for the data is typically obtained through a model of comparable\ncomplexity with the underlying function generating the data.\nHere a quadratic model gives us an intuitively natural \ufb01t.\nIndeed this is just quadratic data laced with Gaussian noise.\n16\n\nThe simplest training algorithm\nAt this point you should\n\u2022 Know how to write a model\n(you can use a line, or be inventive with a polynomial)\n\u2022 Understand the use of the parametrization\n((m, q) works for a line, but higher polynomials work the same way)\n\u2022 Know how to calculate the risk of a model\n(calculate the di\ufb00erence (loss) between prediction and target for all training\nset and sum it up)\n\u2022 Understand that the model is not expected to be a perfect \ufb01t for the data,\nbut rather a sensible approximation\nWhat you need now is a learning algorithm. This will take a while (next week),\nbut for today why don\u2019t we make the machine crunch some numbers for us?\n17\n\nThe simplest training algorithm\nCan you simply guess the parameters? Do you think it will work?\n\u2022 Write your model (class of functions): a Python callable (e.g. method,\nlambda, function) that takes a parameter set as input and returns a another\nfunction: the solution (trained model, hypothesis): m : Rp \u2192S\n\u2022 The solution itself should take a data point x \u2208X as input and return a\nprediction \u02c6\ny: s \u2208S, s : Rn \u2192R\n\u2022 Now write a Python loop that takes a training set of data points and returns\nthe corresponding predictions\n\u2022 Finally write the risk function for a given solution: compute the total error\nas the sum of the di\ufb00erences (loss) between predictions and correct targets\n\u2022 This allows you now to compare two solutions: you can select the one with\nthe least error\n\u2022 Write an outer loop where you generate a parameter set, construct the\ncorresponding solution, compare it with the (score of the) best found so far,\nand update your best if the new solution is better\n18\n\nSummary\n\u2022 To understand a machine learning technique, ask yourself\n\u2022 What is the model?\n\u2022 What is the parameters set?\n\u2022 What is the learning algorithm?\n\u2022 Supervised learning deals with the training of models (hypotheses, adaptive\nsystems) based on training data.\n\u2022 The goal of learning is to minimize the risk, empirically de\ufb01ned as cumulative\nerror. This is di\ufb03cult, because the full input/decision space is unknown: we\nsee only a \ufb01nite data sample, and the corresponding empirical risk.\n\u2022 Beware of under\ufb01tting and over\ufb01tting: your model needs to be complex\nenough to generalize, but not so to learn the data by heart.\n19\n\n", "07.kernels.pdf": "07: Kernels\nProf. Philippe Cudr\u00e9-Mauroux\nslides by Dr Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 15, 2024\n\nMapping Intuition\nClass 1\nClass 2\nFeature Map\n\u03a6 : t \u21a6\ue234\n\u008d2\n\u008d1\n\u008d1\n\u008d2\n\u008d3\nNon linearly separable\nLinearly separable\n\u2022 Working directly with the data in the original space can be challenging. A\nnon-linear classi\ufb01er separates orange from blue dots\n\u2022 The purpose of mapping data points into a higher-dimensional space is to\nmake learning easy (e.g. linear classi\ufb01er)\n\u2022 In this example a nonlinear curve (circle) x2\n1 + x2\n2 \u2212r2 = 0 is a more\ncomplex function than a plane ax1 + bx2 + cx3 + d = 0\n1\n\nMapping Example\nClass 1\nClass 2\nFeature Map\n\u03a6 : t \u21a6\ue234\n\u008d2\n\u008d1\n\u008d1\n\u008d2\n\u008d3\nNon linearly separable\nLinearly separable\n\u2022 Left: data is low-dimensional (2D) but classi\ufb01cation is complex\nNotice how the correct boundary would have to be a circle\n\u2022 Mapping: adds a new feature x3 = x2\n1 + x2\n2\nPoints are now described by (x1, x2, x2\n1 + x2\n2 )\n\u2022 Right: data is higher-dimensional (3D) but classi\ufb01cation is simpler\nNow the correct boundary is a linear hyperplane perpendicular to x3\n2\n\nFeature Space and Kernel\nClass 1\nClass 2\nFeature Map\n\u03a6 : t \u21a6\ue234\n\u008d2\n\u008d1\n\u008d1\n\u008d2\n\u008d3\nOriginal space\nFeature space\n\u2022 In kernel-based methods the data in original space, x \u2208X is mapped to a\nfeature space H using a mapping function \u03c6 : X \u2192H\n\u2022 The feature space H is a vector space with inner product property, so called\nReproducing Kernel Hilbert Space (RKHS)\n3\n\nRepresenter Property\n\u2022 A kernel function associates each data point with a function. For every\nx \u2208X the mapped data point is \u03c6(x) = k(\u00b7, x) \u2208H. You read it as kernel\nfunction k(\u00b7, x) with the \ufb01xed x and an arbitrary variable denoted by \u00b7.\n\u2022 Suppose we have a kernel k on bird images. If x is the image of a bird\nwe get a representation of that bird as a real valued function de\ufb01ned on\nthe space of birds as\n\u2022 Therefore \u2200x, y \u2208X we have:\n\u03c6(x) = k(\u00b7, x) \u2208H, \u03c6(y) = k(\u00b7, y) \u2208H\n4\n\nKernel Trick\n\u2022 The kernel function is equal to the inner product of mapped data\nk(x, y) = \u27e8\u03c6(x), \u03c6(y)\u27e9\n\u2200x, y \u2208X, k(x, y)\n=\n\u27e8\u03c6(x), \u03c6(y)\u27e9\n=\n\u27e8k(\u00b7, x), k(\u00b7, y)\u27e9\n\u2022 This dot product de\ufb01nition of a kernel provides a straightforward and\ne\ufb03cient computation with no need for explicitly de\ufb01ning \u03c6(\u00b7):\nThis is called the kernel trick\n\u2022 This is very useful in real-world problems where \u03c6 is a complex and time\nconsuming function to de\ufb01ne \u2013 sometimes you may not even be able to\nde\ufb01ne this function explicitly\n5\n\nKernels\nThe necessary and su\ufb03cient condition for de\ufb01ning a valid kernel is as follows:\nDe\ufb01nition: A (Mercer) Kernel on a space X is a continuous function of two\narguments\nk : X \u00d7 X \u2192R\nwith the following properties:\n\u2022 Symmetry: it holds k(x1, x2) = k(x2, x1) for all x1, x2 \u2208X,\n\u2022 Positive de\ufb01niteness: k(x1, x2) \u22650 for all x1, x2 \u2208X. In other words for\neach \ufb01nite subset {x1, . . . , xn} we can de\ufb01ne the Gram (kernel) matrix\nK \u2208Rn\u00d7n with entries Kij = k(xi, xj) is positive semi-de\ufb01nite.\n6\n\nExample\nClass 1\nClass 2\nFeature Map\n\u03a6 : t \u21a6\ue234\n\u008d2\n\u008d1\n\u008d1\n\u008d2\n\u008d3\n\u03c6 : R2 \u2192R3 ,\n(x1, x2) 7\u2192(x2\n1,\n\u221a\n2x1x2, x2\n2 )\nk(x, y) =< \u03c6(x), \u03c6(y) >\n= (x2\n1,\n\u221a\n2x1x2, x2\n2 )(y2\n1,\n\u221a\n2y1y2, y2\n2 )T\n= ((x1, x2)(y1, y2)T)2\n=\n\u27e8x, y\u27e92\nComputing \u03c6 has quadratic time complexity O(D2)\nwith D number of dimensions. Using the kernel function instead means\ncomputing \u27e8x, y\u27e9, lowering the time complexity to linear O(D).\n7\n\nProducing Kernel Functions\n\u2022 Every constant function taking a non-negative value is a (very boring) kernel\n\u2022 The standard inner product is a kernel: the linear kernel\nk(x1, x2) = xT\n1 x2\n\u2022 Kernels can be constructed by composing other kernels, e.g. k1 and k2:\n\u2022 For \u03bb \u22650, \u03bb \u00b7 k1 is a kernel\n\u2022 k1 + k2 is a kernel\n\u2022 k1 \u00b7 k2 is a kernel (thus also kn\n1)\n\u2022 This allows us to immediately derive the polynomial kernel:\nk(x1, x2) = (xT\n1 x2 + b)d, for b \u22650 and d \u2208N\n\u2022 The \u201cradial\u201d Gaussian kernel (also RBF: radial basis function) is de\ufb01ned as\nk(x1, x2) = exp(\u2212\u03b3 \u00b7 \u2225x1 \u2212x2\u22252) for \u03b3 > 0\n\u2022 A straightforward extension is k(x1, x2) = exp\n\u0000 \u2212(x1 \u2212x2)TC(x1 \u2212x2)\n\u0001\nfor any symmetric, positive de\ufb01nite matrix C\n8\n\nKernel Selection\n\u2022 The choice of the kernel is left to the user as a hyperparameter\n\u2022 This allows incorporating prior knowledge into the learning process: the \nkernel can simplify the learning by highlighting discriminative (or otherwise \nimportant) features\n\u2022 Hyperparameters however are a double edged sword: we pay the \ufb02exibility by \nhaving now to \u201clearn\u201d something ourselves, while it would be more \nconvenient if the machine had a mechanism for identifying a good kernel by \nitself\n\u2022 In SVM applications, the choice of a good kernel and its parameters (like the \ndegree d of the polynomial kernel or the bandwidth \u03b3 of the Gaussian \nkernel), as well as e.g. the constant C, is called the model selection \nproblem\n9\n\nIn practice: the Gram (kernel) matrix\n\u2022 In practice, for a given dataset with data points {x1, . . . , xn}, we de\ufb01ne the\nGram matrix K \u2208Rn\u00d7n over the data: this new matrix will be used as the\ninput of kernel-based algorithms, in place of the original data\n\u2022 Entries in the Gram matrix Kij = k(xi, xj) are the value of the kernel\nfunction for the pair of data points (xi, xj)\n\u2022 Kernel functions are also known as nonlinear similarities measures and\ncovariance functions\n[Cuturi M., 2009 ]\n10\n\nFeature Approach\n\u2022 Imagine you have read 100 books, you would like to know whether a new\nbook is interesting for you to read\n\u2022 There are two distinct approaches in Machine Learning to get the answer:\n1) Feature-based, or 2) Kernel matrix\n\u2022 In the Feature approach:\n1 Extract some features that represent each book. For instance, popularity,\nnumber of pages, language, ... as features\n2 Collect the ratings of how much you liked each book\n3 Find a decision/ prediction function based these features and ratings\n[Cuturi M. ]\n11\n\nKernel Matrix Approach\nThe Kernel Matrix approach instead \ufb01rst de\ufb01nes how similar are two books\ncomparing to each other:\n1 De\ufb01ne a kernel function k that quanti\ufb01es the similarity of books\n2 Map the books you have read into a Gram matrix:\n3 Find a decision function that takes this 100 \u00d7 100 matrix as input, and\nbased on book similarities \ufb01nds a simple relationship (typically linear) to \ufb01nd\nwhich would be closest to our preference\n[Cuturi M. ]\n12\n\nFeature Approach vs Kernel Matrix Approach\nFor a new book:\n\u2022 The feature approach answers the question \u201cwhat are the describing features\nof this new book?\u201d, or \u201dwhich features are good indicators of my interest in\na book?\u201d\n\u2022 The kernel approach answers the question \u201cwhich books that I have read are\nsimilar to this new book?\u201d, or \u201cwhat set of books de\ufb01nes my\ninterest/preference in books?\u201d\n\u2022 Kernel methods only use the similarities de\ufb01ned by kernels, and not the\nfeatures. Features can help de\ufb01ne these similarities but are not used directly.\n13\n\nAdvantages of Kernel Methods\n\u2022 Kernel methods can handle data\nwith complex structures: the\nentries of Gram (kernel) matrix are\nnot data points, thus the original\ndata can be of any complex form\nsuch as, strings, graphs, trees, and\nhistograms\n\u2022 The conversion e\ufb00ort into\nmeaningful numbers is left to the\nGram matrix, and these numbers\nrepresent similarity between pairs of\npoints, regardless of the data types\nof their features\n[Cuturi M., 2009 ]\n\u2022 They are applicable to high\ndimensional data, as the kernel\nmatrix is dependent on sample size\nn and not directly on the\ndimension of data d\n14\n\nAdvantages of Kernel Methods\nAny algorithm that has a dot product representation can bene\ufb01t from the\napplication of the kernel trick, a process called kernelization, which produces a\nkernel variant of the base algorithm. This allows applying the linear methods we\nhave seen so far to data with nonlinear relationships\nRemember our questions?\nQ: How to extend the Perceptron to non-linear separation?\nQ: How to extend the Perceptron to non-vectorial data?\nA: With kernelization!\nKernels are a very \ufb02exible way to introduce nonlinear feature maps; they can be\nde\ufb01ned on virtually any space, and therefore do not require vector-valued input\ndata\n15\n\nSVMs with Kernels\nTo de\ufb01ne kernel-based SVMs, we replace data and inner product operations\nrespectively with corresponding mapped data and kernel functions.\nUsing the representer theorem we assume w \u2208H and \u03c6(xi)are :\nw =\nn\n\u2211\nj=1\n\u03b2j \u00b7 k(xj, \u00b7),\n\u03c6(xi) = k(xi, \u00b7) \u21d2\u27e8w, \u03c6(xi)\u27e9=\nn\n\u2211\nj=1\n\u03b2jk(xi, xj)\nDe\ufb01nition. The soft-margin support vector machine is de\ufb01ned as the solution\nof the convex quadratic program:\nargmin\n\u03b2\u2208Rn,b\u2208R,\u03be\u2208Rn\n1\n2\nn\n\u2211\ni,j=1\n\u03b2i\u03b2jk(xi, xj) + C \u00b7\nn\n\u2211\ni=1\n\u03bei\ns.t.\n\u2200i \u2208{1, . . . , n} : yi \u00b7\n \nn\n\u2211\nj=1\n\u03b2jk(xi, xj) + b\n!\n\u22651 \u2212\u03bei\nand\n\u2200i \u2208{1, . . . , n} : \u03bei \u22650 .\n16\n\nSVMs with Kernels\n\u2022 We de\ufb01ne the Kernel Matrix K \u2208Rn\u00d7n of the training data with entries\nKi,j = k(xi, xj).\n\u2022 Let Ki denote the i-th column of this matrix (K T\ni\nthe i-th row, due to\nsymmetry).\n\u2022 The SVM problem in matrix notation is as\nargmin\n\u03b2\u2208Rn,b\u2208R,\u03be\u2208Rn\n1\n2\u03b2TK \u03b2 + C \u00b7\nn\n\u2211\ni=1\n\u03bei\ns.t.\nfor all i \u2208{1, . . . , n} : yi \u00b7\n\u0010\nK T\ni \u03b2 + b\n\u0011\n\u22651 \u2212\u03bei\nand\nfor all i \u2208{1, . . . , n} : \u03bei \u22650 .\n17\n\nExamples of SVMs with Kernels\nHere are a few examples of SVM decision function with Gaussian kernel\nk(x1, x2) = exp(\u2212\u03b3 \u00b7 \u2225x1 \u2212x2\u22252) for di\ufb00erent values of \u03b3. The value of C is\nequal to 1000 in all three cases.\n\u03b3 = 0.001, 113 SVs\n\u03b3 = 0.008, 64 SVs\n\u03b3 = 0.02, 33 SVs\n[Billard A. ]\n18\n\nExamples of SVMs with Kernels\nHere are a few examples of SVM decision function with Gaussian kernel for\ndi\ufb00erent values of C.\nSmall C\nIntermediate C\nLarge C\n19\n\nSVMs with Kernels\n\u2022 Do we still need soft-margin SVMs for kernel applications?\n\u2022 The answer is yes: the roles of the nonlinear feature map and\nthe soft-margin constraints are very di\ufb00erent\n\u2022 The purpose of the kernel (and its feature map) is to make\nlearning \u201ceasy\u201d\n\u2022 Even in an in\ufb01nite dimensional feature space, we should still\naccount for margin violators, because we should not trust noisy\ndata\n20\n\nSummary\n\u2022 We have learned about kernel functions and kernel matrices\n\u2022 We have introduced support vector machines as an extension to\nthe Perceptron, by maximizing the margin\n\u2022 We have \u201ckernelized\u201d the SVM, which turns it into a non-linear\nclassi\ufb01er\n\u2022 The kernel SVM also allows for dealing with non-vectorial data\n21\n\nExtra material\n\u2022 Full derivation for SVM:\nhttp://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf\n\u2022\n[Cuturi M., 2009 ]: Positive De\ufb01nite Kernels in Machine Learning\nhttps://arxiv.org/abs/0911.5367\n22\n\n", "09.neural_networks_basic.pdf": "09: Neural Networks (basic)\nDr Giuseppe Cuccu\nMachine Learning\nBachelor in Computer Science [SP23]\nApril 24, 2023\nPresented by Dr Anna Scius-Bertrand\nSlides from Dr Giuseppe Cuccu\nArpil 29, 2024\n\nWhere are we\n\u2022 Our very \ufb01rst model + learning algorithm was the Perceptron\n\u2022 Main limitations:\n(i) The model is limited to linear separation\n(ii) The learning algorithm requires linearly separable data\n\u2022 Neural Networks overcome such limitations by extending the\nPerceptron with nonlinear activation functions and the\nBackpropagation algorithm\n\u2022 As their name suggests, they were originally inspired by\nbiological neural networks, such as the human nervous system\n1\n\nToday\u2019s menu\n\u2022 Original Inspiration\n\u2022 Multilayer Networks\n\u2022 Universal Approximation\n\u2022 Stochastic Gradient Descent\n\u2022 Backpropagation\n2\n\nNeuroscience roots\n\u2022 There are many di\ufb00erent types of\nneurons in the human nervous\nsystem.\n\u2022 Lots of specializations, and each\ntype itself quite complex.\n\u2022 Neurons connect to each other with\nsynapses, thus forming a network.\nThis complicates things even more.\n\u2022 How to understand/model this\nsystem?\n\u2022 Basic idea: simplify the neuron\nuntil reduced to its essentials.\n(source for all neuron images: Wikipedia)\n3\n\n4\n\n5\n\n6\n\nSimplifying Real Neurons\n\u2022 A neuron receives inputs from a\nnumber of other neurons through\nits dendrites. The contact points\nare called synapses. Its outputs are\ndistributed by its axon, itself having\nmultiple synapses.\n\u2022 The inputs typically come from the\noutputs of other neurons, in the\nform of spikes: short pulses of\nelectrical current\n\u2022 The neuron averages them over\ntime, which can be represented by\nits input spike frequency\n\u2022 The spikes arrive at the neuron\u2019s\nmembrane and alter its membrane\n(electric) potential\n\u2022 There are both excitatory and\ninhibitory connections between\nneurons, each with di\ufb00erent\nstrength, i.e. synaptic weight\n7\n\nModeling Arti\ufb01cial Neurons\nNeuron\nPotential\nAxon\nInput\nSpike\nDendrite\n\fp\no\nm\n\fq\n\u2022 Input of a neuron: outputs of other neurons, i.e. array of their spike\nfrequencies \u03bd.\n\u2022 Strength of connection: array of synaptic weights w. Weights are positive\nfor excitatory connections, and negative for inhibitory ones; absolute value is\nlarge for strong connections, small for weak ones.\n\u2022 Neuron potential: sum of weighted inputs, i.e. its membrane potential u.\n8\n\nArti\ufb01cial Neurons\nWhen the neuron\u2019s membrane potential exceeds a threshold then the neuron\nemits a spike (which can propagate to multiple receivers) and resets its\nmembrane potential. The spike frequency as a function of the incoming power is\na non-linear transfer function (also called activation or simply non-linearity).\nClassical literature utilizes the logistic function:\n\u03bd = \u03c3(u) =\n1\n1+e\u2212u\n \u22124.0 \n \u22123.0 \n \u22122.0 \n \u22121.0 \n 0.0 \n 1.0 \n 2.0 \n 3.0 \n 4.0 \n0.0\n1.0\n9\n\nThe Logistic Function\n\u03c3(u) =\n1\n1+e\u2212u\n \u22124.0 \n \u22123.0 \n \u22122.0 \n \u22121.0 \n 0.0 \n 1.0 \n 2.0 \n 3.0 \n 4.0 \n0.0\n1.0\n\u2022 Nonlinear, but close to linear around/near x = 0\n\u2022 Output is bounded between 0 and 1 (open)\n\u2022 In\ufb02ection point is at x = 0 with a value of y = 0.5\n\u2022 Very high and very low inputs are squashed to the boundaries (saturation)\n\u2022 Its derivative is easy by design (more on why later): \u2202\u03c3(u)\n\u2202u\n= \u03c3(u)(1 \u2212\u03c3(u))\n\u2022 Functions with shape resembling an S, such as the Logistic, are often called\nsigmoids or sigmoidal functions (also check out e.g. arctan)\n10\n\nNetworks of Neurons\n\u2022 Neurons in nervous systems are connected in large, complex networks. Our\narti\ufb01cial neurons can approximate this concept using directional graphs of\ncomputation, with neurons in the nodes: (round: neuron; square: input/output)\n\u2022 There are two ways to stack neurons:\n\u2022 In parallel: neurons that are supposed to \ufb01re independently and can be\nmodeled together (i.e. in a same layer)\n\u2022 In series: neurons which are connected sequentially axion-dendrite (i.e. in\nseparate layers), meaning the output of the \ufb01rst neuron is an input for the\nsecond neuron: the model runs in sequence\n11\n\nArti\ufb01cial Neurons\n\u2022 While inspiration from biology, neural networks are so abstract and simplistic\nthat in the end they retain little in common with their biological counterpart.\nThey are better understood as a mathematical sequential learning machine.\n\u2022 Let ui be the membrane potential and \u03bdi be the \ufb01ring rate of neuron i, and\nlet wji be the synaptic weight of the connection from i to j (which is zero if\nthe neurons are not connected). We obtain the model:\nuj \u2190\nk\n\u2211\ni=1\nwji \u00b7 \u03bdi ,\n\u03bdj \u2190\u03c3(uj)\n\u2022 The relation uj \u2190\u2211k\ni=1 wji \u00b7 \u03bdi is familiar: if \u03bdi are the inputs, then this is a\nlinear function, which links neurons to Perceptron models.\n12\n\nMultilayer Neural Networks\n\u2022 The resulting architecture is called a layered feed-forward neural network, or\nmore commonly a Multi-Layer Perceptron (MLP)\nThis example network has\nan input layer with 5 nodes,\ntwo hidden layers with 8 and 6\nneurons, and\nan output layer with 2 neurons\n(the number of connections grows fast)\n\u2022 The sizes of the input and output layers correspond to the dimensions of the\nvectors x and y, which are determined by the problem\n\u2022 Number and size of the hidden layers is arbitrary, changing the complexity of\nthe model\n13\n\nNeural Networks\nNeural networks are easily depicted as connected graphs\n14\n\nNeural Networks\ninputs\nhidden\nneurons\noutput\nneurons\nnetwork\noutput\nThe input layer is not composed of neurons, but houses the inputs to the model.\nThe output of the model corresponds to the outputs of\nthe neurons in the network\u2019s output layers.\n15\n\nNeural Networks\nw1\nw2\nn3\nw3\nw4\nw5\nw6\nact \nx1\nx2\nn1\nn2\n\u2022 Each layer of neurons receives inputs through weighted connections.\n\u2022 The output dimensionality corresponds to the number of output neurons.\n\u2022 Each neurons weights it inputs, aggregates them, then activates.\n\u2022 The graph is just a human-readable representation of a complex equation.\n16\n\nNeural Networks\nw1\nw2\nn3\nw3\nw4\nw5\nw6\nact \nx1\nx2\nn1\nn2\nWin =\n\u0014w1\nw2\nw3\nw4\n\u0015\n, Whid =\n\u0002\nw5, w6\n\u0003\nx =\n\u0012x1\nx2\n\u0013\n, nhid =\n\u0012n1\nn2\n\u0013\n, nout =\n\u0000n3\n\u0001\n\u2022 One weight matrix connects two\nlayers\n\u2022 Each row holds the weights\nentering a neuron\n\u2022 Each column shares the same\nconnection origin\n17\n\nNeural Networks\nw1\nw2\nn3\nw3\nw4\nw5\nw6\nact \nx1\nx2\nn1\nn2\nWin =\n\u0014w1\nw2\nw3\nw4\n\u0015\n, Whid =\n\u0002\nw5, w6\n\u0003\nx =\n\u0012x1\nx2\n\u0013\n, nhid =\n\u0012n1\nn2\n\u0013\n, nout =\n\u0000n3\n\u0001\nnhid = \u03c3(Winx) =\n\u0012\u03c3(w1x1 + w2x2)\n\u03c3(w3x1 + w4x2)\n\u0013\nnout = \u03c3(Whidnhid) =\n\u0000\u03c3(w5n1 + w6n2)\n\u0001\nact = n3 = \u03c3 [w5\u03c3(w1x1 + w2x2) + w6\u03c3(w3x1 + w4x2)]\n18\n\nFunction Approximation\nw1\nw2\nn3\nw3\nw4\nw5\nw6\nact \nx1\nx2\nn1\nn2\nact = \u03c3 [w5\u03c3(w1x1 + w2x2) + w6\u03c3(w3x1 + w4x2)]\n\u2022 Parametrization: structure, activations and weights\nThe \ufb01rst two are most often \ufb01xed hyperparameters\n\u2022 Remember that function composition increases complexity fast\n\u2022 Network complexity gives an upper bound on the complexity of the\nfunctions that can be approximated (think: what if all w = 0?)\n19\n\nMultilayer Neural Networks\nact = \u03c3 [w5\u03c3(w1x1 + w2x2) + w6\u03c3(w3x1 + w4x2)]\n\u2022 Hidden layers traditionally employ a sigmoidal transfer (activation) function;\nThe choice for the output layer instead is task speci\ufb01c, depending on how\nthe model output is interpreted:\n\u2022 Regression: sigmoidals are bound; to achieve an unbounded range of\noutputs the identity function is a common choice for the transfer of the\noutput neurons, i.e. the output layer consists of linear neurons.\n\u2022 Classi\ufb01cation: the values range does not matter, so either linear or sigmoid\nactivations can be used. Multiclass problems however are typically addressed\nthrough multiple output decision neurons, i.e. one for each class.\n\u2022 Speci\ufb01c applications can have variations over the two types above: for\nexample control problems with a discrete action set are handled similarly to\nClassi\ufb01cation, while continuous actions are closer to Regression (more on\nthat later)\n20\n\nMultilayer Neural Networks\n\u2022 The linear function u = W \u00b7 \u03bdx is usually extended to an a\ufb03ne function\nu = W \u00b7 \u03bdx + b by means of a so-called bias input. This is a constant\n\ufb01ring rate (value) of one, connected to each and all neurons with weights bi.\n\u2022 A multilayer neural network thus applies two types of transformations in\neach layer in sequence:\n\u2022 A linear combination: left multiplication with the matrix W (i).\nThis matrix is a parameter of the model, subject to learning.\n\u2022 A non-linear function: component-wise transfer function \u03c3.\nThis function is traditionally \ufb01xed, with no parameters.\n\u2022 The non-linearities are very important! Without them the model can be\nsimpli\ufb01ed into the linear map W = W (1) \u00b7 ... \u00b7 W (\u2113) with \u2113number of layers.\n(can you prove it?)\nOne-layer neural networks are linear models, like a Perceptron.\n\u2022 A neural network with sigmoid transfer and (at least) one hidden layer\n(of arbitrary size) is instead proven to be able to compute \u201ceverything\u201d,\ni.e. it is a generic function approximator.\n21\n\nUniversal Approximation Property\nTheorem. Let \u03c3 : R \u2192R be a continuous, non-constant, bounded, and\nmonotonically increasing function. Let K \u2282Rp, and C(K) denotes the space of\ncontinuous functions K \u2192R. Then, given a function g \u2208C(K) and an accuracy\n\u03b5 > 0, there exists a hidden layer size q \u2208N and a set of coe\ufb03cients w(1)\ni\n\u2208Rp,\nw(2)\ni\n, bi \u2208R (for i \u2208{1, . . . , q}), such that\nf : K \u2192R ;\nf (x) =\nq\n\u2211\ni=1\nw(2)\ni\n\u00b7 \u03c3\n\u0010\n(w(1)\ni\n)Tx + bi\n\u0011\nis an \u03b5-approximation of g, that is,\n\u2225f \u2212g\u2225\u221e:= max\nx\u2208K |f (x) \u2212g(x)| < \u03b5 .\nCorollary. The theorem extends trivially to multiple outputs.\nCorollary. Neural networks with a single sigmoidal hidden layer and linear output\nlayer are universal function approximators.\n22\n\nUniversal Approximation Property\n\u2022 This means that, in the space of all functions, for any given target function\ng, there exists a sequence of networks\n\b\nfk\n\t\nk\u2208N that converges (pointwise)\nto the target function to arbitrary precision in k learning steps\n\u2022 The trade-o\ufb00of course is that this approximation will require more (hidden)\nneurons (i) as the complexity of g grows, and (ii) for increasingly smaller \u03f5\n\u2022 This also implies that the space of networks with \ufb01xed structure and\nactivation function map weight vectors to a de\ufb01nite, bounded subspace of\nthe space of all functions\n\u2022 Networks with higher complexity (e.g. large hidden layer sizes) will map to a\nlarger space, as they include more complex functions (remember, network\ncomplexity is only upper bound)\n\u2022 Note: the universal approximation property is not as special as it seems.\nFor example, polynomials are universal approximators (Weierstrass theorem)\n23\n\nFrom Models to Learners\n\u2022 The class of functions represented by neural networks can approximate the\nsolution to any problem to arbitrary precision \u2013 provided that the network is\n\u201cbig enough\u201d (to approximate the underlying function\u2019s complexity)\n\u2022 But how can we select in practice the network structure (ergo size),\nactivation function and weights? We still need a method to learn the\nparametrization\n\u2022 Neural Networks by themselves are just a class of models, a family of\nparametrized generic function approximators\n\u2022 As traditional, for now we will consider structure and activation as \ufb01xed (i.e.\nuser-de\ufb01ned hyperparameters), and derive a gradient-based training method\nas a two-step procedure (akin to the Perceptron):\n1 Compute the derivative of the empirical risk w.r.t. the weights\n2 Change the weights so that the empirical error is reduced\nThis process can be iterated, until a local optimum is reached\n24\n\n(Online) Steepest Descent Training\nIt is typically easier to understand if we begin with the second step (reduce\nempirical error). Hypothesize we have a way to compute the derivative of the\nempirical risk w.r.t. the weights \u2207wE(w) (the error gradient).\n\u2022 Let w denote a vector collecting all weights of a neural network: think of it\nas a \u201clinearized\u201d version of all of its weight matrices, concatenated then\n\ufb02attened\n\u2022 Let fw be the function corresponding to the network with weights w\n\u2022 Let the error of the network be computed as average risk in function\nof the network weights (S is a subset of points from the training set):\nE(w) = 1\n|S| \u2211\ni\u2208S\nL(fw(xi), yi)\nChoosing di\ufb00erent S yields di\ufb00erent training techniques\n25\n\n(Online) Steepest Descent Training\nE(w) = 1\n|S| \u2211\ni\u2208S\nL(fw(xi), yi)\nThere are traditionally three ways to estimate the error for a neural network, each\nwith its pros and cons:\n\u2022 Batch mode: run the sum over the whole dataset, |S| = n. This produces\na single, dataset-wide error for each update step. If the dataset is very large\n(or uniform) this may severely limit performance.\n\u2022 Online mode: compute a di\ufb00erent error for each element in the dataset,\n|S| = 1. These fast updates sometimes perform better than slower, more\ncareful updates, especially on simpler problems.\n\u2022 Mini batches: a trade-o\ufb00between the two above; randomly select small\nsubsets at each step, |S| \u226an. This allows for a certain degree of control on\nperformance depending on batch size, at the cost of introducing a new\nhyperparameter (most common nowadays).\n26\n\n(Online) Steepest Descent Training\n\u2022 The derivative of the error function \u2207wE(w) gives us a gradient of\n(increasing) error. The negative gradient thus points in the direction where\nit decreases fastest.\n\u2022 We can then derive the learning rule:\nw \u2190w \u2212\u03b7 \u00b7 \u2207wE(w)\nwith \u03b7 > 0 learning rate\n\u2022 This method is known as stochastic gradient descent: a generic,\niterative, simple optimization scheme based on error gradients, which is\napplied in many forms throughout ML\n\u2022 The learning rate \u03b7 can be a \u201csmall\u201d constant or decay over time (i.e.\ntraining iterations)\n\u2022 A decaying learning rate of the form \u03b7(t) = \u03b70/t eventually reaches a value\nbelow machine precision (becoming \u201czero\u201d), making the algorithm\nconverge/terminate on a local optimum of the error function\n27\n\nBackpropagation\nLet\u2019s now go back to the \ufb01rst step: computing the error gradient \u2207wE(w)\n\u2022 The error is a simple sum over loss terms, each of the form\nE(w) = L(fw(x), y) for point (x, y)\n\u2022 We can expand this error as\nE(w) = L\n\"\n\u03c3\n \nW (\u2113) \u00b7 \u03c3\n\u0010\nW (\u2113\u22121) \u00b7 \u03c3\n\u0000. . . W (1) \u00b7 x . . .\n\u0001\u0011!\n, y\n#\nwhere each W (k) is the weight matrix for layer k, and \u03c3 is the\ncomponent-wise non-linearity\n\u2022 The gradient (i.e. derivative) of this error function can be calculated using\nthe chain rule, yielding the Backpropagation algorithm (often shortened\nto backprop)\n\u2022 Most errors in understanding and implementing the rule come from not\npaying enough attention to the index of the layer: please be extra\ncareful here\n28\n\nBackpropagation: output layer\n\u2022 For the last (output) layer, k = \u2113, the gradient is de\ufb01ned as \u03b4(k) =\n\u2202E\n\u2202u(k)\n\u2022 This means that we have E = L(\u03bd(\u2113), y) and \u03bd(\u2113) = \u03c3(u(\u2113))\n\u2022 Assuming the squared loss L(\u03bd(\u2113), y) = 1\n2\u2225\u03bd(\u2113) \u2212y\u22252 we obtain\n\u03b4(\u2113)\n=\n\u2202L(\u03c3(u(\u2113)), y)\n\u2202u(\u2113)\n=\n\u2202L(\u03bd(\u2113), y)\n\u2202\u03bd(\u2113)\n\u00b7 \u2202\u03bd(\u2113)\n\u2202u(\u2113)\n=\n\u0010\n\u03bd(\u2113) \u2212y\n\u0011\n\u00b7\n\u0010\n1 \u2212\u03c3(u(\u2113))\n\u0011\n\u00b7 \u03c3(u(\u2113))\n\u2022 To compute this we need the target/label y (SL!), the network output \u03bd(\u2113),\nand the membrane potential u(\u2113) (the derivative of the logistic here has u\nonly appearing inside a \u03c3: this is not generally true)\n\u2022 This computation can be generalized to any arbitrary (di\ufb00erentiable) loss L\nand transfer function \u03c3\n29\n\nBackpropagation: output layer\n\u2022 Now we are interested in the derivative\n\u2202E\n\u2202W (\u2113)\ni,j\nof the error w.r.t. a weight in\nthe last weight matrix, with u(\u2113) = W (\u2113) \u00b7 \u03bd(\u2113\u22121)\nx\n\u2022 The derivative is:\n\u2202E\n\u2202W (\u2113)\ni,j\n=\n\u2202E\n\u2202u(\u2113)\ni\n\u00b7 \u2202u(\u2113)\ni\n\u2202W (\u2113)\ni,j\n=\n\u03b4(\u2113)\ni\n\u00b7 \u03bd(\u2113\u22121)\nj\n\u2022 This introduces an additional term: the output of the\nsecond-to-last layer \u03bd(\u2113\u22121)\n\u2022 Before moving on (backprop on hidden layers), make sure that you\nunderstand what \u03bd(\u2113\u22121) is, and why did it appear in the equation\n30\n\nBackpropagation: hidden layers\n\u2022 Let us now consider earlier layers k < \u2113: consider the derivative\n\u2202E\n\u2202W (k)\ni,j\n\u2022 The same decomposition as before works:\n\u2202E\n\u2202W (k)\ni,j\n=\n\u2202E\n\u2202u(k)\ni\n\u00b7 \u2202u(k)\ni\n\u2202W (k)\ni,j\n=\n\u03b4(k)\ni\n\u00b7 \u03bd(k\u22121)\nj\n\u2022 But this time, we are not \ufb01nished yet!\n\u2022 This requires the layer inputs \u03bd(k\u22121), i.e. the output of the previous layer\n\u2022 We also need the residual error \u03b4(k), which is back-propagated from the\noutputs\n\u2022 At each step, one layer is adjusted to reduce a bit the error, then the rest of\nthe error is sent down to the preceeding layers as \u201ctheir responsibility\u201d to\ndeal with\n31\n\nBackpropagation: hidden layers\n\u2022 Here is the last part of the derivation for the hidden layers:\n\u03b4(k) =\n\u2202E\n\u2202u(k)\n=\n\u2202E\n\u2202uk+1 \u00b7 \u2202u(k+1)\n\u2202\u03bd(k)\n\u00b7 \u2202\u03bd(k)\n\u2202u(k)\n=\n\u03b4(k+1) \u00b7 W (k+1) \u00b7 \u03c3\u2032(u(k))\n\u2022 The errors \u03b4(k) can be computed based on the errors \u03b4(k+1): the error is\ncomputed end to front, from the last layers (where we can compute it\nagainst the label) backwards until the \ufb01rst, hence \u201cbackpropagation\u201d\n\u2022 The \u03b4(k)-terms are only intermediate results for the computation of the\nweight derivatives\n\u2022 In all steps we need the activations from multiple layers of the network\n\u2022 When implementing a neural network, you need to maintan a state holding\nthe current sequence of activations, to be used (in reverse order, end to\nstart) to compute your error gradients\n32\n\nBackpropagation: outline\nThis hints at the following order of computation:\ny0\ny1\ny2\ny3\nu1\nu2\nu3\nsigma\nsigma\nsigma\nW1\nW2\nW3\nL\nE\ndelta1\ndelta2\ndelta3\ndEdW1\ndEdW2\ndEdW3\n1 Propagate the input front-to-back: the forward pass.\n2 Compute the error at the network output: we need to change the weights,\none layer at a time, in the direction that decreases this output error.\n3 Propagate the error back-to-front: the backward pass.\n33\n\nSummary\n\u2022 The sigmoid transfer function extends the Perceptron into a neuron model\n\u2022 Multiple neurons can be stacked in parallel (layers), and multiple layers in\nsequence, to de\ufb01ne feed-forward neural networks (multi-layer Perceptrons)\n\u2022 Multilayer nonlinear neural networks are proven universal function\napproximators \u2014 if you have in\ufb01nite neurons\n\u2022 Networks can be trained by online or batch gradient descent\n\u2022 The error gradient can be computed and propagated e\ufb03ciently with the\nbackpropagation algorithm\n\u2022 This algorithm incorporates no exploration capabilities, thus converging into\nthe nearest local optimum\n34\n\nExtra material\n\u2022 Stochastic Gradient Descent:\nhttps://en.wikipedia.org/wiki/Stochastic_gradient_descent\n\u2022 Clear derivation of Backpropagation:\nhttps://codesachin.wordpress.com/2015/12/06/backpropagation-\nfor-dummies/\n\u2022 Stanford\u2019s tutorial:\nhttp://ufldl.stanford.edu/tutorial/supervised/\nMultiLayerNeuralNetworks/\n\u2022 Easy to read, step-by-step backpropagation:\nhttps://eli.thegreenplace.net/2018/backpropagation-through-\na-fully-connected-layer/\n\u2022 Tutorial on function approximation and why neural networks:\nhttps://machinelearningmastery.com/neural-networks-are-\nfunction-approximators/\n35\n\n", "01.math_perceptron.pdf": "01 Math recap (Perceptron)\nMachine Learning\nBachelor in Computer Science [SP24]\nFebruary 26, 2024\n\n(Real) Vector Space\n\u2022 Any space V with operands for addition and scalar multiplication (ful\ufb01lling\nthe vector space axioms) is a vector space. We will not need further formal\ntreatment.\n\u2022 Any \ufb01nite-dimensional real vector space of dimension n \u2208N can be\nidenti\ufb01ed with tuples (x1, . . . , xn)T \u2208Rn. We will treat all\n\ufb01nite-dimensional vector spaces in this form.\n\u2022 The space of all continuous functions f : [0, 1] \u2192R is an example of an\nin\ufb01nite-dimensional vector space.\n1\n\nInner Product\n\u2022 Notation: we write an inner product as \u27e8x, y\u27e9.\n\u2022 An inner product is a positive de\ufb01nite bilinear form. Not a helpful de\ufb01nition?\nOkay, let\u2019s see what it does:\n\u2022 An inner product induces a norm by \u2225x\u2225=\np\n\u27e8x, x\u27e9. Thus, it de\ufb01nes the\nmeaning of length and distance on top of a vector space.\n\u2022 It also induces the notion of an angle by the formula\n\u27e8x, y\u27e9= \u2225x\u2225\u00b7 \u2225y\u2225\u00b7 cos(\u03b1), where \u03b1 denotes the angle between the vectors\nx and y.\n\u2022 Important special cases of angle relations are:\n\u2022 \u27e8x, y\u27e9= \u00b1\u2225x\u2225\u00b7 \u2225y\u2225indicates that x and y are parallel.\n\u2022 \u27e8x, y\u27e9= 0 indicates that x and y are orthogonal.\n2\n\nInner Product\n\u2022 \u201cStandard\u201d inner product (dot product) on tuples x = (x1, . . . , xn)T and\ny = (y1, . . . , yn)T:\n\u27e8x, y\u27e9= xTy =\nn\n\u2211\ni=1\nxiyi .\n\u2022 The canonical example of an inner product on functions f , g : [0, 1] \u2192R is\n\u27e8f , g\u27e9=\nZ 1\n0 f (x)g(x) dx .\n3\n\nLinear Function\nA linear function is a function f : V \u2192R with the properties\nf (x + y) = f (x) + f (y)\nf (\u03bb \u00b7 x) = \u03bb \u00b7 f (x) .\nFor a linear function f : V \u2192R in a \ufb01nite-dimensional vector space there exists\na unique vector v \u2208V such that f (x) = \u27e8v, x\u27e9.\nUsing the standard inner product on n-tuples, every linear function can be written\nas f (x) = vTx.\n4\n\nDrawing Functions\n\u2022 A function f : R \u2192R is best visualized by plotting its graph.\n\u2022 A function f : R2 \u2192R can be visualized by drawing level sets\n{(x, y) \u2208R2 | f (x, y) = c} for a range of values c \u2208R.\n\u2022 Functions f : Rn \u2192R for n > 2 are hard to visualize in general.\n \u22124.0  \u22123.0  \u22122.0  \u22121.0  0.0  1.0  2.0  3.0  4.0 \n\u22124.0\n\u22123.0\n\u22122.0\n\u22121.0\n0.0\n1.0\n2.0\n3.0\n4.0\nx\nf(x)\nf(x) = 1/2 x\n \u22124.0  \u22123.0  \u22122.0  \u22121.0  0.0  1.0  2.0  3.0  4.0 \n\u22124.0\n\u22123.0\n\u22122.0\n\u22121.0\n0.0\n1.0\n2.0\n3.0\n4.0\nx\ny\nf(x, y) = 3x + 4y\n5\n\nA\ufb03ne Function\nAn a\ufb03ne function (also a\ufb03ne linear function) is a linear function plus a\nconstant: f (x) = vTx + b.\nThe space of a\ufb03ne functions on Rn can be embedded into the space of linear\nfunctions on Rn+1 by appending a \ufb01xed unit constant to the input vector and\ntreating the variable intercept as an extra dimension.\nf (x) = vTx + b\n=\n\u0012v\nb\n\u0013T \u0012x\n1\n\u0013\n= v\u2032Tx\u2032 = f \u2032(x\u2032)\nf \u2032(x\u2032) is linear in x\u2032 on Rn+1.\n6\n\n", "06.support_vector_machines.pdf": "06: Support Vector Machines\nMachine Learning\nBachelor in Computer Science [SP24]\nApril 08, 2024\n\nWhere are we\n\u2022 The Perceptron trains a classi\ufb01er iteratively from data\n\u2022 Its procedure stops as soon as no point is misclassi\ufb01ed\n\u2022 It returns the \ufb01rst decision boundary it \ufb01nds (no further\noptimization)\n\u2022 It cannot be applied to:\n\u2022 Data from a linear process not linearly separable (even just because of noise)\n\u2022 Data from a non-linear process (e.g. a class with multiple clusters)\n\u2022 Non-vectorial data types (e.g. strings, trees, sequences, etc.)\n\u2022 In practice, its applicability is very limited:\nwe need a better classi\ufb01er\n1\n\nToday\u2019s menu\n\u2022 Margin recap\n\u2022 Optimal margin: Linear Hard-Margin SVM\n\u2022 Margin violations: Linear Soft-Margin SVM\n\u2022 Regularized Empirical Risk Minimization\n\u2022 Training SVMs with Quadratic Programming\nToday we will only work with linear SVMs,\nwe will introduce kernels next week\n2\n\nMotivation\n\u2022 Recall: what (not how!) does the Perceptron learn?\n\u2022 Input: linearly separable data for binary classi\ufb01cation\n\u2022 Output: a\ufb03ne/linear function, a\ufb03ne/linear separation\n\u2022 We will now answer the following four questions:\n\u2022 Is the Perceptron solution good? Optimal?\n\u2022 Can its reasoning be extended to non-separable data?\n\u2022 How can we extend the Perceptron to non-linear separation?\n\u2022 Could we work directly on discrete features?\n3\n\nThe Perceptron Revisited\nWhich of the classi\ufb01ers above is better? Why?\nIntuitively, the better separation has a larger \u201csafety margin\u201d.\n4\n\nThe Margin\n\u2022 We have seen margins before, namely in the analysis of the Perceptron\nalgorithm.\n\u2022 Reminder: Let f : Rp \u2192R, f (x) = \u27e8w, x\u27e9+ b, be an a\ufb03ne linear function,\nand let (x, y) \u2208Rn \u00d7 {\u22121, +1} be a labeled example. Then\ny \u00b7 f (x)\nis the margin of example (x, y) with respect to f . If it is positive, the\nexample is classi\ufb01ed correctly by h(x) = sign(f (x))\n\u2022 Let D =\n\b(x1, y1), . . . , (xn, yn)\n\t\nbe a (training) data set.\nWe call \u03b3 the margin of boundary f with respect to D is the smallest margin\namong examples in D:\n\u03b3 = min\ni\n{yi \u00b7 f (xi)}\nIf it is positive, then f separates the two classes\n5\n\nMaximum Margin Separation\n\u2022 The Perceptron algorithm changes the weight vector until a\nlinear separation is found, that is, until the margin \u03b3 becomes\npositive.\n\u2022 However, the resulting (boundary) margin is often small.\n\u2022 Intuitively a large margin should be better than a small margin:\nallowing more spacing between the classes should lead to better\ngeneralization.\n\u2022 This leads to the idea of maximum margin separation:\ninstead of \ufb01nding only any positive margin, we should aim for\nthe largest possible margin.\n6\n\nMaximum Margin Separation\nMaximum Margin Separation can be written as an optimization problem:\nargmax\nw,b {\u03b3}\ns.t. \u2200i \u2208{1, . . . , n} :\nyi \u00b7 (\u27e8w, xi\u27e9+ b) \u2265\u03b3 ,\n\u2225w\u2225= 1\n\u2022 Any algorithm that identi\ufb01es the optimal solution of the above problem\nis a maximum margin learning algorithm\n\u2022 Connections between machine learning and classical optimization are\ncommon (as with statistical analysis): many algorithms can be understood\nas an iterative solution of an (unconstrained) optimization problem\n\u2022 This de\ufb01nition however is very nasty to optimize: \u2225w\u2225= 1 is a non-convex\nconstraint, which is bad for gradient descent algorithms\n(we need well de\ufb01ned, possibly smooth derivatives)\n7\n\nMaximum Margin Separation as a Quadratic Problem\nWe can instead reformulate the optimization problem as follows:\nargmin\nw,b\n\u001a1\n2\u2225w\u22252\n\u001b\ns.t.\n\u2200i \u2208{1, . . . , n} :\nyi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651\n\u2022 Maximizing \u03b3 is the same as minimizing 1/(2\u03b32)\n\u2022 Using \u03b3 = 1/\u2225w\u2225removes \u03b3 from the equation\n\u2022 The objective function is quadratic and the constraints are linear: this form\nis called a quadratic program\n\u2022 Quadratic Programming is a classical CS problem that can be easily solved\nwith o\ufb00-the-shelf optimizers (we won\u2019t go into details here)\n\u2022 The solution of the convex quadratic program for the maximum margin\nseparation is called linear hard-margin support vector machine (SVM).\n8\n\nSupport Vectors and Hard Margin\nargmin\nw,b\n\u001a1\n2\u2225w\u22252\n\u001b\ns.t.\n\u2200i \u2208{1, . . . , n} : yi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651\n\u2022 This \u201cmachine\u201d is called linear because its hypothesis class (model)\nf (x) = \u27e8w, x\u27e9+ b consists of linear functions, doing linear separation\n\u2022 We will understand later today why it is called \u201chard-margin\u201d\n\u2022 Data points with minimal margin are called support vectors (SV):\n(xi, yi)\ns.t.\nyi \u00b7 f (xi) = 1. These are located by de\ufb01nition at a distance\nof \u03b3 = 1/\u2225w\u2225from the separating hyperplane\n\u2022 The solution ignores examples that are not support vectors, because their\nmargin will always be greater than \u03b3, and thus ignored in the de\ufb01nition of\nthe boundary\u2019s margin (which is the minimum \u03b3 among all points)\n9\n\nGeometrical De\ufb01nition of Margin\ngamma\n\u2022 The boundary\u2019s margin \u03b3 is\nthe minimum margin among\nall points in the dataset\n\u2022 Points further from the\ndecision boundary have larger\nmargin, thus ignored for \u03b3\n\u2022 This set is linearly separable:\nthe minimum distance\nbetween two points of\nopposite classes is 2\u03b3\n10\n\nMargin Optimality\nReturning to our original Perceptron questions:\nQ: Is the Perceptron solution good? Optimal?\nA: No, because it does not optimize the margin.\nThe SVM solution is optimal in the sense that it has maximum margin.\nWe will now explore the second question:\nQ: Can its reasoning be extended to non-separable data?\n11\n\nMargin Violations and Slack Variables\n\u2022 Assume that dataset D is not linearly separable\nargmin\nw,b\n\u001a1\n2\u2225w\u22252\n\u001b\ns.t.\n\u2200i \u2208{1, . . . , n} : yi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651\n\u2022 The hard-margin SVM optimization problem has now\ncontradicting constraints, and thus an empty feasible region\n\u2022 However, it would be \ufb01ne in practice if we just get a large margin for most\nof the examples: we need to allow for violations of the margin constraint:\nyi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651\n\u2192\nyi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651 \u2212\u03bei\n\u2022 Margin violations are explicitly handled through slack variables \u03bei \u22650\n(That\u2019s a Greek xi), which measure the violation size\n\u2022 We enforce a large margin and allow for outliers by requiring for \u03bei \u22650,\nwhile keeping the violations size \u03bei as small as possible.\n12\n\nGeometrical De\ufb01nition of non-separability\ngamma\n\u2022 This set is not linearly separable.\n\u2022 Slack variables \u03bei are the distance\nfrom a misclassi\ufb01ed point until its\ncorrect classi\ufb01cation margin.\n\u2022 Notice we could have picked a\nsmaller margin and eliminate one\nslack: we need to trade o\ufb00between\nthe two objectives.\n13\n\nOptimizing Two Objectives\n\u2022 Now we have two distinct and opposite goals:\n\u2022 Maximize the margin\n\u2022 Minimize margin violations\n\u2022 This is our very \ufb01rst encounter with multiobjective optimization\n\u2022 For now, the easiest way to deal with multiple objectives is to optimize\na linear combination (weighted sum) of the objectives:\narg min\nw,b,\u03be\n(\n1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\n\u03bei\n)\n\u2022 Note how the problem is composed of two objectives:\n\u2022\n1\n2\u2225w\u22252 controls the margin size\n\u2022 C \u2211\u03bei controls the margin violations\n\u2022 The hyperparameter C > 0 trades-o\ufb00their relative importance, based on\nthe problem\u2019s priorities: large margin vs. small (sum of) margin violations\n14\n\nMaximizing Margin vs. Minimizing Violations: C\ngamma\ngamma\nLarge C: violations > margin\nSmall C: margin > violations\n15\n\nSoft-Margin SVMs\n\u2022 The linear soft-margin support vector machine is de\ufb01ned as the\nsolution of the following (convex) quadratic program:\narg min\nw,b,\u03be\n(\n1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\n\u03bei\n)\ns.t.\n\u2200i \u2208{1, . . . , n} :\nyi \u00b7 (\u27e8w, xi\u27e9+ b) \u22651 \u2212\u03bei\nand\n\u03bei \u22650\n\u2022 This SVM is called \u201csoft-margin\u201d because it replaces its previous \u201chard\u201d\nmargin constraint with a \u201csoftened\u201d version that can be violated by a\ncontrolled amount \u03bei\n\u2022 For large enough C, margin violations become extremely costly, and the\noptimal solution should not violate any margins if the data is separable: The\nhard-margin SVM can be derived as a special case\n16\n\nUnderstanding the C Parameter\narg min\nw,b,\u03be\n(\n1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\n\u03bei\n)\ns.t.\n\u2200i \u2208{1, . . . , n} :\nyi (\u27e8w, xi\u27e9+ b) \u22651 \u2212\u03bei\nand\n\u03bei \u22650\n\u2022 The parameter C controls the trade-o\ufb00between the two objectives of\nmaximizing the margin and minimizing margin violations\n\u2022 It is known under di\ufb00erent names, such as \u201ctrade-o\ufb00parameter\u201d,\n\u201cregularization parameter\u201d, and \u201ccomplexity control parameter\u201d\n\u2022 These names (and their meaning) will become more clear in the following\n17\n\nThree Types of Points\nFor a given decision boundary we can now partition the training examples in D\ninto three categories:\n\u2022 Non-SV examples have a margin larger than 1: they do not a\ufb00ect the\nsolution and can be safely ignored\n\u2022 \u201cSome\u201d support vectors are located exactly on the margin hyperplane:\nthey have a margin of exactly 1.\n\u2022 Other support vectors are margin violators, with a margin smaller than 1:\nthey still count because of their positive slack variable \u03bei > 0\n(They can even be misclassi\ufb01ed if margin is negative and \u03bei \u22651)\nThe term large margin classi\ufb01er is also used for soft-margin SVMs, in contrast\nto maximum-margin classi\ufb01er\n18\n\nThree Types of Points\ngamma\nA soft-margin SVM partitions the points\ninto three types:\n\u2022 Translucent: non-SV (margin > 1)\n\u2022 Brighter: SV (margin = 1, \u03be = 0)\n\u2022 Darker: Violator (margin < 1,\n\u03be \u2208(0, 1] if correctly classi\ufb01ed,\n\u03be > 1 if misclassi\ufb01ed)\n19\n\nRegularized Empirical Risk Minimization\n\u2022 We begun with understanding SVMs from a geometrical approach: the\nmargin is a distance, which we maximize\n\u2022 The slack variables however are not proper distances: the quantity by which\nan example violates the margin is \u03b3 \u00b7 \u03bei\n\u2022 There is a limit in transferring the geometric intuition from hard-margin\nSVMs to the soft margin case\n\u2022 Next, let\u2019s re-examine soft-margin SVMs under the interpretation of being\nregularized empirical risk minimizers\n20\n\nRegularized Empirical Risk Minimization\n1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\nL(yi \u00b7 f (xi))\n\u2022 While maximizing the margin (left term), we have been computing a loss on\nthe margin L(yi \u00b7 f (xi)): minimize the sum of margin violations\nL(m) = \u2211n\ni=1(\u03bei)\n\u2022 Alternatively, we could as well minimize the number of misclassi\ufb01ed\nexamples.\n\u2022 The corresponding Loss is L(m) = 1 if m \u22640, and L(m) = 0 if m > 0\n\u2022 This function is not convex, making its optimization NP-hard\n\u2022 Instead, let\u2019s use a convex relaxation based on the hinge loss, which\nmeasures the margin violation \u03bei as L(m) = max{0, 1 \u2212m}\n21\n\n \u22123.0 \n \u22122.0 \n \u22121.0 \n 0.0 \n 1.0 \n 2.0 \n 3.0 \n0.0\n1.0\n2.0\n3.0\n4.0\n22\n\nAn Alternative De\ufb01nition\n\u2022 A soft-margin SVM is thereby the solution of the unconstrained convex\noptimization problem:\nargmin\nw,b\n(\n1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\nL\n\u0000yi \u00b7 (\u27e8w, xi\u27e9+ b)\n\u0001\n)\nwhere L(m) = max{0, 1 \u2212m} denotes the hinge loss\n\u2022 The problem is still composed of two terms:\n1\n2\u2225w\u22252 (margin) and C \u2211L(m) (violations)\n23\n\nRegularizer and Empirical Risk\n\u2022 The \ufb01rst term (margin) 1\n2\u2225w\u22252 is a so-called regularizer: it encodes a\npreference for solutions of a certain form, in this case for small \u2225w\u2225which\ncorresponds to a large margin \u03b3 = 1/\u2225w\u2225.\n\u2022 The second term (violations) happens to be once again a multiple of the\nempirical risk, de\ufb01ned as:\nC \u00b7 1\nn\nn\n\u2211\ni=1\nL\n\u0000yi \u00b7 (\u27e8w, xi\u27e9+ b)\n\u0001\nwith respect to the hinge loss L\n24\n\nGeneralizing the Loss\n\u2022 Understanding the roles of the regularization, Risk, Loss and minimization,\nallows to gracefully generalize SVMs to arbitrary Loss functions\n\u2022 SVM loss functions need to be always convex, and depends on the input\nonly in terms of f (x) = \u27e8w, x\u27e9+ b\n\u2022 Typical choices for classi\ufb01cation:\n\u2022 Hinge Loss: max{0, 1 \u2212y \u00b7 f (x)}\n\u2022 Squared Hinge Loss: (max{0, 1 \u2212y \u00b7 f (x)})2\n\u2022 Huber Loss (smoothed Hinge Loss)\n\u2022 Typical choices for regression (remember, it\u2019s just an application!):\n\u2022 Squared Loss (y \u2212f (x))2 (often misused for classi\ufb01cation!)\n\u2022 Squared \u03b5-insensitive Loss\n25\n\n \u22123.0 \n \u22122.0 \n \u22121.0 \n 0.0 \n 1.0 \n 2.0 \n 3.0 \n0.0\n1.0\n2.0\n3.0\n4.0\n\u2022 Zero-one Loss:\n1 if m \u22640, 0 if m > 0\n\u2022 Hinge Loss:\nmax{0, 1 \u2212y \u00b7 f (x)}\n\u2022 Squared Hinge Loss:\n(max{0, 1 \u2212y \u00b7 f (x)})2\n\u2022 Huber Loss: (Smoothed Hinge)\n26\n\nTraining Support Vector Machines\n\u2022 Quadratic Programming is a vast topic in and by itself: in this course we\njust ignore it as a solved problem\n\u2022 De\ufb01ning the SVM as regularized empirical risk minimization problem\nhowever provides us with a simple alternative: we can perform gradient\ndescent on the objective function:\nF(w, b) = 1\n2\u2225w\u22252 + C \u00b7\nn\n\u2211\ni=1\nL\n\u0000yi \u00b7 (\u27e8w, xi\u27e9+ b)\n\u0001\n\u2022 We will see gradient descent in depth with Neural Networks; for now, know\nthat it can be done in batch mode, online, or with mini batches\n\u2022 Mini-batches with a cooling scheme \u03b7 \u221d1/t leads to the very e\ufb00ective\nPEGASOS algorithm (Shalev-Shwartz et al., 2007)\n\u2022 There exist even more specialized SVM training algorithms: SVMs have\nbeen heavily optimized over the past decades, and you can often \ufb01nd a\nsolution tailored to your exact needs\n27\n\nSoft-Margin SVMs are applicable\nto non-separable data\nThis answers our second question:\nQ: Can its reasoning be extended to non-separable data?\nA: Yes, by allowing for margin violations, which are then penalized in the\nobjective function\nThere are two questions left:\nQ: How can we extend the Perceptron to non-linear separation?\nQ: Could we work directly on discrete features?\nWe will address these next week using Kernels\n28\n\nSummary\n\u2022 The de\ufb01nition of margin allows comparing the quality of discrimination\nboundaries in classi\ufb01cation problems\n\u2022 The Perceptron uses the margin uniquely to identify misclassi\ufb01ed examples,\nbut does not optimize its quality\n\u2022 Support Vector Machines compute optimal boundaries by maximizing the\nmargin under constraints\n\u2022 The resulting margin is built using the Support Vectors (SVs), examples (=\ndata points) which are closest (minimum distance) to the separation\nhyperplane\n\u2022 The introduction of slack variables enables coping with misclassi\ufb01ed\nexamples, by controlling the acceptable amount of margin violations\n\u2022 Training an SVM corresponds to solving its quadratic optimization problem,\nwhich can be reinterpreted as another case of gradient descent\n29\n\nExtra material\n\u2022 Full derivation for SVM (part VI, because most teach SVMs after kernels):\nhttp://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf\n\u2022 Simpler and perhaps clearer derivation (but longer):\nhttps://sustech-cs-courses.github.io/IDA/materials/\nClassification/SVM_tutorial.pdf\n\u2022 Why we kept b in the past weeks? Because of W regularization:\nhttps://stats.stackexchange.com/questions/184272/why-is-the-\nbias-term-in-svm-estimated-separately-instead-of-an-extra-\ndimension\n\u2022 Quadratic Programming in Python: https:\n//scaron.info/blog/quadratic-programming-in-python.html\n\u2022 Good guide on practical applications:\nhttps://scikit-learn.org/stable/modules/svm.html\n\u2022 PEGASOS implementation (check out the speed section):\nhttps://github.com/ejlb/pegasos\n30\n\n", "00.lab_notes.pdf": "Lab notes\nMachine Learning\nBachelor in Computer Science [SP24]\nFebruary 26, 2024\n\nWelcome to the \ufb01rst lab\n\u2022 Pick your (\ufb01rst) teammate\n\u2022 Set up a Python environment\n\u2022 Work through the \ufb01rst assignment\n\u2022 You need to work in pairs \u2013 you can change teammates any time\n\u2022 Submissions are personal for each student\n\u2022 If you do not have access to a laptop, team up with a friend\n(or talk to us for alternatives)\n\u2022 You should have a working local installation, we support pipenv\n\u2022 There is also Google Colab if you prefer (and has GPUs/TPUs)\n\u2022 Any other environment is allowed but under your own responsibility\n1\n\nLearning Python\nWe will highlight and refresh the Python features we use, but you need build up\nprogramming experience by yourself if you don\u2019t yet.\nIf you need to learn Python, fast,\nI strongly recommend you follow an online course.\n2\nE.g., https://www.w3schools.com/python/\n\nFormat\n\u2022 Assignments are distributed as Jupyter Notebooks: https://jupyter.org/\n\u2022 They provide an interactive interface to Python 3 from a web browser\n\u2022 Jupyter notebooks are single \ufb01les with extension .ipynb\n\u2022 Main advantages:\n\u2022 Advanced text markup using markdown and latex\n\u2022 Embedded media (e.g. plotting)\n\u2022 The server can run locally or on a remote machine\n\u2022 Your local setup will launch a Jupyter notebook server on your\nmachine, then connect to it with your browser\n\u2022 A remote setup simply connects to a server using your browser\n3\n\nGoogle Colab\n\u2022 Log into https://colab.research.google.com/\n\u2022 You need a Google account to use it\n(feel free to make a throwaway account just for the assignments)\n\u2022 Main advantages:\n\u2022 Zero setup: just login and run, today\n\u2022 All the computation runs remotely: you only need a browser, any\nChromebook or old hardware will work, even for lectures requiring GPUs\n\u2022 You need to (i) download the assignment \ufb01le, (ii) upload it to your Colab\naccount (check the File menu), then (iii) you can edit it and run it from\nthe browser\n5\n\nLocal Python installation\n\u2022 We only support pipenv\n\u2022 python3 -m pip install \u2013user pipenv\n\u2022 python3 -m pipenv install\n\u2022 python3 -m pipenv run jupyter notebook\n\u2022 Here\u2019s some guides to install Python 3 and pip:\n\u2022 Mac:\nhttps://programwithus.com/learn-to-code/install-python3-mac/\n\u2022 Windows:\nhttps://phoenixnap.com/kb/how-to-install-python-3-windows\n\u2022 Ubuntu: sudo apt install python3 python3-pip\n\u2022 Pyenv: this is easier https://github.com/pyenv/pyenv\n6\n\nSubmission\n\u2022 The assignments are mandatory, submit your solution through\nMoodle\n\u2022 Your submission should be a single .ipynb notebook \ufb01le,\ncompleted with you name and your answers\n\u2022 You can attach extra \ufb01les if needed (.py, PDF, signatures,\netc.), but the only \ufb01le evaluated is the .ipynb\n\u2022 Start early at least the \ufb01rst weeks, no late submissions\n7\n\nMath recaps\n\u2022 Sometimes we will have a quick math recap before the lab\n\u2022 The content should already be familiar, so we will quickly\nrefresh a few concepts that will be used in the next lecture\n\u2022 If you feel that anything is unclear, you need to fully undestand\nit before the next lecture\n8\n\n", "ReinforcementLearning.pdf": "Reinforcement Learning\nPhilippe Cudr\u00e9-Mauroux\nSlides by Julien Audiffren\nMay, 27th 2024\n\nThe three branches of ML\nReinforcement Learning (RL) is \nvery rich, we will barely scratch the \nsurface today\nA 15 hours class on RL that \ngoes a bit deeper\nhttps://www.davidsilver.uk/teaching/\nGoal:\nGoal: Identify some properties \nof X\n\nHow do toddlers learn to walk ?\nStep 1) They collect as much data\nas possible\nStep 2) They label examples of good \nwalking and bad walking (a.k.a falling) \nStep 3) They deduce how to walk  \nSupervised learning approach\n(Un)Supervised learning paradigms do not apply\n\nHow do toddlers learn to walk ?\n\u2794\nOf course it does not work like that\n\u2794\nToddlers learn to walk by trying to walk\n\u2794\nSame for many other things (biking, driving, \u2026.)\n\u2794\nThis is the starting point of RL : learning things by doing things\n\nThe RL paradigm\nEnvironment\nAgent\nActions\nFeedback\nare not identically distributed\nare not even independent \n\nDoes it work ?\n\nSome applications of RL \nSelf Driving Cars\nSuperhuman Gaming\n-\nChess (Deep Blue) \n-\nStarcraft (AlphaStar)\n-\nGo (AlphaGo)\n-\nAnd so much more...\nAlso self driving train, helicopters, ...\nRecommender Systems\nTo each its own \nadvertisement, recommended \npost, product ...\n\nThe many faces of RL\nReinforcement \nLearning\nMathematics\nComputer Science\nNeuroscience\nPsychology\nEconomics\nEngineering\nEx :  Pavlovian Conditioning\nEx :  Dopamine reward circuit\nEx :  Auctions Theory\nEx : \nLarge Deviation theory\nSupermartingales \nand so much more\nEx :  Optimal control\n\nToday\u2019s class\n-\nMulti Armed Bandits (MAB)\nExploration versus Exploitation\n-\nMarkov Decision Process (MDP)\nState, planning and   - function\n-\nSome Extension & Limits\n\nMulti Armed Bandits \nOptimism Against Uncertainty\n\nMAB origin story\n\u2794\nThe agent is in a Casino, and will stay there for \nminutes\n\u2794\nAt each time           , she can choose and play any of \nthe multiple one armed bandit (aka pull any arm).\nGoal : \nMaximize the expected agent gain\nChallenges :\n\u2794\nSome machines are better than \nothers (higher average gain), but \nwhich one ?\n\u2794\nThe reward of machines are \nrandom\nExample of applications\nClinical \nTrials\n\nExample\nARM 1\nARM 2\nt=1. Pull arm 1\nr = 1\nt=2. Pull arm 1\nt=3. Pull arm 2\nt=4. Pull arm 2\nr = 0\nr = 0\nr = 0\nWhat next ?\n\nMulti Armed Bandits\nEnvironment\nAgent\nActions:\npull an arm\nFeedback:\nobtain a random \nreward\n\nMAB : Formal Definition (simplest setting)\nNotation:\nis the time horizon\nis the number of arms\nare 2 independent, real \nvalued, bounded between 0 and 1, \nrandom variables.\nThe problem:\nAt each time t = 1, \u2026,T  \nThe agent chooses to pull arm      (    =1 or 2)\nThe agent observes a realization of       , conditionally independent from \nall previous sampling\nAt the end we compute the (pseudo)-Regret of the agent ;\nwhere \nObjective : \nDesign a strategy that minimizes the regret \n\nRegret in details\n\u2794\nSimplifying the formula. \nSuppose that                . Then\n\u2794Pseudo-Regret measures loss in expectation\n\u25c6\nExpectation always wins in the long run (law of large number)\n\u25c6\nRegret w.r.t. observed value is meaningless\u2026. except in adversarial setting\n\nOne example\nRegret of playing arms alternatively :\nBest regret achievable for consistent \nalgorithm:\nARM 1\nARM 2\nt=1. Pull arm 1\nr = 1\nt=2. Pull arm 1\nt=3. Pull arm 2\nt=4. Pull arm 2\nr = 0\nr = 0\nr = 0\nWhat next ?\nTrue values :  \nRegret of playing only one arm :\n(50 % to be wrong!)\nCan we achieve this optimal regret ?\nTheorem\n\nExploration vs exploitation\nRegret of playing arms alternatively :\nRegret of playing only one arm :\nWe need both\n\nForced Exploration :      - greedy\n-greedy used with                               has \nthe following regret upper bound\n\u2794\nNatural implementation of the Exploration principle\n\u2794\nover time, as our estimates become better, to avoid Linear Regret\nBetter than linear regret, but \nfar from the lower bound.\nCan we do better ?\nTheorem\n\nQuantifying Uncertainty\nLarge Deviation Theory:\nQuantify uncertainty. Control the tail of the distribution of empirical averages \nHow different is what the agent observes (Empirical average) \nfrom the truth (Mean) ?\nif X is Lebesgue integrable, then \nLaw of Large Numbers\nif X is square Lebesgue integrable with variance \n\u03c32, then \nCentral Limit Theorem\n\nAzuma-Hoeffding Inequality\nis in this interval with \nprobability \nIn our setting, it implies\nLet  (Zi ) be a martingale with respect to the filtration (Fi) with Z0=0. Suppose that \u2203(ci) such that \nThen, \nTheorem\n\nThe Optimal Solution: UCB\n\u201cImplicit \nExploration\u201d\n\nMarkov Decision Process\n\nThe case for state\nMAB are not enough to modelize many problems\n\u2794The value of actions may change depending on the \nsituation\n\u2794Sequences of actions may be required to achieve goal\nEx : Ice Maze\n-\nThe agent need to learn to cross the ice \nmaze \n-\nShe starts at the top left corner and \nmust reach the END square\n-\nShe should avoid stepping on the thin \nice (in red)\nSTART\nEND\n\nMDP : Adding states to MAB\nEnvironment\nAgent\nActions:\nchoose a  \ndirection\nFeedback:\nobserve a reward, and \nnew state\n\nWhy MDP are more complicated\n\u2794Reward may be significantly delayed (ex : only when the end is reached)\n\u2794Time matters : rewards for actions depend on state,which depends on \nprevious actions !\n\u2794The Agent\u2019s action affect which states the agent will encounter \nSTART\nr=0\nr=-10\nr=0\nr=-10\nr=0\nr=0\nr=-10\nr=0\nr=0\nr=0\nr=0\nr=-10\nr=0\nr=-10\nr=0\nr=0\nr=0\nr=0\nr=-10\nEND\nr=10\n\nFormal definition of (finite - SA) MDP\na MDP is defined as a  tuple \nSet of states\nSet of actions\nFunction of transition\nTerminal state(s) \nInitial distribution \nDiscount factor\nReward\n\nFormal definition of (finite - SA) MDP\nand    are finite sets, and are generally supposed to be known. Together they \ndefine the structure of the problem\nSTART\ns1\ns5\ns2\ns3\ns4\nEND\nIn this example : \nis the set of all the cases\nis all four possible directions ( up down, left, right) \n\nFormal definition of (finite - SA) MDP\nis the function of transition. It defines the dynamics of the problem.\nSTART\nEND\ndefines the probability of moving from s\nto s\u2019 when using the action a\nIn this example,     is deterministic\nDistributions over \n\nThe Markov property\n\u2794The Transition function only depends on the present, not the past \n\u2794This is called the Markov property\nLet       be a stochastic process. Then        is markovian iff\n\u2794This will be important later in the class\n\nFormal definition of (finite - SA) MDP\nreward\n,  bounded\n\u2794Like in MAB,     is the quantity of interest.\n\u2794\ndepends on the state and the action\n\u2794Defines the real objective of the problem:\nThe objective of the agent is to maximize its reward\nSTART\nr=0\nr=-10\nr=0\nr=-10\nr=0\nr=0\nr=-10\nr=0\nr=0\nr=0\nr=0\nr=-10\nr=0\nr=-10\nr=0\nr=0\nr=0\nr=0\nr=-10\nEND\nr=10\n\nFormal definition of (finite - SA) MDP\nThe discount factor \n\u2794\n\u2794\nRewards that occur later are discounted:\n\u2794\nHaving                  favors rewards sooner rather than later. Useful for e.g. not starving in the \nmaze. \n\u2794\ncan also encodes a MDP that stops at an unknown random time. \n\u2794\nFinally,                 ensure convergence of many of MDP solvers.\n\nFormal definition of (finite - SA) MDP\nare respectively the start and finish \ncondition of the process\n\u2794\nis a distribution over states, defines where the agent \nstarts.\n\u2794\nare the ending state(s). The agent journey \nstops when she reaches one of them.\n\u2794There exists other types of ending condition, or no \nending at all.\n\u2794The trajectory of an agent between start and finish is \ncalled an epoch \nSTART\ns1\ns5\ns2\ns3\ns4\nEND\n\nOne other example of MDP\nExample from\nhttps://www.davidsilver.uk/teaching/\nStart\nEnd\n\nThe answer to MDPs : the policy\n\u2794A policy\ndescribes the agent\u2019s behaviour\n\u2794\n\u201cIn State s do action                    \u201d \n\u2794Stochastic policies also exist. In this case,          defines a \ndistribution over \n\u2794An agent following a stochastic policy samples\nto \nchoose its new action\nSTART\nEND\n\nEvaluating policies :\nHow good is a policy ?\nIdea : follows the agent over time\nThe sequence of state reached by the \nagent is a Markov Chain. \nTotal reward for the agent:\nHow to compute this in practice ?\nTheorem\n\nEvaluating policies : the V & Q functions\n= Expected total reward acquired by the agent following policy\nstarting\nfrom state s\n= Expected total reward acquired by the agent following policy    starting \nfrom state s doing action a\nThe total reward acquired by the agent is \nComputing           :\n(case deterministic reward)\nBELLMAN \nEQUATION\nV and Q have similar role,\njust use the most convenient \n\nSolving Bellman\u2019s equation \nBellman\u2019s equation can be rewritten matricially\nVector of size N, \ncontaining the value of \neach state s\nVector of size N, \ncontaining the reward \nfor each pair \nTransition matrix for \nthe Markov Chain \nassociated with the \nagent\nWhose solution is \nguaranteed invertible  if      <1\n\nExample : Evaluating the V function \n2\n1\n3\n4\n5\n\nFinding the optimal policy\nThe         function induces partial order on policies. \nGoal of MDP : find      !! \ni.f.f. \nThere exists a Condorcet winner       , i.e.\nis called the optimal policy.\nIt can always be chosen deterministic !\nhas the best expected reward\nTheorem\n\nFinding \nBellman Optimality Equation \nIntuition: \npick the action that will lead to the best cumulative reward !\nHowever, this equation has no closed form solution. \nHow to solve it ?\nshortcut for\n\nValue Iteration \n\u25cf\nOne of many methods (arguably the simplest)\n\u25cf\nStart with \n\u25cf\nThen build iteratively\n\u25cf\nConverge in N iteration !  (in this setting)\n\nValue Iteration : Example \n0\n0\n0\n0\nStep 1\n\nValue Iteration : Example \n0\nX\nStep 2\n0\n0\n-1\n0\n10\n0\n0\n\nValue Iteration : Example \n-1\n-1\n8\n10\nStep 3\n\nValue Iteration : Example \n-1\n6\n8\n10\nStep 4\n\nValue Iteration : Example \n6\n6\n8\n10\nStep 5\n\nValue Iteration : Example \n6\n6\n8\n10\nStep 6\nOptimal policy\nhas \nconverged\n\nExploration in MDP : unknown R / \n\u2794If rewards and/or transitions are unknown and stochastic, the agent has to \nlearn \u201con the job\u201d\n?\n?\n?\n?\n\u2794\nCan be we still use the greedy algorithm ?\n\u25c6\nNo ! Remember MAB. \n\u25c6\nThere is a need for EXPLORATION\n\u2794\nCan we still find the optimal policy ? \n\u25c6\nYes.\n\u25c6\nTwo families of methods : On Policy / Off Policy\n\nOn / Off policy learning\nOn-Policy Learning :\n-\nIterate over     to converge toward\n-\nPlay according to    , and try to improve it\n-\nRequire enforced exploration and long term planning (such as Time Difference methods)\n-\nMany successful algorithm, such as SARSA  \nOff-Policy Learning :\n-\nPlay using two policies, the behaviour\n, and the target\n-\nExploration comes from the difference between\nand \n-\nMost Famous algorithm : Q-learning\n\nQ learning\n\u2794Objective :  learn\n\u2794The agent learns two policies at the same time :        and      (off policy)\n\u25c6\nshould embed exploration   (ex :     -greedy)\n\u25c6\nhas no need for exploration and should be greedy w.r.t \n\u2794The algorithm :\n\u25c6\nStart by initializing     to a neutral value\n\u25c6\nAt each time t, when the agent is in state     :\n\u25cf\nChoose next action     to do by using  \n\u25cf\nImagine the 2nd next action            using \n\u25cf\nUpdate      using\nis the learning coefficient\n\nQ learning\nIn this setting, by using the Q-learning algorithm, \nconverge toward \nExample :    =       -greedy,         = greedy w.r.t.\nAt time t\nwith proba   \n, do a \nrandom \naction\nwith proba \n(1-\n), do\nchoose\nbut don\u2019t execute it\n1.\n2.\nreach state \n3.\n4.\nUpdate \nREPEAT\nTheorem\n\nQ-function approximation\n\u2794Often directly learning the Q function is impossible\n\u25c6\nThere might be too many states (ex : Go             has                         )\n\u25c6\nStates might even be continuous (ex : walking robot)\n\u2794\nSolution: Approximate the Q/V function by using features\n\u25c6\nWe assume that each pair of (state,action) is fully described by a feature function \n\u25c6\nThe agent has access to these feature as parts of her observation\nAssumption : \nNew objective : Learn f !\n\u25cf\nwhere          , a set of \u201ceasy to learn\u201d \nfunctions\n\u25cf\nEx :      linear maps\n\u25cf\nEx 2 :      deep learning network \n\nDeep Q Learning (DQN)\n\u2794Choose a DNN architecture \n\u2794Initialize       at random\n\u2794Play the agent using Q-learning with     =   -greedy and     = \ngreedy w.r.t                  , collecting the agent trajectory \n\u2794After each epoch / fixed interval of time, update     by using \ncollected data to minimize  \n\u201cBellman\u201d Term\n\nDQN limitations, Double DQN\n\u2794DQN is a useful and simple algorithm\n\u2794Q-learning always converges, but DQN most often don\u2019t\n\u2794This is because in this setting        and    are not \u201cdistant\u201d enough, which \nprevent     to converge toward the optimal Q function\n\u2794Many improvement have been developed since, such as Double Q learning. \n\nSome Extensions and Limitations\n\nNon Markovian Problems\n\u2794Many problems are not Markovian\n\u25c6\nEx : Some information may be missing\n\u2794These problem can be modelized using\nPOMDP \n\u2794POMDP are much more complex than MDP\n\u2794Main idea to solve POMDP: include the past\nin states to make the problem markovian\nWhat if only the position \nis available ?\n\nEverything is reward\n\u2794The reward is at the heart of Reinforcement Learning.\n\u2794Can anything be expressed as a reward ?\n\u2794What if there are multiple objective ?\n\u25c6\nHow to weight them ?\n\u25c6\nEverything is trade-off\nSAFE\nFAST\nECO\n\nWhen the AI \u201ccheats\u201d\nIn RL we get what we ask for : the agent only optimizes the reward, \ntherefore it is our responsibility to carefully design it\nMany more examples in this \npaper \nhttps://arxiv.org/pdf/1803.034\n53.pdf\n\n", "11.deep_learning.pdf": "11: Deep Learning\nMachine Learning\nBachelor in Computer Science [SP24]\nMay 13, 2024\nSome material courtesy of Dr. A. Tonon\nPresentedby Dr Anna Scius-Bertrand\nSlides fromGiuseppe Cuccu and, \nMSE \u2013 TSM-DeLearn\n\nToday\u2019s menu\n\u2022 Deep Learning, typologie, learning strategies\n\u2022 Increasing training data\n\u2022 Transfert learning\n\u2022 Deep CNNs Architectures\n\u2022 Transformers\n\u2022 Limitations\n\n\nFrom rules based to deep learning\n\nDeep Learning\n\nMachine Learning Typologie\nhttps://vitalflux.com/great-mind-maps-for-learning-machine-learning/\n\nLearning Strategies\n\u2022\nSupervised learning: The goal is to extract some relevant \nfeatures x from raw observation data o and to learn a mapping \nfrom inputs x to outputs y given a set of example data called the \ntraining set.\n\u2022\nUnsupervised learning: The goal is to discover interesting \nstructures from inputs x given a set of data called the training set.\n\u2022\nSemi-supervised learning: Combining labeled and unlabeled data \nto train a model.\n\u2022\nSelf-supervised learning: Model train with unlabelled data. \nPredicted label is used to retrained the model. \n\nHow to train a ANN?\n\u2022\nSplit your data in 3 sets:\n-\nTraining set: Used to train and optimize the models\n-\nValidation set: Used during training to evaluate performance of \nthe model on unseen samples and to avoid overfitting\n-\nTest set: Used for quantitative evaluation\n\u2022\nClassical repartition (60, 20, 20) or (70, 15, 15)\n\nStopping the training\n\nIncreasing training \ndata\n10\nData augmentation\nSynthetic data\n\nPrinciples of data augmentation\n\u2022 Overfitting can be caused by having networks with too \nmany parameters that are trained on too few samples. \nThrough training, the model learns by heart and \ngeneralises poorly.\n11\n\u2022 Data augmentation takes the approach of \ngenerating artificially more training data from existing \ntraining samples.\n\u2022 For images, data augmentation is performed via a number of random \ntransformations that yield believable-looking images. \n\u2022 The goal is that at training time, the model will not see the exact same \npicture twice. This helps expose the model to more aspects of the data \nand generalise better.\n\n2D data augmentation\n\u2022 Quantity of \u201cperturbations\u201d are \nrandomly chosen\n\u2022 Rotation\n-\nrange of degrees\n\u2022 Translation\n-\npercentage of shift in a range\n-\nfor width and height\n\u2022 Flip\n-\nmay not be applicable to all types \nof images\n-\ne.g. images with text\n12\nx\n\n2D data augmentation\n\u2022 Shear\n\u2022 Zoom\n\u2022 Color \n-\nshifting\n-\nillumination\n-\n\u2026\n13\n\nMSE - TSM-DeLearn\n14\n\nCNN for FashionMNIST - data augmentation\n\u2022 We may observe that with data augmentation:\n-\nTraining shows less difference between training set and validation set - less prone to overfitting\n-\nOverall improvement is not so high in this case.\n-\nThis is probably due to the \u201cstability\u201d of the images of FashionMNIST\n15\nWithout data augmentation\nWith data augmentation\n\nWho is real?\n1\n2\n5\n4\n3\n6\n\nWho is real?\n1\n2\n5\n4\n3\n6\nhttps://thispersondoesnotexist.com/\n\nGenerative Adversarial Network\nRandom \nvector\nGenerator\nSynthetic \nexample\nDonn\u00e9es r\u00e9elles\nReal example\nDisciminator\nReal \nOR \nSynthetic\nGenerator \nloss function\nBack-propagation\nDiscriminator \nloss function\nBack-propagation\nInput: Real data\nOutput:\nLabel: \nGAN Network: Godefollow and all. 2014\nReal data\n\nTransfer \nlearning\n19\n\nConvolution layers are extracting features\n\u2022 The filter parameters are learnt \nthrough the training process. The \nwhole system is learning not only to \nclassify but also to extract features.\n20\nDeep Learning\nLearning\nTransfer-learning: \nattempt to re-use this part\n\nA hierarchy of features\n\u2022 Above is the AlexNet architecture [2012]\n\u2022 The deeper the network:\n-\nThe bigger are the number of filters in the conv layers\n-\nThe smaller are the activation maps due to max pooling\n\u2022 Intuitively:\n-\nEarly layers will extract lower-level features\n-\nLate layers will extract higher-level features\n21\nIf trained on \u201cbroad\u201d \ntypes of inputs, the layers \nof the network will act as a \n\u201cgeneric\u201d feature \nextractor\n\nTransfer learning - definition\n\u2022 More concretely: re-use the feature extraction part of pre-\ntrained network on which years of engineering have been \nspent\n\u2022 Traditional machine learning: generalise to unseen data based \non patterns learned from the training data\n\u2022 Transfer learning: kickstart the generalisation by starting from \npatterns learned for a different task\n22\n\u2022 Transfer learning is about using knowledge learned \nfrom tasks for which a lot of labelled data is available in \nsettings where only little labelled data is available.\nBut not so different, let\u2019s say different \nin the same \u201cdomain\u201d of tasks.\n\nTransfer learning - discussions\n\u2022 Transfer learning is about exporting knowledge into new \nenvironments\n-\ntransfer learning is therefore a form of generalisation \u201cacross tasks\u201d\n\u2022 Many real-life problems do not have massive amounts of labelled \ndata to train millions of parameters in deep models\n-\nCreating labelled data is expensive, so optimally leveraging existing datasets \nis key.\n-\nNLP Penn treebank dataset took 7 years to label for Part-of-Speech tagging\n\u2022 There is a limit to the concept of transfer learning\n-\nIf the data on which we transfer is too different, then it fails\n-\n\u201cYou should not trust a toddler that drives around in a toy car to be able to \nride a Ferrari\u201d\n23\n\nTransfer learning - best practices\n24\n\nSome real-life feedback on transfer learning\n\u2022 3 main strategies:\n1. Use a pre-trained network as feature extractor; use the features with your favorite classifier, e.g. \nSVM\n2. Remove the last layers of a pre-trained network, freeze the rest, add new classification layers \n(typically Dense) and train only those last layers\n3. Do the step 2 and then perform another round of training on the rest of the network: \u201cunfreeze\u201d \nthe layers and use typically a very small learning rate to avoid destroying what the initial network \nhas learnt = \u201cfine tuning\u201d\n\u2022 Which one to use?\n-\nIt depends to your quantity and quality of data!\n-\nReally few data = 1\n-\nFew data = 2\n-\nFew data and few variabilities intra-class = 2, eventually 3\n-\nReasonable data set covering all potential variabilities = 3\n-\nSome examples we did on image classification: \n-\n50 to 100 images per class with few variabilities = strategy 1\n-\n100 to 1000 images per class = strategy 2\n-\n1000 to 5000 images per class with quite a lot of variabilities = strategy 3\n25\n\nDeep CNN \nArchitectures\nAlexNet\nVGGNet\nResNet\n26\n\nCommon ConvNets\nSome well-known CNN architectures\n\u2022 LeNet-5 (~60K parameters, 1st)\n\u2022 ResNet-50 (~25M parameters)\n\u2022 AlexNet (~60M parameters)\n\u2022 VGG-16 (~138M parameters)\n\u2022 GPT-3 (~175 BILLION parameters)\nCommon patterns\n\u2022 Most layers are convolution chains: conv \u2192 pool \u2192 conv \u2192pool \u2192\u00b7\u00b7\u00b7\n\u2022 Height and width decrease towards late layers, number of filters increases\n\u2022 Last few layers are dedicated to decision making, fully connected (Dense)\n\u2022 Nonlinear activations in (decision) hidden layers\n\u2022 Softmax on output layer for classification\n\nLeNet-5  [1998] \u2013 60k params\n\u2022 LeNet. The first successful applications of Convolutional Networks were developed by Yann LeCun\nin 1990\u2019s. Of these, the best known is the LeNet architecture that was used to read zip codes, \ndigits, etc.\n\u2022 http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n28\n\nImageNet Large Scale Visual Recognition \nChallenge (ILSVRC) - winners\n29\n\nAlexNet [2012]\n\u2022 The first work that popularized Convolutional Networks in Computer Vision was the \nAlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. \n\u2022 The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly \noutperformed the second runner-up (top 5 error of 16% compared to runner-up with \n26% error). \n\u2022 The Network had a very similar architecture to LeNet, but was deeper, bigger, and \nfeatured Convolutional Layers stacked on top of each other (previously it was common \nto only have a single CONV layer always immediately followed by a POOL layer).\n\u2022 http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-\nneural-networks\n30\n\nAlexNet [2012]\nInput : 227x227x3 images\nFirst layer (CONV1): 96 filters \n11x11, S = 4\nQ1: what is the output volume size of \nCONV1?\nQ2: how many parameters for \nCONV1?\n31\n\nAlexNet [2012]\nInput : 227x227x3 images\nFirst layer (CONV1): 96 filters \n11x11, S = 4\nQ1: what is the output volume size of \nCONV1?\nQ2: how many parameters for \nCONV1?\n32\nQ1: \nactivation map size = (227-11)/4 + 1 \n= 55x55\noutput volume = (55x55x96)\nQ2: \n96x(11x11x3) + 96 = 34\u2019944 ~ 35K\n\nVGGNet [2014]\n\u2022 Simonyan and Zisserman, 2014\n33\nSmall filters, Deeper networks\nFrom 8 layers (AlexNet) to 16 - 19 layers (VGGNet) \nOnly 3x3 CONV S=1, P=1 and 2x2 MAX POOL S=2 \n7.3% top 5 error in ILSVRC\u201914 \nStack of three 3x3 conv (stride 1) \nlayers has same effective receptive field \nas one 7x7 conv layer.\nBut deeper, more non-linearities.\n\nVGGNet [2014]\n34\n\nResNet [2015]\n\u2022 He et al., 2015\n\u2022 Observation they did: from AlexNet and VGGNet, \nincreasing the number of layers did not succeed. They \nobserved something like this:\n35\nOverfitting?\n56-layer model performs worse on both training \nand test errorThe deeper model performs worse, \nbut it\u2019s not caused by overfitting! \n\nResNet [2015]\n36\n\u2022 Hypothesis: the problem is an optimization problem, deeper models are more \ndifficult to optimize \n\u2022 Ideas:\n-\nDeeper layers should be able to \u201cfall back\u201d to identity mapping if difficulties to converge\n-\nEasier to model a \u201cdelta\u201d from one layer to the other than a full feature\n\u2022 Solution: Use network layers to fit a residual mapping instead of directly trying to fit \na desired underlying mapping\n\nResNet [2015]\n37\n\u2022 Trick to go deeper than 50 \nlayers:\n-\nUse bottleneck layers 1x1x64 to \nreduce the number of operations \nand parameters \n\u2022 Other training strategies:\n-\nBatch Normalization after every \nCONV layer\n-\nXavier 2/ initialization from He et al. \n-\nSGD + Momentum (0.9) \n-\nLearning rate: 0.1, divided by 10 \nwhen validation error plateaus \n-\nMini-batch size 256 \n-\nWeight decay of 1e-5 \n-\nNo dropout used\n\nTransformers\n38\nTransformers architecture\nLarge Langage Models \nSoftmax\nLinear\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nAdd & Norm\nMulti-Headed\nAttention\nOutput embedding\n(positional)\nInput embedding\n(positional)\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nInputs\nOutputs\nso far\nClassification\noutput\n(probabilistic)\n\nSequential modeling\n\u2022 We have seen sequential modeling when studying RNNs: a sequence can be\nfed to the network over multiple activations\n\u2022 We will use sequences of words to model human language as an example,\nbut these methods are applicable to any kind of sequential data\n\u2022 For example, consider the autocompleting keyboard in your phone: if you\nkeep selecting the next predicted word you get something that sort of makes\nsense grammatically but is not very sophisticated.\n\u2022 Example: I inputed \u201cTell me a joke.\u201d then autocompleted the rest on my\nphone (incognito mode, self-learning off for basic privacy) and got this:\nTell me a joke. I will take care of yourself and your family and friends.\n\u2022 It is evident that each word depends on past context (autoregression), but\nthe context is too small for this application\n\nSequential modeling\n\u2022 Past sequential modeling approaches, based on RNNs, suffer from:\n\u2022 Short memory and loss of depth: the network only has the previous\nactivation as context. To the unrolled model, each input enters the model at\na later layer, limiting the leverage of functional composition across layers\n\u2022 Hard to train: backpropagation through time cannot be effectively \naccelerated through parallelization, limiting the performance achievable with \ncurrent hardware\n\u2022 Further research improved over RNN results with Gate Recurrent Units \n(GRU) and Long Short Term Memory (LSTM): adding special memory \nlayers, controlled by dedicated neurons, mitigates but does not solve the \nproblem of limited contextualization\n\nTransformers\n\u2022 The modern solution to sequential modeling, as you are probably already \naware, is an architecture called a Transformer\n\nTransformers\n\u2022 Transformers are a class of neural network \narchitectures with state-of-the-art performance in \nsequential modeling\n\u2022 For example, the final \u201cT\u201d in GPT (/ChatGPT) and \nBERT stands for Transformer\n(respectively Generative Pre-trained Transformer and\nBidirectional Encoder Representations from Transformers)\n\u2022 Their results are a consequence of three \nfundamental innovations:\n\u2022 Self-attention\n\u2022 Multi-headed attention\n\u2022 Arbitrary-length context\n\u2022 We will now explore these concepts briefly, before \nputting them together\nSoftmax\nLinear\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nAdd & Norm\nMulti-Headed\nAttention\nOutput embedding\n(positional)\nInput embedding\n(positional)\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nInputs\nOutputs\nso far\nClassification\noutput\n(probabilistic)\n\n\n\nSelf-attention and Multi-Headed Attention\n\u2022 The self-attention mechanism generates for\neach token (~word) three independent values\nfor a Query, Key and Value, akin to most\nknowledge\nretrieval\nsystems\n(e.g.\nsearch\nengines)\n\u2022 This contextualized attention matrix is then\nprocessed once again through a Linear layer, to\ncreate a context to steer the decoder towards the\ncontext of the prompt\n\u2022 This is what makes ChatGPT better at generating\nmeaningful\nlarge\ntexts\nthan\nyour\nphone\u2019s\nautocomplete keyboard \u2013 for now...\n\u2022 The last step is Multi-Headed Attention (MHA):\ntensors Q, K and V are split into equal-size chunks,\nthen each chunk is processed by a different head\nusing this architecture (which actually depicts one\nMHA head)\nLinear\nConcat\nDot product\nSoftmax\nRescale\nDot product\nLinear\nLinear\nLinear\nQ\nK\nV\nAttention\nmatrix\n\nEncoder-decoder structure\n\u2022 The architecture is composed, like GAN, of \ntwo components: encoder and decoder\n\u2022 The role of the encoder is to construct a \nfocusing context (self-attention mechanism)\n\u2022 The decoder instead is an autoregression \nsequence generator: it computes the next \nelement on a sequence based on the \noutputs it produced so far\n\u2022 The encoder however injects the prompt \ncontext in the decoder, forcing it to \nmaintain attention on the user prompt\n\u2022 Transformers can in principle scale to \narbitrary-length sequences, only limited by \nperformance constraints\nSoftmax\nLinear\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nAdd & Norm\nMulti-Headed\nAttention\nOutput embedding\n(positional)\nInput embedding\n(positional)\nAdd & Norm\nFeed-forward\nAdd & Norm\nMulti-Headed\nAttention\nInputs\nOutputs\nso far\nClassification\noutput\n(probabilistic)\n\nGPT models\n\u2022 With this context, it is now possible to understand entirely the structure of \nGPT models, such as the GPT-3 and GPT-4 behind OpenAI\u2019s ChatGPT\n\u2022 GPT extends the basic transformer architecture by scaling the number of \nencoder blocks, the number of decoder blocks, the number of heads in each \nMHA block, and the size of the prompt\n\u2022 The size of the network quickly scales into the hundreds of billions of \nweights, and the training data exceeds 45 terabytes and includes most of the \nInternet, spanning into e.g. medical and legal information, the entire \nWikipedia, all open-source code, and of course social media\n\u2022 GPT4 tell me a joke: Why don't skeletons fight each other? They don't \nhave the guts\n\nLarge Language Models and Intelligence\n\u2022 Be very careful: language models simply model language\n\u2022 They do not model meaning, intention, or any form of thought: they are \nlimited to approximating the dynamics and patterns found in the language, \nand reproduce them to high precision\n\u2022 Asking ChatGPT if it is sentient is exactly the same as asking a parrot if it \nis sentient: if it has learned that the answer to that question is most \ncommonly yes, then this is the answer you will get, no truly existential \nquestions are being posed\n\u2022 Of course parrots are highly intelligent creatures with complex emotional \ndynamics, social behavior and general intention, making them light years \nahead of any transformer-based product from the perspective of Artificial \nGeneral Intelligence\n\nMSE - TSM-DeLearn\nLimitations\n\nEstimated 2-4% of homosexuals \nin a normal population \u00e0 a\nmodel that always output \nheterosexual would get 96-98% \naccuracy!!\nBy the way\u2026\nWorldwide there are at least \neight countries in which \nhomosexuality or homosexual \nacts are punishable by death\nLimitations of Deep Learning\n1: (Hidden) Biais\nAre You a Criminal??\n\u201cThere\u2019s software used across the \ncountry to predict future criminals. \nAnd it\u2019s biased against blacks.\u201d\nby Julia Angwin, Jeff Larson, Surya \nMattu and Lauren Kirchner\n\u201cThe \u2018three black teenagers\u2019 \nsearch shows it is society, not \nGoogle, that is racist\u201d by Antoine \nAllen\n3 white vs 2 black teenagers on \nGoodle image search\nMachines Predict Sexual \nOrientation\n\nLimitations of Deep Learning\n2: Need more data\n\u2022 Rule of thumb: need at least 5000 examples per each label\n\u2022 Many application (often as important as medical imaging) cannot comply to \nthis requirement\n\u2022 You can generate synthetic data as variations of what you have (data \naugmentation), but will contain by definition no extra information\n\u2022 Transfer learning can help: train on a larger dataset over similar \ndata/task, then re-initialize and re-train the decision making layers only \n(keep feature extraction)\n\u2022 Synthetize data: with GAN for example, difficult to obtain high \nvariability, need to be combine with real data\n\nLimitations of Deep Learning\n3: No interpretability \u2192No trust\n\u2022 Deep Networks are black boxes in the sense that it is not possible to reason \nand interpret their behavior\n\u2022 Too complex, too high dimensional, we need them to generalize to unseen \ncases but we cannot predict how\n\u2022 A self-driving car that stays on road 99% of the time, or a surgeon that cuts \nperfectly 99% of the time, are not enough\n\u2022 There are Bayesian interpretations of Neural Networks that can help here, \nbut only on (very) limited problems\n\nLimitations of Deep Learning\n4: No such thing as AI\nIn 2016, Microsoft designed a self-learning chatbot (named Tay) with the goal of \nlearning human communication through interacting with real people.\nOnly they connected it to a real Twitter account and advertised that anybody\ncould chat with Tay. Over Twitter. And it should learn from these interactions.\nYeah that sounds perfectly safe, right??\nBEWARE:\nThe next slides OF COURSE contains a \u201crogue AI\u201d \nthat has been optimized to offend people\u2019s sensibilities\n\nLimitations of Deep Learning\n4: No such thing as AI\n\u2022 Neural networks do not encompass higher values such as ethics, morals, \nemotions, customs, sarcasm, subtlety or simply practical jokes.\n\u2022 NNs are just functions, they can only represent the data they are trained on\n\u2022 Even more: as long as we keep relying on Supervised Learning and \nsingle-agent methods, we cannot create truly-new skills\n\u2022 We are still defining (and arguing over) what IS intelligence \u2013 far from \nimplementing an artificial one\n\u2022 Modern Machine Learning provides tools, not intelligence: the \nresponsibility of using a knife to prepare a meal or to threaten a person\n\u2022\nFALLS ON ITS USER\n44\nThis content is from Microsoft Chatbot Tay, rogue AI; content not from slides\u2019 author\n\nLimitations of Deep Learning\n5: Deep Fakes\n\u2022\nDeepfake pornography surfaced on the \nInternet in 2017\n\u2022\nThe falsification of images and videos \nis even older than the advent of video \nediting software and image editing \nprograms; in this case it is the realism\nthat is a new aspect\n\u2022\n\u201cHumanity could fall into an age in \nwhich it can no longer be determined \nwhether a medium's content \ncorresponds to the truth\u201d Aargauer\nZeitung\n44\ngeneration from scratch\nface to face transformation\nunethical consequences!\n\nSummary\n\u2022 DL is just the current chapter in ML history, itself spanning decades\n\u2022 Repeating cycle: create amazing technology, over-promise, hit technological \nwall, under-deliver, be blamed and forgotten (aka AI Winters)\n\u2022 Today\u2019s trick: training huge (overly-complex) function approximators based \non an efficient if simplistic SL algorithm (backprop), leveraging\nembarrassingly parallel computation on specialized hardware and enormous, \nlabeled datasets\n\u2022 Success: machine translation (finally!), visual processing (object recognition, \nimage generation, etc.), simplistic reinforcement learning applications such \nas games (more on that next week!), and a few more related applications\n\u2022 Deep Learning is just a tool, and we have not solved Artificial Intelligence \nyet \u2013 or even found a plausible path!\n\u2022 As the user, YOU are responsible for the consequences\nCalling technology \u201cmagic\u201d only means\n\u201cI can use it but not understand or explain it\u201d\n45\n\nExtra material 1/2\n\u2022 Turing Award page on Wikipedia:\nhttps://en.wikipedia.org/wiki/Turing_Award\n\u2022 The Deep Learning book:\nhttps://www.deeplearningbook.org\n\u2022 Good Old AI approach to Tic-Tac-Toe (MENACE):\nhttps://www.youtube.com/watch?v=hK25eXRaBdc\n\u2022 What is Cybernetics, fundamental for Computer Science:\nhttps://en.wikipedia.org/wiki/Cybernetics\n\u2022 Hardware scaling: CPUs vs GPUs (video):\nhttps://www.youtube.com/watch?v=-P28LKWTzrI\n\u2022 Model Quantization for Tensorflow: \nhttps://www.tensorflow.org/lite/performance/post_training_\nquantization\n\nExtra material 2/2\n\u2022 Skip-gram model (source for graph): \nhttp://mccormickml.com/2016/04/19/word2vec-tutorial-the-\nskip-gram-model\n\u2022 Adam, the most broadly adopted improvement on backprop:\nhttps://arxiv.org/pdf/1412.6980.pdf\n\u2022 Bengio is in favor of backpropagation...\nhttps://www.youtube.com/watch?v=W86H4DpFnLY\n\u2022 ... while Hinton thinks we need alternatives:\nhttps://www.youtube.com/watch?v=qIEfJ6OBGj8\n\u2022 The official Transformer tutorial from Tensorflow:\nhttps://www.tensorflow.org/text/tutorials/transformer\n\u2022 The GPT architecture step-by-step:\nhttps://dugas.ch/artificial_curiosity/GPT_architecture.html\n\n"}